---
title: Coding
categories:
- Software
- Concept
---
# Coding

## Java SE

### 基本数据类型和大小

- boolean(1) = byte(1) < short(2) = char(2) < int(4) = float(4) < long(8) = double(8)

### int 和 Integer的区别

- int是基本数据类型,Integer是他的包装类
- Integer保存的是对象的引用,int保存的变量值
- Integer默认是null,int默认是0
- Integer变量必须实例化后才能使用,而int变量不需要

### Integer 缓存

```java
Integer i1 = 59;
int i2 = 59;
Integer i3 = Integer.valueOf(59);
Integer i4 = new Integer(59);
System.out.println(i1 == i2); // true:包装类和基本类型比较时,包装类自动拆箱为基本类型
System.out.println(i1 == i3); // true:数值59 在-128到127之间,上文所说的缓存,因此为true,若数值不在-128到127之间则为false
System.out.println(i1 == i4); // false:引用类型比较地址值,地址值不同
System.out.println(i2 == i3); // true:i1的源码是i3,i2和i3比较结果和i2与i1比较结果相同,包装类和基本类型比较时自动拆箱
System.out.println(i2 == i4); // true:包装类和基本类型比较时自动拆箱
System.out.println(i3 == i4); // 同i1 == i4
```

### 接口和抽象类的区别

1. 类可以实现很多个接口,但是只能继承一个抽象类
2. 接口中所有的方法隐含的都是抽象的除了default方法,而抽象类则可以同时包含抽象和非抽象的方法
3. 接口中声明的变量默认都是public final的,抽象类可以包含非public final的变量
4. 抽象类是对实体类的抽象,接口是对行为的抽象

### 访问控制

| 控制等级  | 同一类中   | 同一包中   | 不同包的子类中 | 其他       |
| --------- | ---------- | ---------- | -------------- | ---------- |
| private   | 可直接访问 |            |                |            |
| 默认      | 可直接访问 | 可直接访问 |                |            |
| protected | 可直接访问 | 可直接访问 | 可直接访问     |            |
| public    | 可直接访问 | 可直接访问 | 可直接访问     | 可直接访问 |

### super 和 this的异同

- super(参数):调用父类中的某一个构造函数(应该为构造函数中的第一条语句),每个子类构造方法的第一条语句,都是隐含地调用 super(),如果父类没有这种形式的构造函数,那么在编译的时候就会报错
- this(参数):调用本类中另一种形成的构造函数(应该为构造函数中的第一条语句)
- this 和 super 不能同时出现在一个构造函数里面,因为this必然会调用其它的构造函数,其它的构造函数必然也会有 super 语句的存在,所以在同一个构造函数里面有相同的语句,就失去了语句的意义,编译器也不会通过
- this() 和 super() 都指的是对象,所以,均不可以在 static 环境中使用,包括:static 变量,static 方法,static 语句块

### 继承中父类和子类的初始化顺序

1. 父类中静态成员变量和静态代码块
2. 子类中静态成员变量和静态代码块
3. 父类中普通成员变量和代码块,父类的构造函数
4. 子类中普通成员变量和代码块,子类的构造函数

### 方法的覆盖与隐藏

- 父类的实例方法被子类的同名实例方法**覆盖**,父类的静态方法被子类的同名静态方法**隐藏**,父类的实例变量和类变量可以被子类的实例变量和类变量**隐藏**
- 通过父类引用可以暴露隐藏的变量和方法
- 方法覆盖不能改变方法的静态与非静态属性,子类中不能将父类的实例方法定义为静态方法,也不能将父类的静态方法定义为实例方法
- 不允许子类中方法的访问修饰符比父类有更多的限制,例如,不能将父类定义中用public修饰的方法在子类中重定义为private方法,但可以将父类的private方法重定义为public方法,通常应将子类中方法访问修饰与父类中的保持一致

### 方法重写后的动态绑定

- 多态允许具体访问时实现方法的动态绑定,Java对于动态绑定的实现主要依赖于方法表,通过继承和接口的多态实现有所不同
- **继承**:在执行某个方法时,在方法区中找到该类的方法表,再确认该方法在方法表中的偏移量,找到该方法后如果被重写则直接调用,否则认为没有重写父类该方法,这时会按照继承关系搜索父类的方法表中该偏移量对应的方法
- **接口**:Java 允许一个类实现多个接口,从某种意义上来说相当于多继承,这样同一个接口的的方法在不同类方法表中的位置就可能不一样了,所以不能通过偏移量的方法,而是通过搜索完整的方法表

### 异常

-   **Error**:程序中无法处理的错误,此类错误一般表示代码运行时JVM出现问题,通常有VirtualMachineError(虚拟机运行错误),OutOfMemoryError等,JVM将终止线程
-   **Exception**:程序本身可以捕获并且可以处理的异常
    -   运行时异常(非受检异常):RuntimeException类及其子类,表示JVM在运行期间可能出现的错误,编译器不会检查此类异常,并且不要求处理异常,比如用空值对象的引用(NullPointerException),数组下标越界(ArrayIndexOutBoundException),此类异常属于不可查异常,一般是由程序逻辑错误引起的,在程序中可以选择捕获处理,也可以不处理
    -   非运行时异常(受检异常):Exception中除RuntimeException极其子类之外的异常,编译器会检查此类异常,如果程序中出现此类异常,比如说IOException,必须对该异常进行处理,要么使用try-catch捕获,要么使用throws语句抛出,否则编译不通过

### String & StringBuilder & StringBuffer

- String是只读字符串,也就意味着String引用的字符串内容是不能被改变的
- StringBuffer/StringBuilder类表示的字符串对象可以直接进行修改
- StringBuilder和StringBuffer的方法完全相同,区别在于StringBuilder不是线程安全,因为它的所有方面都没有被synchronized修饰,因此它的效率也比StringBuffer要高
- 当使用 + 号将字符串拼接时,其实底层是调用了StringBuilder 的append方法
- 创建一个字符串时,首先会检查池中是否有值相同的字符串对象,如果有就直接返回引用,不会创建字符串对象,如果没有则新建字符串对象,返回对象引用,并且将新创建的对象放入池中,但是,通过new方法创建的String对象是不检查字符串常量池的,而是直接在堆中创建新对象,也不会把对象放入池中,上述原则只适用于直接给String对象引用赋值的情况

```java
String str1 = new String("a"); //不检查字符串常量池的
String str2 = "bb"; //检查字符串常量池的
```

### 浅复制和深复制

- 如果在拷贝这个对象的时候,只对基本数据类型进行了拷贝,而对引用数据类型只是进行了引用的传递,而没有真实的创建一个新的对象,则认为是浅拷贝
- 反之,在对引用数据类型进行拷贝的时候,创建了一个新的对象,并且复制其内的成员变量,则认为是深拷贝

### 自动拆箱装箱

**自动装箱与自动拆箱的实现原理**

```java
public static void main(String[]args){
  Integer integer=1; //装箱
  int i=integer; //拆箱
}
```

- 对以上代码进行反编译后可以得到以下代码:

```java
public static void main(String[]args){
  Integer integer=Integer.valueOf(1);
  int i=integer.intValue();
}
```

**哪些地方会自动拆装箱**

1. 将基本数据类型放入集合类
2. 包装类型和基本类型的大小比较:包装类与基本数据类型进行比较运算,是先将包装类进行拆箱成基本数据类型,然后进行比较的
3. 包装类型的运算:两个包装类型之间的运算,会被自动拆箱成基本类型进行
4. 三目运算符的使用:当第二,第三位操作数分别为基本类型和对象时,其中的对象就会拆箱为基本类型进行操作

### Object类的方法

-   **clone()**:创建并返回此对象的一个副本
-   **equals(Object obj)**:指示某个其他对象是否与此对象"相等”
-   **finalize()**:当垃圾回收器确定不存在对该对象的更多引用时,由对象的垃圾回收器调用此方法
-   **getClass()**:返回一个对象的运行时类
-   **hashCode()**:返回该对象的Hash值
-   **notify()**:唤醒在此对象监视器上等待的单个线程
-   **notifyAll()**:唤醒在此对象监视器上等待的所有线程
-   **toString()**:返回该对象的字符串表示
-   **wait()**:导致当前的线程等待,直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法
-   **wait(long timeout)**:导致当前的线程等待,直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法,或者超过指定的时间量
-   **wait(long timeout, int nanos)**:导致当前的线程等待,直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法,或者其他某个线程中断当前线程,或者已超过某个实际时间量

### String为什么不可变

-   **不可变对象**:指一个对象的状态在对象被创建之后就不再变化,不能改变对象内的成员变量,包括基本数据类型的值不能改变,引用类型的变量不能指向其他的对象,引用类型指向的对象的状态也不能改变
-   String 不可变是因为在 JDK 中 String 类被声明为一个 final 类,且类内部的 value 字节数组也是 final 的
-   只有当字符串是不可变时字符串池才有可能实现,字符串池的实现可以在运行时节约很多 heap 空间,因为不同的字符串变量都指向池中的同一个字符串

### 序列化与反序列化

- Java序列化是将一个对象编码成一个字节流,反序列化将字节流编码转换成一个对象
- 为了实现用户自定义的序列化,相应的类必须实现`Serializable`接口,`Serializable`接口中没有定义任何方法在,实现了 Serializable 接口后, JVM 会在底层帮我们实现序列化和反序列

#### serialVersionUID 的作用

-   在进行反序列化时,JVM会把传来的字节流中的serialVersionUID与本地相应实体(类)的serialVersionUID进行比较,如果相同就认为是一致的,可以进行反序列化,否则就会出现序列化版本不一致的异常
-   如果不显示指定 serialVersionUID, JVM 在序列化时会根据属性自动生成一个 serialVersionUID, 然后与属性一起序列化, 再进行持久化或网络传输. 在反序列化时, JVM 会再根据属性自动生成一个新版 serialVersionUID, 然后将这个新版 serialVersionUID 与序列化时生成的旧版 serialVersionUID 进行比较, 如果相同则反序列化成功, 否则报错
-   当序列化了一个类实例后,希望更改一个字段或添加一个字段,不设置serialVersionUID,所做的任何更改都将导致无法反序化旧有实例,并在反序列化时抛出异常

### Java 值传递与引用传递

-   Java参数传递分为值传递和引用传递,基本类型是值传递,封装的对象时引用传递

### Statement和PreparedStatement的区别

- PreparedStatement接口代表预编译的语句,它主要的优势在于可以减少SQL的编译错误并增加SQL的安全性(减少SQL注射攻击的可能性)
- PreparedStatement中的SQL语句是可以带参数的,避免了用字符串连接拼接SQL语句的麻烦和不安全
- 当批量处理SQL或频繁执行相同的查询时,PreparedStatement有明显的性能上的优势,由于数据库可以将编译优化后的SQL语句缓存起来,下次执行相同结构的语句时就会很快(不用再次编译和生成执行计划)

### JDBC中Class.forName的作用

- `Class.forName`方法的作用,就是初始化给定的类,而我们给定的 MySQL 的 Driver 类中,它在静态代码块中通过 JDBC 的 DriverManager 注册了一下驱动,我们也可以直接使用 JDBC 的驱动管理器注册 mysql 驱动,从而代替使用`Class.forName`

## Java EE

### forward与redirect

- **forward**:服务器请求资源,服务器直接访问目标地址的URL,把对应URL的响应内容读取过来,再发送给浏览器,所以URL不变,可以共享request的数据
- **redirect**:服务器发送一个状态码302,告诉浏览器重新去请求指定的地址,不能共享数据,地址栏显示的是新的URL

### Cookie与Session

1. cookie数据存放在客户的浏览器上,session数据放在服务器上
2. cookie不是很安全,别人可以分析存放在本地的cookie并进行cookie欺骗,考虑到安全应当使用session
4. 单个cookie保存的数据不能超过4K,很多浏览器都限制一个站点最多保存20个cookie
5. 可以考虑将登陆信息等重要信息存放为session,其他信息如果需要保留,可以放在cookie中

## 反射

### 获得一个类的类对象

1. 类名.class,例如:String.class
2. 对象.getClass(),例如:"hello".getClass()
3. Class.forName(),例如:Class.forName("java.lang.String")

### 通过反射创建对象

1. 通过类对象调用newInstance()方法,例如:String.class.newInstance()
2. 通过类对象的getConstructor()或getDeclaredConstructor()方法获得构造器(Constructor)对象并调用其newInstance()方法创建对象,例如:String.class.getConstructor(String.class).newInstance("Hello");

### 通过反射获取和设置对象私有字段的值

1. 通过类对象的getDeclaredField()方法字段(Field)对象
2. 再通过字段对象的setAccessible(true)将其设置为可以访问
3. 通过get/set方法来获取/设置字段的值了

### 通过反射调用对象的方法

1. 通过类对象的getMethod()方法获得方法对象
2. 调用方法对象的invoke()方法

## 集合

### List/Set/Map的区别

- List有序存取元素,可以有重复元素
- Set不能存放重复元素,存入的元素是无序的
- Map保存键值对映射,映射关系可以是一对一或多对一

### HashMap/Hashtable/HashSet/LinkedHashMap/TreeMap 比较

- Hashmap 是一个最常用的 Map,它根据键的 HashCode 值存储数据,HashMap 最多只允许一条记录的键为Null,允许多条记录的值为 Null
- Hashtable 与 HashMap 类似,不同的是:它不允许记录的键或者值为空,是线程安全的,因此也导致了 Hashtale 的效率偏低
- LinkedHashMap 是 HashMap 的一个子类,如果需要输出的顺序和输入的相同,那么用 LinkedHashMap 可以实现
- TreeMap 实现 SortMap 接口,内部实现是红黑树,能够把它保存的记录根据键排序,默认是按键值的升序排序,也可以指定排序的比较器,当用 Iterator 遍历 TreeMap 时,得到的记录是排过序的,TreeMap 不允许 key 的值为 null

### HashSet/LinkedHashSet/TreeSet 比较

- **HashSet**
    - HashSet 是由 HashMap 实现的,不保证元素的顺序
- **LinkedHashSet**
    - LinkedHashSet 集合同样是根据元素的 hashCode 值来决定元素的存储位置,但是它同时使用链表维护元素的次序,这样使得元素看起来像是以插入顺序保存的,也就是说,当遍历该集合时候,LinkedHashSet 将会以元素的添加顺序访问集合的元素
    - **LinkedHashSet 在迭代访问 Set 中的全部元素时,性能比 HashSet 好,但是插入时性能稍微逊色于 HashSet**
- **TreeSet**
    - TreeSet 是 SortedSet 接口的唯一实现类,TreeSet 可以确保集合元素处于排序状态

### ArrayList/LinkedList 比较

- ArrayList内部使用数组存放元素,实现了可变大小的数组,访问元素效率高,当插入元素效率低
- LinkedList内部使用双向链表存储元素,插入元素效率高,但访问元素效率低
- 相对于ArrayList,LinkedList的插入,添加,删除操作速度更快,因为当元素被添加到集合任意位置的时候,不需要像数组那样重新计算大小或者是更新索引
- LinkedList比ArrayList更占内存,因为LinkedList为每一个节点存储了两个引用,一个指向前一个元素,一个指向下一个元素

### Iterator和ListIterator

- Iterator提供了统一遍历操作集合元素的统一接口, Collection接口实现Iterable接口,每个集合都通过实现Iterable接口中`iterator()`方法返回Iterator接口的实例, 然后对集合的元素进行迭代操作
- **优点**
    - 对任何集合都采用同一种访问模型
    - 调用者不用了解集合的内部结构
- **Iterator和ListIterator的区别**
    - Iterator可用来遍历Set和List集合,但是ListIterator只能用来遍历List
    - Iterator对集合只能是前向遍历,ListIterator既可以前向也可以后向
    - ListIterator实现了Iterator接口,并包含其他的功能,比如:增加元素,替换元素,获取前一个和后一个元素的索引

### Hash冲突

- 由于Hash算法被计算的数据是无限的,而计算后的结果范围有限,因此总会存在不同的数据经过计算后得到的值相同,这就是Hash冲突
- 冲突处理分为以下几种方式:
    - **开放地址法**:出现冲突后按照一定算法查找一个空位置存放
        - **线性探测再散列**:线性探测方法就是线性探测空白单元,当数据通过Hash函数计算应该放在700这个位置,但是700这个位置已经有数据了,那么接下来就应该查看701位置是否空闲,再查看702位置,依次类推
        - **二次探测再散列**:二次探测是过程是x+1,x+4,x+9,以此类推,**二次探测的步数是原始位置相隔的步数的平方**
        - **再哈希法**:出现冲突后采用其他的Hash函数计算,直到不再冲突为止
    - **链地址法(拉链法)**:不同与前两种方法,他是在出现冲突的地方存储一个链表,所有的同义词记录都存在其中
    - **建立公共溢出区**:建立公共溢出区的基本思想是:假设Hash函数的值域是[1,m-1],则设向量HashTable[0...m-1]为基本表,每个分量存放一个记录,另外设向量OverTable[0...v]为溢出表,所有关键字和基本表中关键字为同义词的记录,不管它们由Hash函数得到的Hash值是什么,一旦发生冲突,都填入溢出表

### HashMap

- HashMap基于Hash表的Map接口实现,主要用来存放键值对,HashMap通过Hash函数定位元素的存放位置,所以HashMap并不是有序的,使用拉链法解决Hash冲突,在JDK1.8之后,当 HashEntry 的长度大于8并且当前table数组的长度大于64时,将链表改为使用红黑树存储
    - **补充**:将链表转换成红黑树前会判断,即便阈值大于8,但是数组长度小于64,此时并不会将链表变为红黑树,而是选择逬行数组扩容,这样做的目的是因为数组比较小,尽量避开红黑树结构,这种情况下变为红黑树结构,反而会降低效率,因为红黑树需要逬行左旋,右旋,变色这些操作来保持平衡,同时数组长度小于64时,搜索时间相对要快
- **Hash函数**：包括取key的hashCode值,高位运算,取模运算
    - 如果key为null则Hash函数的值为0
    - 高位运算的算法,通过hashCode()的高16位异或低16位实现的可以在数组table的length比较小的时候,也能保证考虑到高低Bit都参与到Hash的计算中,同时不会有太大的开销
    - 取模运算通过h & (table.length -1)来得到该对象的保存位置,而HashMap底层数组的长度总是2的n次方,当length总是2的n次方时,h& (length-1)运算等价于对length取模,也就是h%length,但是&比%具有更高的效率
- **重写方法**：由于HashMap的插入查询等操作需要用到`hashcode()`和`equals()`,所以如果key是自定义的类,就必须重写这两个方法
- **扩容**：
    - HashMap的扩容一个耗时的操作,在第一次`put()`的时候会初始化,发生第一次`resize()`到16,默认负载因子是0.75,当容量达到`size*Load Factor`时就会扩大容量为原来的2倍,所以如果我们知道大概的数据量,应该使用对应的构造方法直接初始化指定容量的HashMap
    - 扩容长度扩为原来2倍,所以元素的位置要么是在原位置,要么是在原位置再移动2次幂的位置，因此,我们在扩充HashMap的时候,不需要像JDK1.7的实现那样重新计算hash,只需要看看原来的hash值新增的那个bit是1还是0就好了
- **非线程安全**：HashMap的不是线程安全的,如果需要使用线程安全的Map集合,可以通过Collections类的synchronizedMap方法包装一下,或者直接使用JUC包下的ConcurrentHashMap

### ConcurrentHashMap

-   Concurrenthashmap是线程安全的

-   JDK1.7

    -   ConcurrentHashMap 和 HashMap 实现上类似,最主要的差别是 ConcurrentHashMap 采用了分段锁(Segment),它继承自重入锁 ReentrantLock,每个分段锁维护着几个桶(HashEntry),多个线程可以同时访问不同分段锁上的桶,从而使其并发度更高(并发度就是 Segment 的个数)
    -   在 HashEntry 类中:key,hash 和 next 域都被声明为 final 型,value 域被声明为 volatile 型
    -   在ConcurrentHashMap 中,如果产生Hash冲突,将采用**拉链法**来处理,即把碰撞的 HashEntry 对象链接成一个链表,由于 HashEntry 的 next 域为 final 型,所以新节点只能在链表的表头处插入,由于只能在表头插入,所以链表中节点的顺序和插入的顺序相反
    -   size()的计算是先采用不加锁的方式,连续计算元素的个数,最多计算3次:
        1.  如果前后两次计算结果相同,则说明计算出来的元素个数是准确的
        2.  如果前后两次计算结果都不同,则给每个Segment进行加锁,再计算一次元素的个数

-   JDK1.8

    -   放弃了 Segment 臃肿的设计,使用了 CAS 操作来支持更高的并发度,在 CAS 操作失败时使用内置锁 synchronized,在链表过长时会转换为红黑树

### poll & offer

|                    | throw Exception | 返回false或null    |
| :----------------- | :-------------- | ------------------ |
| 添加元素到队尾     | add(E e)        | boolean offer(E e) |
| 取队首元素并删除   | E remove()      | E poll()           |
| 取队首元素但不删除 | E element()     | E peek()           |

### 快速失败(fail-fast)和安全失败(fail-safe)

- Iterator的安全失败是基于对底层集合做拷贝,因此,它不受源集合上修改的影响,java.util包下面的所有的集合类都是快速失败的
- JUC包下面的所有的类都是安全失败的,快速失败的迭代器会抛出ConcurrentModificationException异常,而安全失败的迭代器永远不会抛出这样的异常

### 重写 equals 和 hashCode

1. 作为`key`的对象必须正确覆写`equals()`方法,相等的两个`key`实例调用`equals()`必须返回`true`
2. 作为`key`的对象还必须正确覆写`hashCode()`方法,因为通过`key`计算索引的方式就是调用`key`对象的`hashCode()`方法,它返回一个`int`整数,`HashMap`正是通过这个方法直接定位`key`对应的`value`的索引,继而直接返回`value`,且`hashCode()`方法要严格遵循以下规范:
    - 如果两个对象相等,则两个对象的`hashCode()`必须相等
    - 如果两个对象不相等,则两个对象的`hashCode()`尽量不要相等
3. 即对应两个实例`a`和`b`:
    - 如果`a`和`b`相等,那么`a.equals(b)`一定为`true`,则`a.hashCode()`必须等于`b.hashCode()`
    - 如果`a`和`b`不相等,那么`a.equals(b)`一定为`false`,则`a.hashCode()`和`b.hashCode()`尽量不要相等

## 多线程

### 线程的状态

**线程的状态**

- Java中线程的状态分为6种

  1. 初始(NEW):新创建了一个线程对象,但还没有调用start()方法
  2. 运行(RUNNABLE):Java线程中将就绪(ready)和运行中(running)两种状态笼统的称为"运行”
     线程对象创建后,其他线程(比如main线程)调用了该对象的start()方法,该状态的线程位于可运行线程池中,等待被线程调度选中,获取CPU的使用权,此时处于就绪状态(ready),就绪状态的线程在获得CPU时间片后变为运行中状态(running)
  3. 阻塞(BLOCKED):表示线程阻塞于锁
  4. 等待(WAITING):进入该状态的线程需要等待其他线程做出一些特定动作(通知或中断)
  5. 超时等待(TIMED_WAITING):该状态不同于WAITING,它可以在指定的时间后自行返回
  6. 终止(TERMINATED):表示该线程已经执行完毕
- 这6种状态定义在Thread类的State枚举中,可查看源码进行一一对应

**线程的状态图**

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2021-04-14-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3BhbmdlMTk5MQ==,size_16,color_FFFFFF,t_70.jpeg" alt="线程状态图" style="zoom: 50%;" />

### 多线程的创建方式

- 继承Thread类
- 实现Runnable接口
- 实现Callable接口

### 多线程有关的方法

- `Thread.sleep(long millis)`:一定是当前线程调用此方法,当前线程进入`TIMED_WAITING`状态,但不释放对象锁,millis后线程自动苏醒进入就绪状态
- `Thread.yield()`:一定是当前线程调用此方法,当前线程放弃获取的CPU时间片,但不释放锁资源,由运行状态变为就绪状态,让OS再次选择线程
- `thread.join()/thread.join(long millis)`:当前线程里调用其它线程t的join方法,当前线程进入`WAITING/TIMED_WAITING`状态,当前线程不会释放已经持有的对象锁,线程t执行完毕或者millis时间到,当前线程一般情况下进入RUNNABLE状态,也有可能进入BLOCKED状态(因为join是基于wait实现的)
- `thread.interrupt()`:标记线程为中断状态,不过不会中断正在运行的线程
- `thread.isInterrupted()`:测试线程是否已经中断,该方法由对象调用
- `obj.wait()`:当前线程调用对象的`wait()`方法,当前线程释放对象锁,进入等待队列,依靠`notify()/notifyAll()`唤醒或者`wait(long timeout)` timeout时间到自动唤醒
    - `ObjectSynchronizer::wait`方法通过object的对象中找到ObjectMonitor对象调用方法`void ObjectMonitor::wait(jlong millis, bool interruptible, TRAPS) `
    - 通过`ObjectMonitor::AddWaiter`调用把新建立的`ObjectWaiter`对象放入到`_WaitSet`的队列的末尾中然后在`ObjectMonitor::exit`释放锁,接着 `thread_ParkEvent->park`也就是wait
- `obj.notify()`:唤醒在此对象监视器上等待的单个线程,选择是任意性的
- `obj.notifyAll()`:唤醒在此对象监视器上等待的所有线程

### 停止线程

- **stop()停止**:   线程调用stop()方法会被暴力停止,方法已弃用,该方法会有不好的后果:
    1. 强制让线程停止有可能使一些清理性的工作得不到完成
    2. 对锁定的对象进行了**解锁**,导致数据得不到同步的处理,出现数据不一致的问题(比如一个方法加上了synchronized,并在其中进行了一个长时间的处理,而在处理结束之前该线程进行了`stop()`,则未完成的数据将没有进行到同步的处理)
- **异常法停止**: 线程调用interrupt()方法后,在线程的run方法中判断当前对象的interrupted状态,如果是中断状态则抛出异常,达到中断线程的效果
- **在沉睡中停止**:先将线程sleep,然后调用interrupt标记中断状态,interrupt会将阻塞状态的线程中断,会抛出中断异常,达到停止线程的效果

### CAS

-  CAS,是Compare and Swap的简称,在这个机制中有三个核心的参数
    - 主内存中存放的共享变量的值:V(一般情况下这个V是内存的地址值,通过这个地址可以获得内存中的值)
    - 工作内存中共享变量的副本值,也叫预期值:A
    - 需要将共享变量更新到的最新值:B
- 如果内存中的值与预期值一样,则更新为最新值
-  **ABA问题**
    - CAS需要在操作值的时候检查内存值是否发生变化,没有发生变化才会更新内存值,但是如果内存值原来是A,后来变成了B,然后又变成了A,那么CAS进行检查时会发现值没有发生变化,但是实际上是有变化的,ABA问题的解决思路就是在变量前面添加版本号,每次变量更新的时候都把版本号加一,这样变化过程就从`A－B－A`变成了`1A－2B－3A`
    - JDK从1.5开始提供了AtomicStampedReference类来解决ABA问题,具体操作封装在compareAndSet()中,compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等,如果都相等,则以原子方式将引用值和标志的值设置为给定的更新值
-  CAS操作在底层有一条对应的汇编指令,硬件直接支持,LOCK_IF_MP,如果是多个CPU,前面还要加一条LOCK指令,本身cmpxchg指令是没有原子性的,在多个cpu执行时,加上LOCK指令就是保证一个CPU执行后面的empxchg指令时,其他CPU不能执行,在硬件层,LOCK指令在执行后面指令的时候锁定一个北桥信号(不采用锁总线方式),所以CAS最终实现就是lock cmpxchg指令

### cyclicbarrier与countdownlatch区别

- CountDownLatch一般用于某个线程A等待若干个其他线程执行完任务之后,它才执行
- CyclicBarrier一般用于一组线程互相等待至某个状态,然后这一组线程再同时执行
- CountDownLatch是不能够重用的,而CyclicBarrier是可以重用的

### 多线程回调

- 所谓回调,就是客户程序C调用服务程序S中的某个方法A,然后S又在某个时候反过来调用C中的某个方法B,对于C来说,这个B方法便叫做回调方法框架

### 原子性,可见性,有序性

- **原子性**:能够保证同一时刻有且只有一个线程在操作共享数据,其他线程必须等该线程处理完数据后才能进行
- **可见性**:当一个线程在修改共享数据时,其他线程能够看到
- **有序性**:在Java中,JVM能够根据处理器特性(CPU多级缓存系统,多核处理器等)适当对机器指令进行重排序,最大限度发挥机器性能,Java中的指令重排序有两次,第一次发生在将字节码编译成机器码的阶段,第二次发生在CPU执行的时候,也会适当对指令进行重排

### JMM

- Java虚拟机规范中定义了一种Java内存模型(Java Memory Model,即JMM)来屏蔽掉各种硬件和操作系统的内存访问差异,以实现让Java程序在各种平台下都能达到一致的并发效果,Java内存模型的主要目标就是**定义程序中各个变量的访问规则,即在虚拟机中将变量存储到内存和从内存中取出变量这样的细节**
- JMM中规定所有的变量都存储在主内存(Main Memory)中,每条线程都有自己的工作内存(Work Memory),线程的工作内存中保存了该线程所使用的变量的从主内存中拷贝的副本,线程对于变量的读,写都必须在工作内存中进行,而不能直接读,写主内存中的变量,同时,本线程的工作内存的变量也无法被其他线程直接访问,必须通过主内存完成
- 关于主内存与工作内存之间的具体交互协议,即一个变量如何从主内存拷贝到工作内存,如何从工作内存同步到主内存之间的实现细节,Java内存模型定义了以下八种操作来完成:
    1. **lock(锁定)**:作用于主内存的变量,把一个变量标识为一条线程独占状态
    2. **unlock(解锁)**:作用于主内存变量,把一个处于锁定状态的变量释放出来,释放后的变量才可以被其他线程锁定
    3. **read(读取)**:作用于主内存变量,把一个变量值从主内存传输到线程的工作内存中,以便随后的load动作使用
    4. **load(载入)**:作用于工作内存的变量,它把read操作从主内存中得到的变量值放入工作内存的变量副本中
    5. **use(使用)**:作用于工作内存的变量,把工作内存中的一个变量值传递给执行引擎,每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作
    6. **assign(赋值)**:作用于工作内存的变量,它把一个从执行引擎接收到的值赋值给工作内存的变量,每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作
    7. **store(存储)**:作用于工作内存的变量,把工作内存中的一个变量的值传送到主内存中,以便随后的write的操作
    8. **write(写入)**:作用于主内存的变量,它把store操作从工作内存中一个变量的值传送到主内存的变量中

### volatile

- volatile关键字是用来保证有序性和可见性的
- **有序性**:`volatile`是通过编译器在生成字节码时,在指令序列中添加**内存屏障**来禁止指令重排序的
- 当对volatile变量执行写操作后,JMM会把工作内存中的最新变量值强制刷新到主内存写操作会导致其他线程中的缓存无效这样,其他线程使用缓存时,发现本地工作内存中此变量无效,便从主内存中获取,这样获取到的变量便是最新的值,实现了线程的可见性

### 锁

- **乐观锁与悲观锁**

    - 对于同一个数据的并发操作,悲观锁认为自己在使用数据的时候一定有别的线程来修改数据,因此在获取数据的时候会先加锁,确保数据不会被别的线程修改,Java中,synchronized关键字和Lock的实现类都是悲观锁

    - 而乐观锁认为自己在使用数据时不会有别的线程修改数据,所以不会添加锁,只是在更新数据的时候去判断之前有没有别的线程更新了这个数据,如果这个数据没有被更新,当前线程将自己修改的数据成功写入,如果数据已经被其他线程更新,则根据不同的实现方式执行不同的操作(例如报错或者自动重试)

    - 乐观锁在Java中是通过使用无锁编程来实现,最常采用的是CAS算法,Java原子类中的递增操作就通过CAS自旋实现的

- **公平锁与非公平锁**

    - 公平锁是指多个线程按照申请锁的顺序来获取锁,线程直接进入队列中排队,队列中的第一个线程才能获得锁,公平锁的优点是等待锁的线程不会饿死,缺点是整体吞吐效率相对非公平锁要低,等待队列中除第一个线程以外的所有线程都会阻塞,CPU唤醒阻塞线程的开销比非公平锁大
    - 非公平锁是多个线程加锁时直接尝试获取锁,获取不到才会到等待队列的队尾等待,但如果此时锁刚好可用,那么这个线程可以无需阻塞直接获取到锁,所以非公平锁有可能出现后申请锁的线程先获取锁的场景,非公平锁的优点是可以减少唤起线程的开销,整体的吞吐效率高,因为线程有几率不阻塞直接获得锁,CPU不必唤醒所有线程,缺点是处于等待队列中的线程可能会饿死,或者等很久才会获得锁

- **独享锁与共享锁**

    - 独享锁也叫排他锁,是指该锁一次只能被一个线程所持有,如果线程T对数据A加上排它锁后,则其他线程不能再对A加任何类型的锁,获得排它锁的线程即能读数据又能修改数据,JDK中的synchronized和JUC中Lock的实现类就是互斥锁
    - 共享锁是指该锁可被多个线程所持有,如果线程T对数据A加上共享锁后,则其他线程只能对A再加共享锁,不能加排它锁,获得共享锁的线程只能读数据,不能修改数据
    - 独享锁与共享锁也是通过AQS来实现的,通过实现不同的方法,来实现独享或者共享

- **可重入锁与非可重入锁**

    - 可重入锁又名递归锁,是指在同一个线程在外层方法获取锁的时候,再进入该线程的内层方法会自动获取锁(前提锁对象得是同一个对象或者class),不会因为之前已经获取过还没释放而阻塞,Java中ReentrantLock和synchronized都是可重入锁,可重入锁的优点是可以一定程度避免死锁

#### 死锁

- **避免死锁**
    1. 避免一个线程同时获得多个锁导致循环等待
    2. 尝试使用定时锁,使用`lock.tryLock ( timeOut )` ,当超时等待时当前线程不会堵塞,破坏了请求与保持条件
    3. 线程获取锁的顺序要一致,临界资源按顺序分配,破坏了循环等待条件
- **解决死锁**
    1. 使用`jps -l`定位进程号
    2. 使用`jstack`进程号,找到死锁问题并解决

#### synchronize

- synchronize可以用来修饰实例方法,静态方法,还有代码块,主要有三种作用:可以确保原子性,可见性,有序性
- synchronized的底层原理是跟monitor有关,也就是视图器锁,每个对象都有一个关联的monitor,当Synchronize获得monitor对象的所有权后会进行两个指令:加锁指令跟减锁指令
- monitor里面有个计数器,初始值是从0开始的,如果一个线程想要获取monitor的所有权,就查看它计数器是不是0
    - 如果是0的话,那么就说明没人获取锁,那么它就可以获取锁了,然后将计数器+1,也就是执行monitorenter加锁指令,monitorexit减锁指令是跟在程序执行结束和异常里的
    - 如果不是0的话,就会陷入一个堵塞等待的过程,直到为0等待结束
- synchronized是独占锁,同步块内的代码相当于同一时刻单线程执行,故不存在原子性和指令重排序的问题

#### AQS

- AQS核心思想是,如果被请求的共享资源空闲,那么就将当前请求资源的线程设置为有效的工作线程,将共享资源设置为锁定状态,如果共享资源被占用,就需要一定的阻塞等待唤醒机制来保证锁分配,这个机制主要用的是CLH队列的变体实现的,将暂时获取不到锁的线程加入到队列中
- AQS使用一个Volatile的int类型的成员变量来表示同步状态,通过内置的FIFO队列来完成资源获取的排队工作,通过CAS完成对State值的修改
    1. State初始化的时候为0,表示没有任何线程持有锁
    2. 当有线程持有该锁时,值就会在原来的基础上+1,同一个线程多次获得锁是,就会多次+1,这里就是可重入的概念
    3. 解锁也是对这个字段-1,一直到0,此线程对锁释放
- **加锁**
    - 通过ReentrantLock的加锁方法Lock进行加锁操作
    - 会调用到内部类Sync的Lock方法,由于Sync#lock是抽象方法,根据ReentrantLock初始化选择的公平锁和非公平锁,执行相关内部类的Lock方法,本质上都会执行AQS的Acquire方法
    - AQS的Acquire方法会执行tryAcquire方法,但是由于tryAcquire需要自定义同步器实现,因此执行了ReentrantLock中的tryAcquire方法,由于ReentrantLock是通过公平锁和非公平锁内部类实现的tryAcquire方法,因此会根据锁类型不同,执行不同的tryAcquire
    - tryAcquire是获取锁逻辑,获取失败后,会执行框架AQS的后续逻辑,跟ReentrantLock自定义同步器无关
- **解锁**
    - 通过ReentrantLock的解锁方法Unlock进行解锁
    - Unlock会调用内部类Sync的Release方法,该方法继承于AQS
    - Release中会调用tryRelease方法,tryRelease需要自定义同步器实现,tryRelease只在ReentrantLock中的Sync实现,因此可以看出,释放锁的过程,并不区分是否为公平锁
    - 释放成功后,所有处理由AQS框架完成,与自定义同步器无关

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2021-06-20-watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpc2hlbmc1MjE4,size_16,color_FFFFFF,t_70.png)

> Q:某个线程获取锁失败的后续流程是什么呢？
>
> A:存在某种排队等候机制,线程继续等待,仍然保留获取锁的可能,获取锁流程仍在继续
>
> Q:既然说到了排队等候机制,那么就一定会有某种队列形成,这样的队列是什么数据结构呢？
>
> A:是CLH变体的FIFO双端队列
>
> Q:如果处于排队等候机制中的线程一直无法获取锁,需要一直等待么？还是有别的策略来解决这一问题？
>
> A:线程所在节点的状态会变成取消状态,取消状态的节点会从队列中释放
>
> Q:Lock函数通过Acquire方法进行加锁,但是具体是如何加锁的呢？
>
> A:AQS的Acquire会调用tryAcquire方法,tryAcquire由各个自定义同步器实现,通过tryAcquire完成加锁过程

#### Lock

- Lock是一个接口,而synchronized是Java中的关键字,Lock 能完成synchronized所实现的所有功能
- synchronized在发生异常时,会自动释放线程占有的锁,因此不会导致死锁现象发生,而Lock在发生异常时,如果没有主动通过`unLock()`去释放锁,则很可能造成死锁现象,因此使用Lock时需要在finally块中释放锁
- Lock是显式锁(需要手动开启和关闭锁),synchronized是隐式锁,出了作用域自动释放
- Lock只有代码块锁,synchronized有代码块和方法锁
- Lock可以知道是不是已经获取到锁,而synchronized无法知道
- Lock可以让等待锁的线程响应中断,而synchronized却不行,使用synchronized时,等待的线程会一直等待下去,不能够响应中断

##### ReentrantLock

- `ReentrantLock`可以替代`synchronized`进行线程同步
- 必须先获取到锁,再进入`try {...}`代码块,最后使用`finally`保证释放锁
- 可以使用`tryLock()`尝试获取锁

##### ReadWriteLock

- 使用`ReadWriteLock`可以提高读取效率,读多写少的场景
- `ReadWriteLock`只允许一个线程写入,允许多个线程在没有写入时同时读取

### 线程池

- 线程池顾名思义就是事先创建若干个可执行的线程放入一个池(容器)中,需要的时候从池中获取线程不用自行创建,使用完毕不需要销毁线程而是放回池中,从而减少创建和销毁线程对象的开销
- 使用线程池可以降低资源消耗,提高响应速度,提高线程的可管理性,提供更多更强大的功能

#### 主要参数

- 线程池核心线程数大小
- 最大线程数
    1. CPU 密集型:最大线程数等于本机CPU线程数,可以保持CPU的效率最高`Runtime.getRuntime().availableProcessors()`
    2. IO 密集型:最大线程数大于 > 判断你程序中十分耗IO的线程,应为IO会导致线程阻塞
- 空闲线程存活时长
- 时间单位
- 阻塞队列
- 线程工厂
- 拒绝策略

#### 运行流程

- 当需要任务大于核心线程数时候,就开始把任务往存储任务的队列里,当存储队列满了的话,就开始增加线程池创建的线程数量,如果当线程数量也达到了最大,就开始执行拒绝策略,比如说记录日志,直接丢弃,或者丢弃最老的任务,或者交给提交任务的线程执行
- 当一个线程完成时,它会从队列中取下一个任务来执行,当一个线程无事可做,且超过一定的时间(keepAliveTime)时,如果当前运行的线程数大于核心线程数,那么这个线程会停掉了

#### 线程池种类

- `Executors`:工具类,线程池的工厂类,用于创建并返回不同类型的线程池,本质上是调用ThreadPoolExecutor的构造方法
    - newFixedThreadPool创建一个指定大小的线程池,每当提交一个任务就创建一个线程,如果工作线程数量达到线程池初始的最大数,则将提交的任务存入到等待队列中
    - newCachedThreadPool创建一个可缓存的线程池,这种类型的线程池特点是:
        - 工作线程的创建数量几乎没有限制(其实也有限制的,数目为Interger. MAX_VALUE), 这样可灵活的往线程池中添加线程
        - 如果长时间没有往线程池中提交任务,即如果工作线程空闲了指定的时间(默认为1分钟),则该工作线程将自动终止,终止后,如果你又提交了新的任务,则线程池重新创建一个工作线程
    - newSingleThreadExecutor创建一个单线程的Executor,即只创建唯一的工作者线程来执行任务,如果这个线程异常结束,会有另一个取代它,保证顺序执行
    - newScheduleThreadPool创建一个定长的线程池,而且支持定时的以及周期性的任务执行,类似于Timer

#### 线程池的submit和execute的区别

**execute提交的方式**

execute提交的方式只能提交一个Runnable的对象,且该方法的返回值是void,也即是提交后如果线程运行后,和主线程就脱离了关系了,当然可以设置一些变量来获取到线程的运行结果,并且当线程的执行过程中抛出了异常通常来说主线程也无法获取到异常的信息的,只有通过ThreadFactory主动设置线程的异常处理类才能感知到提交的线程中的异常信息

**submit提交的方式**

- submit提交的方式有如下三种情况

```java
<T> Future<T> submit(Callable<T> task);
```

- 这种提交的方式是提交一个实现了Callable接口的对象,这种提交的方式会返回一个Future对象,这个Future对象代表这线程的执行结果
- 当主线程调用Future的get方法的时候会获取到从线程中返回的结果数据
- 如果在线程的执行过程中发生了异常,get会获取到异常的信息

```java
Future<?> submit(Runnable task);
```

- 也可以提交一个Runable接口的对象,这样当调用get方法的时候,如果线程执行成功会直接返回null,如果线程执行异常会返回异常的信息

```java
<T> Future<T> submit(Runnable task, T result);
```

- 这种方式除了task之外还有一个result对象,当线程正常结束的时候调用Future的get方法会返回result对象,当线程抛出异常的时候会获取到对应的异常的信息

### ThreadLocal

- ThreadLocal的作用是提供线程内的局部变量,这种变量在线程的生命周期内起作用
- 因为一个线程内可以存在多个 ThreadLocal 对象,所以其实是 ThreadLocal 内部维护了一个 Map,这个Map的key是 ThreadLocal 类的实例对象,value为用户的值,ThreadLocalMap 是Thread的成员变量
- **内存泄漏问题**
    - 实际上 ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用,弱引用的特点是,如果这个对象只存在弱引用,那么在下一次垃圾回收的时候必然会被清理掉,这样一来 ThreadLocalMap中使用这个 ThreadLocal 的 key 也会被清理掉,但是,value 是强引用,不会被清理,这样一来就会出现 key 为 null 的 value
    - ThreadLocalMap实现中已经考虑了这种情况,在调用 set(),get(),remove() 方法的时候,会清理掉 key 为 null 的记录,如果说会出现内存泄漏,那只有在出现了 key 为 null 的记录后,没有手动调用 remove() 方法,并且之后也不再调用 get(),set(),remove()
    - 所以尽量在代码中使用finally块进行回收

### 多线程循环打印ABC

- 3个线程A,B,C分别打印三个字母,每个线程循环10次,首先同步,如果不满足打印条件,则调用wait()函数一直等待,之后打印字母,更新state,调用notifyAll(),进入下一次循环

```java
public class PrintABC {
    private static final int PRINT_A = 0;
    private static final int PRINT_B = 1;
    private static final int PRINT_C = 2;

    private static class MyThread extends Thread {
        int which; // 0:打印A,1:打印B,2:打印C
        static volatile int state; // 线程共有,判断所有的打印状态
        static final Object t = new Object();

        public MyThread(int which) {
            this.which = which;
        }

        @Override
        public void run() {
            for (int i = 0; i < 10; i++) {
                synchronized (t) {
                    while (state % 3 != which) {
                        try {
                            t.wait();
                        } catch (InterruptedException e) {
                            e.printStackTrace();
                        }
                    }
                    System.out.print(toABC(which)); // 执行到这里,表明满足条件,打印
                    state++;
                    t.notifyAll(); // 调用notifyAll方法
                }
            }
        }
    }

    public static void main(String[] args) {
        new MyThread(PRINT_A).start();
        new MyThread(PRINT_B).start();
        new MyThread(PRINT_C).start();
    }

    private static char toABC(int which) {
        return (char) ('A' + which);
    }
}
```



### 如何实现主线程等待子线程执行完后再继续执行？

1. 可以使用`join()`方法,在主线程内部调用子线程`join()`方法
2. CountDownLatch实现
    - `await()`方法阻塞当前线程,直到计数器等于0
    - `countDown()`方法将计数器减一

## JVM

### GC

- 垃圾回收可以有效的防止内存泄露,有效的使用可以使用的内存,垃圾回收器通常是作为一个单独的低优先级的线程运行,不可预知的情况下对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收,程序员不能实时的调用垃圾回收器对某个对象或所有对象进行垃圾回收

#### 与垃圾回收相关的JVM参数

> -Xms / -Xmx — 堆的初始大小 / 堆的最大大小
>
> -Xmn — 堆中年轻代的大小
> -XX:-DisableExplicitGC — 让System.gc()不产生任何作用
> -XX:+PrintGCDetails — 打印GC的细节
> -XX:+PrintGCDateStamps — 打印GC操作的时间戳
> -XX:NewSize / XX:MaxNewSize — 设置新生代大小/新生代最大大小
> -XX:NewRatio — 可以设置老生代和新生代的比例
> -XX:PrintTenuringDistribution — 设置每次新生代GC后输出幸存者乐园中对象年龄的分布
> -XX:InitialTenuringThreshold / -XX:MaxTenuringThreshold:设置老年代阀值的初始值和最大值
> -XX:TargetSurvivorRatio:设置幸存区的目标使用率

#### 垃圾回收的流程

- 首先有三个代,新生代,老年代,永久代
- 在新生代有三个区域:一个Eden区和两个Survivor区,当一个实例被创建了,首先会被存储Eden 区中
- 具体过程是这样的:
    - 一个对象实例化时,先去看Eden区有没有足够的空间
    - 如果有,不进行垃圾回收,对象直接在Eden区存储
    - 如果Eden区内存已满,会进行一次minor gc
    - 然后再进行判断Eden区中的内存是否足够
    - 如果不足,则去看Survivor区的内存是否足够
    - 如果内存足够,把Eden区部分活跃对象保存在Survivor区,然后把对象保存在Eden区
    - 如果内存不足,查询老年代的内存是否足够
    - 如果老年代内存足够,将部分Survivor区的活跃对象存入老年代,然后把Eden区的活跃对象放入Survivor区,对象依旧保存在Eden区
    - 如果老年代内存不足,会进行一次full gc,之后老年代会再进行判断 内存是否足够,如果足够 还是那些步骤
    - 如果不足,会抛出OutOfMemoryError(内存溢出异常)

#### GC算法

- **标记-清除算法**:遍历 `GC Roots`,然后将所有 `GC Roots` 可达的对象标记,将没有标记的对象全部清除掉
- **复制算法**:它将可用内存按容量划分为大小相等的两块,每次只使用其中的一块,当这一块内存用完,需要进行垃圾收集时,就将存活者的对象复制到另一块上面,然后将第一块内存全部清除
- **标记-整理算法**:遍历 `GC Roots`,然后将存活的对象标记,移动所有存活的对象,且按照内存地址次序依次排列,然后将末端内存地址以后的内存全部回收

#### 垃圾回收器

- CMS 垃圾收集器以获取最短回收停顿时间为目标的收集器(追求低停顿),它在垃圾收集时使得用户线程和 GC 线程并发执行,因此在垃圾收集过程中用户也不会感到明显的卡顿
    - 初始标记:Stop The World,仅使用一条初始标记线程对所有与 GC Roots 直接关联的对象进行标记
    - 并发标记:使用**多条**标记线程,与用户线程并发执行,此过程进行可达性分析,标记出所有废弃对象,速度很慢
    - 重新标记:Stop The World,使用多条标记线程并发执行,将刚才并发标记过程中新出现的废弃对象标记出来
    - 并发清除:只使用一条 GC 线程,与用户线程并发执行,清除刚才标记的对象,这个过程非常耗时
- G1 通用垃圾收集器是一款面向服务端应用的垃圾收集器,它没有新生代和老年代的概念,而是将堆划分为一块块独立的 Region,当要进行垃圾收集时,首先估计每个 Region 中垃圾的数量,每次都从垃圾回收价值最大的 Region 开始回收,因此可以获得最大的回收效率
    - 从整体上看,G1 是基于"标记-整理”算法实现的收集器,从局部(两个 Region 之间)上看是基于"复制”算法实现的,这意味着运行期间不会产生内存空间碎片
    - 可以非常精确控制停顿时间,在不牺牲吞吐量前提下,实现低停顿垃圾回收
    - 如果不计算维护 Remembered Set 的操作,G1 收集器的工作过程分为以下几个步骤:
        - 初始标记:Stop The World,仅使用一条初始标记线程对所有与 GC Roots 直接关联的对象进行标记
        - 并发标记:使用**一条**标记线程与用户线程并发执行,此过程进行可达性分析,速度很慢
        - 最终标记:Stop The World,使用多条标记线程并发执行
        - 筛选回收:回收废弃对象,此时也要 Stop The World,并使用多条筛选回收线程并发执行

#### 可达性分析法

- 所有和 GC Roots 直接或间接关联的对象都是有效对象,和 GC Roots 没有关联的对象就是无效对象
- GC Roots 是指:
    - Java 虚拟机栈(栈帧中的本地变量表)中引用的对象
    - 本地方法栈中引用的对象
    - 方法区中常量引用的对象
    - 方法区中类静态属性引用的对象
- GC Roots 并不包括堆中对象所引用的对象,这样就不会有循环引用的问题

#### 引用类型

- **强引用**:类似`Object obj = new Object()`这类的引用,就是强引用
- **软引用**:软引用是一种相对强引用弱化一些的引用,可以让对象豁免一些垃圾收集,只有当 JVM 认为内存不足时,才会去试图回收软引用指向的对象,JVM 会确保在抛出`OutOfMemoryError`之前,清理软引用指向的对象,软引用通常用来**实现内存敏感的缓存**,如果还有空闲内存,就可以暂时保留缓存,当内存不足时清理掉,这样就保证了使用缓存的同时,不会耗尽内存
- **弱引用**:弱引用的**强度比软引用更弱**一些,当 JVM 进行垃圾回收时,**无论内存是否充足,都会回收**只被弱引用关联的对象
- **虚引用**:虚引用也称幽灵引用或者幻影引用,它是**最弱**的一种引用关系,一个对象是否有虚引用的存在,完全不会对其生存时间构成影响,它仅仅是提供了一种确保对象被 finalize 以后,做某些事情的机制,比如,通常用来做所谓的 Post-Mortem 清理机制

#### 什么时候对象从新生代转移到老年代

- Eden区满时,进行Minor GC时
- 如果新创建的对象占用内存很大,则直接分配到老年代
- 虚拟机对每个对象定义了一个对象年龄(Age)计数器,当年龄增加到一定的临界值时,就会晋升到老年代中
- 如果在Survivor区中相同年龄的对象的所有大小之和超过Survivor空间的一半,包括比这个年龄大的对象就都可以直接进入老年代

#### 新生代2个Survivor区的好处

解决了内存碎片化问题,整个过程中,永远有一个Survivor区是空的,另一个非空的Survivor区是无碎片的

#### Full GC

- **System.gc()方法的调用**:此方法的调用是建议JVM进行Full GC,虽然只是建议而非一定,但很多情况下它会触发 Full GC,从而增加Full GC的频率,也即增加了间歇性停顿的次数,强烈影响系建议能不使用此方法就别使用,让虚拟机自己去管理它的内存,可通过通过`-XX:+ DisableExplicitGC`来禁止RMI调用System.gc
- **老年代空间不足**:老年代空间只有在新生代对象转入以及创建为大对象,大数组时才会出现不足的现象,抛出`java.lang.OutOfMemoryError: Java heap space`,为避免以上两种状况引起的Full GC,调优时应尽量做到让对象在Minor GC阶段被回收,让对象在新生代多存活一段时间及不要创建过大的对象及数组
- **方法区空间不足**:Permanet Generation中存放的为一些class的信息,常量,静态变量等数据,当系统中要加载的类,反射的类和调用的方法较多时,Permanet Generation可能会被占满,在未配置为采用CMS GC的情况下也会执行Full GC,如果经过Full GC仍然回收不了,那么JVM会抛出如下错误信息:`java.lang.OutOfMemoryError: PermGen space`,为避免Perm Gen占满造成Full GC现象,可采用的方法为增大Perm Gen空间或转为使用CMS GC

### 遇到过OOM怎么解决

- 最常见的OOM情况有以下三种:
    1. `java.lang.OutOfMemoryError: Java heap space`:Java堆内存溢出,此种情况最常见,一般由于内存泄露或者堆的大小设置不当引起,对于内存泄露,需要通过内存监控软件查找程序中的泄露代码,而堆大小可以通过虚拟机参数`-Xms`,`-Xmx`等修改
    2. `java.lang.OutOfMemoryError: PermGen space`:Java永久代溢出,即方法区溢出了,一般出现于大量Class或者jsp页面,或者采用cglib等反射机制的情况,因为上述情况会产生大量的Class信息存储于方法区,此种情况可以通过更改方法区的大小来解决,使用类似`-XX:PermSize=64m -XX:MaxPermSize=256m`的形式修改,另外,过多的常量尤其是字符串也会导致方法区溢出
    3. `java.lang.StackOverflowError`:不会抛OOM error,但也是比较常见的Java内存溢出,JAVA虚拟机栈溢出,一般是由于程序中存在死循环或者深度递归调用造成的,栈大小设置太小也会出现此种溢出,可以通过虚拟机参数`-Xss`来设置栈的大小
- **OOM分析--heapdump**:获取dump堆的内存镜像,可以采用如下两种方式:
    1. 设置JVM参数`-XX:+HeapDumpOnOutOfMemoryError`,设定当发生OOM时自动dump出堆信息,不过该方法需要JDK5以上版本
    2. 使用JDK自带的jmap命令,`jmap -dump:format=b,file=heap.bin <pid>`其中pid可以通过jps获取
- 得到dump堆内存信息后,需要对dump出的文件进行分析,从而找到OOM的原因,常用的工具有:
    1. **mat: eclipse memory analyzer**, 基于eclipse RCP的内存分析工具
    2. **jhat**:JDK自带的java heap analyze tool,可以将堆中的对象以html的形式显示出来,包括对象的数量,大小等等,并支持对象查询语言OQL,分析相关的应用后,可以通过http://localhost:7000来访问分析结果,不推荐使用,因为在实际的排查过程中,一般是先在生产环境 dump出文件来,然后拉到自己的开发机器上分析,所以,不如采用高级的分析工具比如前面的mat来的高效

### 内存泄漏

- JVM是使用引用计数法和可达性分析算法来判断对象是否是不再使用的对象,**本质都是判断一个对象是否还被引用**,那么对于这种情况下,由于代码的实现不同就会出现很多种内存泄漏问题(让JVM误以为此对象还在引用中,无法回收,造成内存泄漏)
- **静态集合类**,如HashMap,LinkedList等等,如果这些容器为静态的,那么它们的生命周期与程序一致,则容器中的对象在程序结束之前将不能被释放,从而造成内存泄漏,简单而言,长生命周期的对象持有短生命周期对象的引用,尽管短生命周期的对象不再使用,但是因为长生命周期对象持有它的引用而导致不能被回收
- **各种连接,如数据库连接,网络连接和IO连接等**,在对数据库进行操作的过程中,首先需要建立与数据库的连接,当不再使用时,需要调用close方法来释放与数据库的连接,只有连接被关闭后,垃圾回收器才会回收对应的对象,否则,如果在访问数据库的过程中,对Connection,Statement或ResultSet不显性地关闭,将会造成大量的对象无法被回收,从而引起内存泄漏
- **变量不合理的作用域**,一般而言,一个变量的定义的作用范围大于其使用范围,很有可能会造成内存泄漏,另一方面,如果没有及时地把对象设置为null,很有可能导致内存泄漏的发生

```java
public class UsingRandom {
    private String msg;
    public void receiveMsg(){
        readFromNet();// 从网络中接受数据保存到msg中
        saveDB();// 把msg保存到数据库中
    }
}
```

- **如上面这个伪代码**,通过readFromNet方法把接受的消息保存在变量msg中,然后调用saveDB方法把msg的内容保存到数据库中,此时msg已经就没用了,由于msg的生命周期与对象的生命周期相同,此时msg还不能回收,因此造成了内存泄漏,实际上这个msg变量可以放在receiveMsg方法内部,当方法使用完,那么msg的生命周期也就结束,此时就可以回收了,还有一种方法,在使用完msg后,把msg设置为null,这样垃圾回收器也会回收msg的内存空间
- **内部类持有外部类**,如果一个外部类的实例对象的方法返回了一个内部类的实例对象,这个内部类对象被长期引用了,即使那个外部类实例对象不再被使用,但由于内部类持有外部类的实例对象,这个外部类对象将不会被垃圾回收,这也会造成内存泄露
- **改变哈希值**,当一个对象被存储进HashSet集合中以后,就不能修改这个对象中的那些参与计算哈希值的字段了,否则,对象修改后的哈希值与最初存储进HashSet集合中时的哈希值就不同了,在这种情况下,即使在contains方法使用该对象的当前引用作为的参数去HashSet集合中检索对象,也将返回找不到对象的结果,这也会导致无法从HashSet集合中单独删除当前对象,造成内存泄露

### JVM内存结构

- Java 虚拟机的内存空间分为 5 个部分:
    - **程序计数器**
    - **Java 虚拟机栈**:Java 虚拟机栈会为每一个即将运行的 Java 方法创建一块叫做"栈帧”的区域,用于存放该方法运行过程中的一些信息,如:
        - 局部变量表
            - 存放基本变量类型(会包含这个基本类型的基本数值)
            - 引用对象的变量(会存放这个引用在堆里面的具体地址)
        - 操作数栈
        - 动态链接
        - 方法出口信息
    - **本地方法栈**
    - **堆**
        - 存放new的对象和数组
        - 可以被所有的线程共享,不会存放别的对象引用
    - **方法区**:方法区逻辑上属于堆的一部分,但是为了与堆进行区分,通常又叫"非堆”
        - 已经被虚拟机加载的类信息
        - 常量
        - 静态变量
        - 即时编译器编译后的代码

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2021-03-14-jvm-memory-structure.jpg" alt="jvm-memory-structure" style="zoom: 67%;" />

**注意**:JDK 1.8 同 JDK 1.7 比,最大的差别就是:元空间(元数据区)取代了永久代,元空间的本质和永久代类似,都是对 JVM 规范中方法区的实现,不过元空间与永久代之间最大的区别在于:元空间并不在虚拟机中,而是使用本地内存

### 类的加载过程

1. 加载:查找和导入Class文件
2. 校验:检查载入Class文件数据的正确性
3. 准备:给类的静态变量分配存储空间
4. 解析:将符号引用转成直接引用
5. 初始化:执行类构造器 `<clinit>()` 方法,对类的静态变量,静态代码块执行初始化操作

> **类初始化的时机**
>
> - **类的主动引用**(一定会发生类的初始化)
>     - 当虚拟机启动,先初始化main方法所在的类
>     - new一个类的对象
>     - 调用类的静态成员(除final常量)和静态方法
>     - 使用`java.lang.reflect`包的方法对类进行反射调用
>     - 当初始化一个类,如果其父类没有被初始化,则会先初始化它的父类
> - **类的被动引用**(不会发生类的初始化)
>     - 当访问一个静态域时,只有真正声明这个域的类才会被初始化,如:当通过子类引用父类的静态变量,不会导致子类初始化
>     - 通过数组定义类引用,不会触发此类的初始化
>     - 引用常量不会触发此类的初始化(常量在链接阶段就存入调用类的常量池中)

#### 加载class文件的原理机制

- 类的加载是由类加载器完成的,类加载器包括:根加载器(BootStrap),扩展加载器(Extension),系统加载器(System)和用户自定义类加载器(java.lang.ClassLoader的子类)
- 类加载过程采取了双亲委托机制,更好的保证了Java平台的安全性,在该机制中,JVM自带的Bootstrap是根加载器,其他的加载器都有且仅有一个父类加载器,类的加载首先请求父类加载器加载,父类加载器无能为力时才由其子类加载器自行加载,JVM不会向Java程序提供对Bootstrap的引用

> **说明**:
>
> Bootstrap:一般用本地代码实现,负责加载JVM基础核心类库(rt.jar)
> Extension:从java.ext.dirs系统属性所指定的目录中加载类库,它的父加载器是Bootstrap
> System:又叫应用类加载器,其父类是Extension,它是应用最广泛的类加载器,它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类,是用户自定义加载器的默认父加载器

#### 类的实例化过程

- **类加载检查**:虚拟机遇到一条new指令时,虚拟机首先会去方法区的类常量池中定位到这个类对象的符号引用,并且检查这个符号引用代表的类是否已被加载,解析和初始化过,如果没有,那必须先执行相应的类加载过程
- **分配内存**:在类加载检查通过后,接下来虚拟机将为新生对象分配内存,对象所需内存的大小在类加载完成后便可以完全确定,为对象分配空间的任务等同于把一块确定大小的内存从java堆中划分出来
- **堆中分配内存的两种方法**:这里需要注重说明一下:java堆的内存分配方式是取决于垃圾回收器是否带有压缩整理功能决定的,在使用Serial,ParNew等带Compact过程的收集器时,系统采用的分配算法是指针碰撞,而使用CMS这种基于Mark-Sweep算法的收集器时,通常采用空闲列表方式
    - **指针碰撞**:假设java堆中内存是完全规整的,所有用过的内存都放到一边,空闲的内存放在另一边,中间放着一个指针作为分界点的指示器,那所分配的内存就是仅仅把那个指针向空闲的空间那边挪动一段与对象大小相同的距离
    - **空闲列表**:假设java堆中的内存不是规整的,已使用的内存和空闲的内存是相互交错的,那就没有办法通过简单的指针碰撞分配内存了,虚拟机就必须维护一个列表,记录上那些内存块是可用的,在分配的时候从列表中找到一块足够大的空间划分给对象实例,并更新列表上的记录

### 静态变量什么时候会被回收

- 静态变量是在类被load的时候分配内存的,并且存在于方法区,当类被卸载的时候,静态变量被销毁
- 类在什么时候被卸载,在进程结束的时候,一般情况下,所有的类都是默认的ClassLoader加载的,只要ClassLoader存在,类就不会被卸载,而默认的ClassLoader生命周期是与进程一致的

## MySQL

### 数据库建表三大范式

1. **原子性**：要求属性具有原子性,不可再分解
2. **唯一性**：要求记录有唯一标识,即实体的唯一性,即不存在部分依赖
3. **消除冗余性**：要求任何字段不能由其他字段派生出来,它要求字段没有冗余,即不存在传递依赖

### Druid连接池

1. 强大的监控特性,通过Druid提供的监控功能,可以清楚知道连接池和SQL的工作情况
    1.  监控SQL的执行时间,ResultSet持有时间,返回行数,更新行数,错误次数,错误堆栈信息;
    2.  SQL执行的耗时区间分布,什么是耗时区间分布呢？比如说,某个SQL执行了1000次,其中`0~1`毫秒区间50次,`1~10`毫秒800次,`10~100`毫秒100次,`100~1000`毫秒30次,`1~10`秒15次,10秒以上5次,通过耗时区间分布,能够非常清楚知道SQL的执行耗时情况
    3.  监控连接池的物理连接创建和销毁次数,逻辑连接的申请和关闭次数,非空等待次数,PSCache命中率等
2. 其次,方便扩展,Druid提供了Filter-Chain模式的扩展API,可以自己编写Filter拦截JDBC中的任何方法,可以在上面做任何事情,比如说性能监控,SQL审计,用户名密码加密,日志等等
3. Druid集合了开源和商业数据库连接池的优秀特性,并结合阿里巴巴大规模苛刻生产环境的使用经验进行优化

### 存储引擎

- InnoDB支持事务,MyISAM不支持
- InnoDB支持行级锁而MyISAM仅仅支持表锁,但是InnoDB可能出现死锁
- InnoDB的关注点在于:并发写,事务,更大资源,而MyISAM的关注点在于:节省资源,消耗少,简单业务
- 在MySQL5.7的时候,默认就是InnoDB作为默认的存储引擎了

### 数据库主键的选择

- **UUID**
    - **占用空间大**：UUID有128位相较于自增主键长度较大，UUID存储时可能会出现节点分裂,导致节点多了,但是每个节点的数据量少了,存储到文件系统中时,无论节点中数据是不是满的都会占用一页的空间
    - UUID是无序的,作为主键会涉及大量索引重排，降低效率
- **自增主键**
    - 在水平分表时，由于自增主键必须连续，只能采用范围分片的方式，会产生**尾部热点**效应
- **雪花算法**
    - 以时间作为依据，结合机器ID，以及12位序列，生成64位数据，生成的数据保证是唯一有序的

### EXPLAIN

- 使用EXPLAIN关键字可以模拟优化器执行SQL查询语句,从而知道MySQL是如何处理你的SQL语句的,分析查询语句或是表结构的性能瓶颈

```
Explain + SQL语句
```

- 通过Explain,我们可以获取以下信息:
    - 表的读取顺序
    - 哪些索引可以使用
    - 数据读取操作的操作类型
    - **哪些索引被实际使用**
    - 表之间的引用
    - 每张表有多少行被物理查询

### 数据库优化

- 选取最适用的字段属性
- 使用连接查询代替子查询
- 选择表合适存储引擎
- 对查询进行优化,应尽量避免全表扫描,首先应考虑在 WHERE 及 ORDER BY 涉及的列上建立索引
- 避免索引失效
- 使用反范式表避免联表查询效率低的问题
- 使用分表策略

### 慢查询优化基本步骤

1. 先运行看看是否真的很慢,注意设置SQL_NO_CACHE
2. where条件单表查,锁定最小返回记录表,这句话的意思是把查询语句的where都应用到表中返回的记录数最小的表开始查起,单表每个字段分别查询,看哪个字段的区分度最高
3. explain查看执行计划,是否与2预期一致(从锁定记录较少的表开始查询)
4. order by limit 形式的sql语句让排序的表优先查
5. 了解业务方使用场景
6. 加索引时参照建索引的几大原则
7. 观察结果,不符合预期继续从头分析

### 分表策略

- **垂直拆分**:表数据拆分到不同表中,单表大数据量依然存在性能瓶颈,垂直拆分就是要把表按模块划分到不同数据库表中,相对于垂直切分更进一步的是服务化改造,说得简单就是要把原来强耦合的系统拆分成多个弱耦合的服务,通过服务间的调用来满足业务需求看,因此表拆出来后要通过服务的形式暴露出去,而不是直接调用不同模块的表
- **水平拆分**:行数据拆分到不同表中,上面谈到垂直切分只是把表按模块划分到不同数据库,但没有解决单表大数据量的问题,而水平切分就是要把一个表按照某种规则把数据划分到不同表或数据库里,例如像计费系统,通过按时间来划分表就比较合适,因为系统都是处理某一时间段的数据,而像SaaS应用,通过按用户维度来划分数据比较合适,因为用户与用户之间的隔离的,一般不存在处理多个用户数据的情况,简单的按user_id范围来水平切分

### 事务

- 事务就是将一组SQL语句放在同一批次内去执行,如果一个SQL语句出错,则该批次内的所有SQL都将被取消执行

#### ACID

- **原子性(Atomicity)**:整个事务中的所有操作,要么全部完成,要么全部不完成,不可能停滞在中间某个环节,事务在执行过程中发生错误,会被回滚(ROLLBACK)到事务开始前的状态,就像这个事务从来没有执行过一样
- **一致性(Consistency)**:一个事务可以封装状态改变(除非它是一个只读的),事务必须始终保持系统处于一致的状态,不管在任何给定的时间并发事务有多少,也就是说:如果事务是并发多个,系统也必须如同串行事务一样操作
- **隔离性(Isolation)**:隔离状态执行事务,使它们好像是系统在给定时间内执行的唯一操作,如果有两个事务,运行在相同的时间内,执行相同的功能,事务的隔离性将确保每一事务在系统中认为只有该事务在使用系统,这种属性有时称为串行化,为了防止事务操作间的混淆,必须串行化或序列化请求,使得在同一时间仅有一个请求用于同一数据
- **持久性(Durability)**:在事务完成以后,该事务对数据库所作的更改便持久的保存在数据库之中,并不会被回滚

#### 隔离级别

- **并发下遇到的问题**
    - **脏读(Dirty Read)**:A事务读取B事务尚未提交的数据并在此基础上操作,而B事务执行回滚,那么A读取到的数据就是脏数据
    - **不可重复读(Unrepeatable Read)**:事务A重新读取前面读取过的数据,发现该数据已经被另一个已提交的事务B修改过了
    - **幻读(Phantom Read)**:事务A重新执行一个查询,返回一系列符合查询条件的行,发现其中插入了被事务B提交的行
    - **第1类丢失更新**:事务A撤销时,把已经提交的事务B的更新数据覆盖了
    - **第2类丢失更新**:事务A覆盖事务B已经提交的数据,造成事务B所做的操作丢失
    - 不可重复读的和幻读很容易混淆,不可重复读重点在于update和delete,而幻读的重点在于insert,解决不可重复读的问题只需锁住满足条件的行,解决幻读需要锁表

- **隔离级别**:数据库为了维护事务的隔离性,数据库通常会通过锁机制来解决数据并发访问问题,直接使用锁是非常麻烦的,为此数据库为用户提供了自动锁机制,只要用户指定会话的事务隔离级别,数据库就会通过分析SQL语句然后为事务访问的资源加上合适的锁
    - **Read uncommitted**:读未提交,可能出现脏读,不可重复读,幻读
    - **Read committed**:读提交,可能出现不可重复读,幻读
    - **Repeatable read**:可重复读,可能出现幻读
    - **Serializable**:可串行化,避免幻读
    - 事务隔离级别和数据访问的并发性是对立的,事务隔离级别越高并发性就越差,所以要根据具体的应用来确定合适的事务隔离级别

- **隔离级别与锁的关系**
    - **未提交读(Read Uncommitted)**:事务在读数据的时候采用当前读，事务在修改数据的时候加共享锁，提交后释放（解决了修改时，数据被删除或修改的情况）
    - **提交读(Read Committed)**:事务在读数据的时候加读锁（当前读），读完即释放共享锁，事务在修改某数据时会加上写锁，直到事务结束再释放，这样的机制保证了RC隔离级别不会发生脏读，只有提交过的事务，才能被其他事务看见
    - **可重复读(Repeatable Read)**:事务在读数据的时候采用的是快照读（解决不可重复读问题），事务在修改数据的时候加写锁，并且MySQL采用了间隙锁，但触发间隙锁的前提是（查询条件列不可以是唯一索引和主键），在触发间隙锁后，会锁住一定范围内的数据，防止在这范围内插入数据，这个机制可以在一定程度上降低发生幻读的可能。
    - **可串行化(Serializable)**:事务在读取数据时，对整个表加读锁，提交或回滚事务后释放，事务在修改数据时，对整个表加写锁，提交或回滚事务后释放

#### 编程式事务

- Connection提供了事务处理的方法,通过调用setAutoCommit(false)可以设置手动提交事务,当事务完成后用`commit()`显式提交事务如果在事务处理过程中发生异常则通过`rollback()`进行事务回滚
- 除此之外,从JDBC 3.0中还引入了Savepoint(保存点)的概念,允许通过代码设置保存点并让事务回滚到指定的保存点

### 锁

- MyISAM支持表锁,InnoDB支持表锁和行锁,默认行锁
    - **表级锁**:开锁小,加锁快,不会出现死锁,锁的粒度大,发生锁冲突的概率最高,并发量最低
    - **行级锁**:开销大,加锁慢,会出现死锁,锁的粒度小,容易发生冲突的概率小,并发度最高,innoDB支持三种行锁定方式
        - 行锁(Record Lock):锁直接加在索引记录上面,无索引项时演变成表锁(因为如果一个条件无法通过索引快速过滤,存储引擎层面就会将所有记录加锁后返回,再由MySQL Server层进行过滤)
        - 间隙锁(Gap Lock):锁定索引记录间隙,确保索引记录的间隙不变,在无索引的情况下是锁全表，间隙锁是针对事务隔离级别为可重复读或以上级别的
        - Next-Key Lock:行锁和间隙锁组合起来就是 Next-Key Lock

### MVCC

>- **快照读**：简单的select操作，属于快照读，不加锁。 `select * from table where ?;`
>- **当前读**：特殊的读操作，插入/更新/删除操作，属于当前读，需要加锁。
>    - ` select * from table where ? lock in share mode;`
>    - `select * from table where ? for update; `
>    - `insert into table values (…);`
>    - `update table set ? where ?;`
>    - `delete from table where ?;`

- MVCC 实现可重复读下的快照读的幻读问题，而当前读的幻读问题是由next-lock实现的
- 在InnoDB中,会在每行数据后添加两个额外的隐藏的值来实现MVCC
    - DB_TRX_ID ： 跟事务相关的，用来标识最近一次对本行记录做修改的id（即事务id）
    - DB_ROLL_PRT：回滚指针，写入日志的信息（在日志中记录这修改之前的数据方便回滚）
- **undoLog**：undoLog 分为两种。
    - 一种是insertUndoLog，此日志表示事务对insert新记录产成时需要的日志记录，主要是发生回滚时需要，事务添加成功提交后即可丢弃。
    - 另一种是updateUndoLog，事务对数据delete或者update时所产生的undolog，不仅在回滚时需要，快照读时也需要，所以不能随便删除。只有当数据库中的快照读不涉及该日志记录，该回滚记录才会被线程删除。
- **MVCC 通过undoLog实现了数据的多个版本，每个版本之间都通过DB_ROLL_PRT进行连接，在进行快照读的时候会生成ReadView对象（RR下会复用同一个ReadView），根据规则构建出满足当前隔离级别的一致性视图**
- 如果在两次快照读中间穿插当前读，会导致ReadView重新生成，从而无法避免幻读问题
- 在RR事务隔离级别下:
    - Select的时候，读取创建版本号<=当前版本号，删除版本号为空，或者是当前版本号大于当前删除版本号的时候，则会把数据读出。
        - InnoDB 只查找版本小于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。
        - 行的删除版本要么未定义，要么大于当前事务版本号，这可以确保事务读取到的行在事务开始之前未被删除。
    - Insert的时候，保存当前事务的版本号为创建版本号。
    - Update的时候，插入一条新的记录，保存当前的版本号为创建版本号，同时当前版本号保存为原来数据的删除版本号。
    - Delete的时候，保存当前版本号为删除版本号。

### 数据库连接池

- 由于创建连接和释放连接都有很大的开销(尤其是数据库服务器不在本地时,为了提升系统访问数据库的性能,可以事先创建若干连接置于连接池中,需要时直接从连接池获取,使用结束时归还连接池而不必关闭连接,从而避免频繁创建和释放连接所造成的开销
- 这是典型的用空间换取时间的策略(浪费了空间存储连接,但节省了创建和释放连接的时间)
- 池化技术在Java开发中是很常见的,在使用线程时创建线程池的道理与此相同,基于Java的开源数据库连接池主要有:C3P0,Hikari ,DBCP,BoneCP,Druid等

### SQL 语句执行过程

1. 应用程序发现 SQL 到服务端
    - 当执行 SQL 语句时,应用程序会连接到相应的数据库服务器,然后服务器对 SQL 进行处理
2. 查询缓存
    - 接着数据库服务器会先去查询是否有该 SQL 语句的缓存,key 是查询的语句,value 是查询的结果,如果你的查询能够直接命中,就会直接从缓存中拿出 value 来返回客户端
    - **注意**:查询不会被解析,不会生成执行计划,不会被执行
3. 查询优化处理,生成执行计划
    - 如果没有命中缓存,则开始第三步
    - **解析 SQL**:生成解析树,验证关键字如 select,where,left join 等是否正确
    - **预处理**:进一步检查解析树是否合法,如检查数据表和列是否存在,验证用户权限等
    - **优化 SQL**:决定使用哪个索引,或者在多个表相关联的时候决定表的连接顺序,紧接着,将 SQL 语句转成执行计划
4. 将查询结果返回客户端
    - 最后,数据库服务器将查询结果返回给客户端,如果查询可以缓存,MySQL 也会将结果放到查询缓存中

### 日志

- Mysql 有4种类型的日志:Error Log,Genaral Query Log,Binary Log 和 Slow Query Log
- **Error Log**:记录Mysql运行过程中的Error,Warning,Note等信息,系统出错或者某条记录出问题可以查看Error日志
- **General Query Log**:记录mysql的日常日志,包括查询,修改,更新等的每条sql
- **Binary Log**:二进制日志,包含一些事件,这些事件描述了数据库的改动,如建表,数据改动等,主要用于备份恢复,回滚操作等
    - **STATMENT**:每一条会修改数据的sql都会记录到master的binlog中,slave在复制的时候sql进程会解析成和原来master端执行多相同的sql再执行
    - **ROW**:日志中会记录成每一行数据被修改的形式,然后在slave端再对相同的数据进行修改,只记录要修改的数据,只有value,不会有sql多表关联的情况
    - **MIXED**:MySQL 会根据执行的每一条具体的 SQL 语句来区分对待记录的日志形式,也就是在 statement 和 row 之间选择一种
- **Slow Query Log**:记录Mysql 慢查询的日志

### 主从同步

- Mysql服务器之间的主从同步是基于二进制日志机制,主服务器使用二进制日志来记录数据库的变动情况,从服务器通过读取和执行该日志文件来保持和主服务器的数据一致
- Slave 会执行以下两个线程读取和执行该日志文件
    - `Slave_IO`:复制master主机 binlog日志文件里的SQL命令到本机的relay-log文件里
    - `Slave_SQL`:执行本机relay-log文件里的SQL语句,实现与Master数据一致
- 在使用二进制日志时,主服务器的所有操作都会被记录下来,然后从服务器会接收到该日志的一个副本RelayLog,从服务器可以指定执行该日志中的哪一类事件(例如只插入数据或者只更新数据),默认会执行日志中的所有语句

### 索引

- 在MySQL中主要有四类索引:主键索引,唯一索引,常规索引,和全文索引
- **主键索引**: 唯一标识数据库表中的每条记录,每个表都应该有一个主键,并且每个表只能有一个主键
- **唯一索引**:不允许出现相同的值,避免同一个表中某数据列中的值重复
- **普通索引**:快速定位特定数据,不会去约束索引的字段的行为
- **全文索引**:快速定位特定数据,全文搜索通过 `MATCH()` 函数完成
- **联合索引**:两个或更多个列上的索引被称作联合索引,利用索引中的附加列,可以缩小搜索的范围,但使用一个具有两列的索引 不同于使用两个单独的索引
- **最左前缀匹配原则**
    - 在MySQL建立联合索引时会遵守最左前缀匹配原则,即查询从索引的最左列开始并且不能跳过索引中的列,如果遇到索引失效的情况,则右边的索引列全部转为全表查询
    - 这是因为索引的底层数据结构B+树的数据结构决定的,B+树是按照从左到右的顺序来建立叶子节点的,B+树会优先比较第一个字段来确定下一步的所搜方向,如果第一个字段相同再依次比较第二和第三个字段,最后得到检索的数据
- **聚簇索引和非聚簇索引**:索引的存储顺序和数据的存储顺序是否是关系的,有关就是聚簇索引,无关就是非聚簇索引
    - **聚簇索引**:Innodb的主键索引,非叶子节点存储的是索引指针,叶子节点存储的是既有索引也有数据
    - **非聚簇索引**:MyISAM的默认索引,B+Tree的叶子节点存储的是数据存放的地址,而不是具体的数据,因此,索引存储顺序和数据存储关系毫无关联,另外Inndob里的辅助索引也是非聚簇索引
- **辅助索引与覆盖索引**
    - **辅助索引**:如果不是主键索引,就称为辅助索引或者二级索引,主键索引的叶子节点存储了完整的数据行,而非主键索引的叶子节点存储的则是主键索引值,通过非主键索引查询数据时,会先查找到主键索引,然后再到主键索引上去查找对应的数据
    - **覆盖索引**:如果需要查询的字段被包含在辅助索引节点中,那么可以直接获得我们所需要的信息,按照这种思想Innodb针对使用辅助索引的查询场景做了优化,称为**覆盖索引**

### 索引优化

- 如果是联合索引,要遵守最左前缀法则,指的是查询从索引的最左前列开始并且不跳过索引中的列
- 不要在索引列上做任何操作(计算,函数,自动或手动类型转换),会导致索引失效而转向全表扫描
- 联合索引范围条件右边的索引列会失效,范围查询的列在定义索引的时候,应该放在最后面
- MySQL 在使用不等于(!= 或者<>)的时候无法使用索引会导致全表扫描
- IS NOT NULL 也无法使用索引,但是IS NULL是可以使用索引的
- LIKE以通配符开头`'%abc...'`的索引失效会变成全表扫描的操作
- 字符串不加单引号索引失效(类型转换导致索引失效)

### B+ 树

- B+树是一种特殊的搜索树,InnoDB 存储引擎默认的底层的数据结构
- **性质**
    - 非叶子节点相当于是叶子节点的索引层,叶子节点是存储关键字数据的数据层,搜索只在叶子节点命中,树的查询效率稳定
    - 所有的叶子结点中包含了全部关键字的信息,及指向含这些关键字记录的指针,且叶子结点本身依关键字的大小自小而大顺序链接,B+树只需要去遍历叶子节点就可以实现整棵树的遍历
- B+树的出度(树的分叉数)
    - 不管是内存中的数据还是磁盘中的数据,操作系统都是按页(一页的大小通常是 4kb,这个值可以通过`getconfig(PAGE_SIZE)`命令查看)来读取的,一次只会读取一页的数据
    - 如果要读取的数据量超过了一页的大小,就会触发多次 IO 操作,所以在选择 m 大小的时候,要尽量让每个节点的大小等于一个页的大小
    - 一般实际应用中,出度是非常大的数字,通常超过 100,树的高度(h)非常小,通常不超过 3

### 分布式锁

- 要实现分布式锁,最简单的方式可能就是直接创建一张锁表,然后通过操作该表中的数据来实现了,当我们想要获得锁的时候,就可以在该表中增加一条记录,想要释放锁的时候就删除这条记录
- 使用唯一性约束,这样如果有多个请求同时提交到数据库的话,数据库可以保证只有一个操作可以成功,那么那么我们就可以认为操作成功的那个请求获得了锁
- **注意**
    - 这种锁没有失效时间,一旦释放锁的操作失败就会导致锁记录一直在数据库中,其它线程无法获得锁,这个缺陷也很好解决,比如可以做一个定时任务去定时清理
    - 这种锁是非阻塞的,因为插入数据失败之后会直接报错,想要获得锁就需要再次操作,如果需要阻塞式的,可以弄个for循环,while循环之类的,直至INSERT成功再返回
    - 这种锁也是非可重入的,因为同一个线程在没有释放锁之前无法再次获得锁,因为数据库中已经存在同一份记录了,想要实现可重入锁,可以在数据库中添加一些字段,比如获得锁的主机信息,线程信息等,那么在再次获得锁的时候可以先查询数据,如果当前的主机信息和线程信息等能被查到的话,可以直接把锁分配给它

### COUNT

- `COUNT(*)`包括了所有的列,相当于行数,在统计结果的时候,不会忽略列值为NULL
- `COUNT(1)`包括了所有的列,用1代表代码行,在统计结果的时候,不会忽略列值为NULL
- `COUNT(列名)`只包括列名那一列,在统计结果的时候,会忽略列值为空(这里的空不是只空字符串或者0,而是表示null)的计数,即某个字段值为NULL时,不统计
- 对于`COUNT(1)`和`COUNT(*)`执行优化器的优化是完全一样的,并没有`COUNT(1)`会比`COUNT(*)`快这个说法

### Group By 和 Having, Where ,Order by执行顺序

- Group By 和 Having, Where ,Order by这些关键字是按照如下顺序进行执行的:Where, Group By, Having, Order by
- 首先where将最原始记录中不满足条件的记录删除(所以应该在where语句中尽量的将不符合条件的记录筛选掉,这样可以减少分组的次数),然后通过Group By关键字后面指定的分组条件将筛选得到的视图进行分组,接着系统根据Having关键字后面指定的筛选条件,将分组视图后不满足条件的记录筛选掉,最后按照Order By语句对视图进行排序,这样最终的结果就产生了
- 在这四个关键字中,只有在Order By语句中才可以使用最终视图的列名

## Redis

### 什么是中间件

中间件是一类提供系统软件和应用软件之间连接,便于软件各部件之间的沟通的软件,应用软件可以借助中间件在不同的技术架构之间共享信息与资源,中间件位于客户机服务器的操作系统之上,管理着计算资源和网络通信

### 分布式锁

- Redis 锁主要利用 Redis 的 setnx 命令
    - 加锁命令:SETNX key value,当键不存在时,对键进行设置操作并返回成功,否则返回失败,KEY 是锁的唯一标识,一般按业务来决定命名
    - 解锁命令:DEL key,通过删除键值对释放锁,以便其他线程可以通过 SETNX 命令来获取锁
    - 锁超时:EXPIRE key timeout, 设置 key 的超时时间,以保证即使锁没有被显式释放,锁也可以在一定时间后自动释放,避免资源被永远锁住
- **SETNX 和 EXPIRE 非原子性**
    - 如果 SETNX 成功,在设置锁超时时间后,服务器挂掉,重启或网络问题等,导致 EXPIRE 命令没有执行,锁没有设置超时时间变成死锁
    - 有很多开源代码来解决这个问题,比如使用 lua 脚本
- **锁误解除**
    - 如果线程 A 成功获取到了锁,并且设置了过期时间 30 秒,但线程 A 执行时间超过了 30 秒,锁过期自动释放,此时线程 B 获取到了锁,随后 A 执行完成,线程 A 使用 DEL 命令来释放锁,但此时线程 B 加的锁还没有执行完成,线程 A 实际释放的线程 B 加的锁
    - 通过在 value 中设置当前线程加锁的标识,在删除之前验证 key 对应的 value 判断锁是否是当前线程持有,可生成一个 UUID 标识当前线程,使用 lua 脚本做验证标识和解锁操作
- **超时解锁导致并发**
    - 如果线程 A 成功获取锁并设置过期时间 30 秒,但线程 A 执行时间超过了 30 秒,锁过期自动释放,此时线程 B 获取到了锁,线程 A 和线程 B 并发执行
    - A,B 两个线程发生并发显然是不被允许的,一般有两种方式解决该问题:
        - 将过期时间设置足够长,确保代码逻辑在锁释放之前能够执行完成
        - 为获取锁的线程增加守护线程,为将要过期但未释放的锁增加有效时间
- **不可重入**
    - 当线程在持有锁的情况下再次请求加锁,如果一个锁支持一个线程多次加锁,那么这个锁就是可重入的,如果一个不可重入锁被再次加锁,由于该锁已经被持有,再次加锁会失败,Redis 可通过对锁进行重入计数,加锁时加 1,解锁时减 1,当计数归 0 时释放锁
    - ThreadLocal或RedisMap实现计数
- **无法等待锁释放**
    - 上述命令执行都是立即返回的,如果客户端可以等待锁释放就无法使用
        - 可以通过客户端轮询的方式解决该问题,当未获取到锁时,等待一段时间重新获取锁,直到成功获取锁或等待超时,这种方式比较消耗服务器资源,当并发量比较大时,会影响服务器的效率
        - 另一种方式是使用 Redis 的发布订阅功能,当获取锁失败时,订阅锁释放消息,获取锁成功后释放时,发送锁释放消息

### 高并发下的问题

#### 缓存穿透

- 缓存穿透是指用户请求的数据在缓存中不存在即没有命中,同时在数据库中也不存在,导致用户每次请求该数据都要去数据库中查询一遍,如果有恶意攻击者不断请求系统中不存在的数据,会导致短时间大量请求落在数据库上,造成数据库压力过大,甚至导致数据库承受不住而宕机崩溃

**解决方法**

1. **将无效的key存放进Redis中**:当出现Redis查不到数据,数据库也查不到数据的情况,我们就把这个key保存到Redis中,设置value="null",并设置其过期时间极短,后面再出现查询这个key的请求的时候,直接返回null,就不需要再查询数据库了,但这种处理方式是有问题的,假如传进来的这个不存在的Key值每次都是随机的,那存进Redis也没有意义
2. **使用布隆过滤器**:如果布隆过滤器判定某个 key 不存在布隆过滤器中,那么就一定不存在,如果判定某个 key 存在,那么很大可能是存在(存在一定的误判率),于是我们可以在缓存之前再加一个布隆过滤器,将数据库中的所有key都存储在布隆过滤器中,在查询Redis前先去布隆过滤器查询 key 是否存在,如果不存在就直接返回,不让其访问数据库,从而避免了对底层存储系统的查询压力

> **布隆过滤器**
>
> - **初始化过程**
>     1. 初始化n位的二进制数组
>     2. 对每个Key做多次Hash,将Hash值数组上的位置设置为1
> - **判断过程**
>     1. 对需要判断的Key同样做多次Hash,判断Hash值在数组上的位置是否全为1,是则存在(可能误判),否则一定不存在
> - **减少误判的措施**
>     - 增加二进制数组位数
>     - 增加Hash的次数
> - **如果Key被删除怎么更新过滤器**:布隆过滤器因为某一位二进制可能被多个编号Hash引用,因此布隆过滤器无法直接处理删除数据的情况
>     1. 定时异步重建布隆过滤器
>     2. 计数Bloom Fliter

#### 缓存击穿

- 缓存击穿是某个热点的key失效,大并发集中对其进行请求,就会造成大量请求读缓存没读到数据,从而导致高并发访问数据库,引起数据库压力剧增,这种现象就叫做缓存击穿

**解决方案**

1. **加互斥锁**:在缓存失效后,通过互斥锁或者队列来控制读数据写缓存的线程数量,比如某个key只允许一个线程查询数据和写缓存,其他线程等待,这种方式会阻塞其他的线程,此时系统的吞吐量会下降
2. **热点数据缓存永远不过期**:永不过期实际包含两层意思
    - 物理不过期,针对热点key不设置过期时间
    - 逻辑过期,把过期时间存在key对应的value里,如果发现要过期了,通过一个后台的异步线程进行缓存的构建

#### 缓存雪崩

- 如果缓存某一个时刻出现大规模的key失效,那么就会导致大量的请求打在了数据库上面,导致数据库压力巨大,如果在高并发的情况下,可能瞬间就会导致数据库宕机,这时候如果运维马上又重启数据库,马上又会有新的流量把数据库打死,这就是缓存雪崩

**解决方案**

1. **事前**
    - **均匀过期**:设置不同的过期时间,让缓存失效的时间尽量均匀,避免相同的过期时间导致缓存雪崩,造成大量数据库的访问
    - **分级缓存**:第一级缓存失效的基础上,访问二级缓存,每一级缓存的失效时间都不同
    - **热点数据缓存永远不过期**
    - **保证Redis缓存的高可用**:防止Redis宕机导致缓存雪崩的问题,可以使用主从+ 哨兵,Redis集群来避免 Redis 全盘崩溃的情况
2. **事中**
    - **互斥锁**:在缓存失效后,通过互斥锁或者队列来控制读数据写缓存的线程数量,比如某个key只允许一个线程查询数据和写缓存,其他线程等待,这种方式会阻塞其他的线程,此时系统的吞吐量会下降
    - **使用熔断机制**:限流降级,当流量达到一定的阈值,直接返回"系统拥挤”之类的提示,防止过多的请求打在数据库上将数据库击垮,至少能保证一部分用户是可以正常使用,其他用户多刷新几次也能得到结果
3. **事后**
    - 开启Redis持久化机制,尽快恢复缓存数据,一旦重启,就能从磁盘上自动加载数据恢复内存中的数据

#### 并发竞争key问题

- Redis 并发竞争问题就是高并发写同一个key时导致的值错误。
- 常用的解决方法：
    - 乐观锁:`watch` 命令会监视给定的每一个key，当提交事务时如果监视的任一个key自从调用watch后发生过变化，则整个事务会回滚，不执行任何动作。
    - 分布式锁:在业务层进行控制，操作 Redis之前，先去申请一个分布式锁，拿到锁的才能操作
    - 时间戳：适合有序场景，在写入时保存一个时间戳，写入前先比较自己的时间戳是不是早于现有记录的时间戳，如果早于，就不写入了。
    - 消息队列:在并发量很大的情况下，可以通过消息队列进行串行化处理

### 数据类型

- **String**:字符串类型是 Redis 最基础的数据结构,首先键都是字符串类型,而且 其他几种数据结构都是在字符串类型基础上构建的,我们常使用的 set key value 命令就是字符串,常用在缓存,计数,共享Session,限速等
- **Hash**:在Redis中,Hash类型是指键值本身又是一个键值对结构,Hash可以用来存放用户信息,比如实现购物车
- **List**:列表(list)类型是用来存储多个有序的字符串,可以做简单的消息队列的功能
- **Set**:集合(set)类型也是用来保存多个的字符串元素,但和列表类型不一 样的是,集合中不允许有重复元素,并且集合中的元素是无序的,不能通过索引下标获取元素,利用 Set 的交集,并集,差集等操作,可以计算共同喜好,全部的喜好,自己独有的喜好等功能
- **Sorted Set**:Sorted Set 多了一个权重参数 Score,集合中的元素能够按 Score 进行排列,可以做排行榜应用,取 TOP N 操作

### 持久化技术

- Redis为了保证效率,数据缓存在了内存中,但是会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件中,以保证数据的持久化
- Redis的持久化策略有两种:
    1. **RDB**:快照形式是直接把内存中的数据定时保存到一个dump的文件中

        - 当Redis需要做持久化时,Redis会fork一个子进程,子进程将数据写到磁盘上一个临时RDB文件中,当子进程完成写临时文件后,将原来的RDB替换掉

    2. **AOF**:把所有的对Redis的服务器进行修改的命令都存到一个文件里

        - 使用AOF做持久化,每一个写命令都通过write函数追加到`appendonly.aof`中

        - AOF的默认策略是每秒钟fsync一次,在这种配置下,就算发生故障停机,也最多丢失一秒钟的数据
        - **缺点**:对于相同的数据集来说,AOF的文件体积通常要大于RDB文件的体积,AOF的速度可能会慢于RDB

- Redis默认是快照RDB的持久化方式

### Redis 效率高的原因

1. 纯内存操作,相对于读写磁盘,读写速度提升明显
2. 单线程操作,避免了频繁的上下文切换
3. 采用了I/O多路复用机制

> **文件描述符**
>
> - 在形式上是一个非负整数,实际上,它是一个索引值,指向内核为每一个进程所维护的该进程打开文件的记录表

**I/O多路复用机制**

- 多路指的是多个网络连接,复用指的是复用同一个线程
- IO 多路复用只需要一个进程就能够处理多个套接字,从而解决了上下文切换的问题
- 在 I/O 多路复用模型中,最重要的函数调用就是 `select`,该方法的能够同时监控多个文件描述符的可读可写情况,当其中的某些文件描述符可读或者可写时,`select` 方法就会返回可读以及可写的文件描述符个数

### Bigkey

- **Redis Bigkey**:即数据量大的 Key,比如字符串 Value 值非常大,哈希,列表,集合,有序集合元素多等,由于其数据大小远大于其他 Key,容易造成内存不均,超时阻塞,网络流量拥塞等一系列问题
- **如何发现 Bigkey**:使用官方的`redis-cli --bigkeys`时,它会对 Redis 中的 key 进行`SCAN`采样,寻找较大的 keys,不用担心会阻塞 Redis
- **删除Bigkey**:如果直接`DEL`bigkey 操作可能会引发 Redis 阻塞甚至是发生 Sentinel 主从切换,可以使用 SCAN 命令来分多批枚举 bigkey,然后实现渐进式删除 bigkey
- **避免Bigkey**:主要是对 Bigkey 进行拆分,拆成多个 key,然后用`MGET`取回来,再在业务层做合并

### 事务

- Redis事务可以理解为一个打包的批量执行脚本,但批量指令并非原子化的操作,中间某条指令的失败不会导致前面已做指令的回滚,也不会造成后续的指令不做
- 以`MULTI`开始一个事务, 然后将多个命令入队到事务中, 最后由`EXEC`命令触发事务, 一并执行事务中的所有命令

###  删除策略

1. 立即删除,在设置键的过期时间时,创建一个定时器,当过期时间达到时,立即执行删除操作
2. 惰性删除,key过期的时候不删除,每次从数据库获取key的时候去检查是否过期,若过期,则删除,返回null
3. 定时删除,每隔一段时间,对全部的键进行检查,删除里面的过期键

### Redis 和 Mysql 数据库数据如何保持一致性

- **先写数据库再删缓存**

    1. 先更新数据库
    2. 再删除缓存

    - 先删缓存可能导致读操作更新旧的缓存数据,导致数据库与Redis数据不一致

    - 更新缓存可能会因为执行顺序与访问顺序不一致导致数据库与Redis数据不一致,而删除缓存不会

- **缓存延时双删**

    - 先操作数据库再操作缓存,也会导致数据不一致,因为不是原子性操作
    - 可能会遇到缓存失效导致读操作更新旧的缓存数据,所以需要延时等待读操作结束再删除缓存

    1. 先删除缓存
    2. 再更新数据库
    3. 休眠一会(读业务逻辑数据的耗时 + 几百毫秒)
    4. 再次删除缓存

    - 第一次删除缓存的目的在于当最后一次延时删除缓存失败的情况发生,至少一致性策略只会退化成先删缓存再更新数据的策略
    - 为了确保读请求结束,写请求第二次删除读请求可能带来的缓存脏数据,只有休眠那一会(比如就那1秒),可能有脏数据,一般业务也会接受的

- **删除缓存重试机制**

    - 不管是延时双删还是Cache-Aside的先操作数据库再删除缓存,都可能会存在第二步的删除缓存失败,导致的数据不一致问题
    - 可以引入删除缓存重试机制,如果删除失败就多删除几次,保证删除缓存成功

    1. 写请求更新数据库
    2. 缓存因为某些原因,删除失败
    3. 把删除失败的key放到消息队列
    4. 消费消息队列的消息,获取要删除的key
    5. 重试删除缓存操作

- **读取binlog异步删除缓存**

    - 一旦MySQL中产生了新的写入,更新,删除等操作,就可以把binlog相关的消息通过消息队列推送至Redis,Redis再根据binlog中的记录,对Redis进行更新
    - 这种同步机制类似于MySQL的主从备份机制,可以结合使用阿里的canal对MySQL的binlog进行订阅

### Redis的应用场景

- 缓存
- 共享Session
- 消息队列系统
- 分布式锁

### 幂等性

- **数据库唯一主键**:数据库唯一主键的实现主要是利用数据库中主键唯一约束的特性,一般来说唯一主键比较适用于"插入”时的幂等性,其能保证一张表中只能存在一条带该唯一主键的记录
- **数据库乐观锁**:数据库乐观锁方案一般只能适用于执行"更新操作”的过程,我们可以提前在对应的数据表中多添加一个字段,充当当前数据的版本标识,这样每次对该数据库该表的这条数据执行更新时,都会将该版本标识作为一个条件,值为上次待更新数据中的版本标识的值
- **下游传递唯一序列号**:所谓请求序列号,其实就是每次向服务端请求时候附带一个短时间内唯一不重复的序列号,该序列号可以是一个有序 ID,也可以是一个订单号,一般由下游生成,在调用上游服务端接口时附加该序列号和用于认证的 ID
    - 当上游服务器收到请求信息后拿取该序列号 和下游认证ID 进行组合,形成用于操作 Redis 的 Key,然后到 Redis 中查询是否存在对应的 Key 的键值对
    - 如果存在,就说明已经对该下游的该序列号的请求进行了业务处理,这时可以直接响应重复请求的错误信息
    - 如果不存在,就以该 Key 作为 Redis 的键,以下游关键信息作为存储的值(例如下游商传递的一些业务逻辑信息),将该键值对存储到 Redis 中,然后再正常执行对应的业务逻辑即可

### 集群

#### 主从复制

- 主从复制,是指将一台Redis服务器的数据,复制到其他的Redis服务器,前者称为主节点(master/leader) ,后者称为从节点(slave/follower)
- 默认情况下,每台Redis服务器都是主节点,且一个主节点可以有0或多个从节点,但一个从节点只能有一个主节点,数据的复制是单向的,只能由主节点到从节点
- **作用**
    - **数据冗余**:主从复制实现了数据的热备份,是持久化之外的一种数据冗余方式
    - **读写分离**:在主从复制的基础上,配合读写分离,可以由主节点提供写服务,由从节点提供读服务 ,分担服务器负载;尤其是在写少读多的场景下,通过多个从节点分担读负载,可以大大提高Redis服务器的并发量
    - **高可用基础**:除了上述作用以外,主从复制还是哨兵和集群能够实施的基础,因此说主从复制是Redis高可用的基础
- **原理**:对于主从复制来说,主从刚刚连接的时候,进行全量同步(RDB),全同步结束后,进行增量同步(AOF)
    1. Slave与Master建立连接
    2. Slave向Master发起同步请求(SYNC)
    3. Master执行bgsave命令生成rdb数据快照,发给Slave
    4. Slave加载RDB数据快照,还原数据,主从保持一致
    5. 之后Master执行的写操作都会发往Slave执行,保持数据同步

#### 哨兵

- 哨兵模式能够后台监控主机是否故障,如果故障了则自动将从节点转换为主节点

- **流程**

    - 哨兵通过发送命令,等待 Redis服务器响应,从而监控运行的多个 Redis实例,让 Redis服务器返回其运行状态
    - 当哨兵监测到主节点宕机,会自动将从节点切换成主节点,然后通过发布订阅模式通知其他的从节点,修改配置文件并切换主节点
    - 当主节点恢复连接后,原主节点自动转换为从节点,现主节点不变

- **优点**

    - 哨兵模式是主从模式的升级,手动转换为自动
    - 主从可以切换,故障可以转移,系统的可用性更好

- **多哨兵模式**

    1. 每个Sentinel以每秒钟一次的频率向它所知的Master,Slaver以及其他Sentinel实例发送一个PING命令

    2. 如果一个实例距离最后一次有效回复PING命令的时间超过`own-after-millisecounds`选项所指定的值,则这个实例会被Sentinel标记为主观下线

    3. 当有足够足够数量的Sentinel(大于等于配置文件指定的值)在指定的时间范围内确定Master的确进入了主观下线状态,则Master会被标记为客观下线

    4. Sentinel从Slave中选出新的Master
        - 剔除主观下线,已断线,或者最后一次回复PING命令的时间大于5s的Slave
        - 剔除与失效主服务器连接断开的时长超过`down-after`选项指定的时长10倍的Slave
        - 按同步数据的偏移量选出数据最完成的Slave
        - 如果偏移量相同,选中ID最小的Slave

    5. 将Slave切换成Master
        1. 向被选中的从服务器发送`SLAVEOF NO ONE`命令,让它转成主服务器
        2. 通过发布于订阅功能,将更新后的配置传播给所有其他Sentinel,其他Sentinel对它们自己的配置进行更新
        3. 向所有Slave下发SLAVEOF命令,指向新的主服务器
        4. redis-slave向master重新建立连接,重放rdb保持数据同步

    - 在上述转移过程中,伴随着Redis本地配置文件的自动重写,这样即使是实例重启配置也不会丢失
    - 原有的master在恢复后降级为slave与新master全量同步

- **哨兵的高可用**

    - Sentinel 自动故障迁移使用 Raft 算法来选举领头(leader)Sentinel
    - 超过半数投票选出 Leader,Sentinel Leader 用于下发故障转移的指令
    - 如果某个 Leader 挂了,则使用Raft从剩余的 Sentinel 中选出 Leader

- **哨兵的感知发现**

    - 每一个Sentinel节点接入Master之后,所有Sentinel的信息也在Master节点上进行了注册
    - Sentinel 可以通过Master获取其他Sentinel节点的信息

#### 分片

- 分片是分割数据到多个Redis实例的处理过程,因此每个实例只保存key的一个子集
- **优点**
    - 通过利用多台计算机内存,可以构造更大的数据库
    - 通过利用多台计算机的多核,允许我们扩展计算能力
    - 通过利用多台计算机的网络适配器,可以扩展网络带宽
- **分片的不足**

    - 涉及多个key的操作通常是不被支持的,例如,当两个set映射到不同的redis实例上时,你就不能对这两个set执行交集操作
    - 涉及多个key的Redis事务不能使用
    - 当使用分片时,数据处理较为复杂,比如需要处理多个rdb/aof文件,并且从多个实例和主机备份持久化文件
- **数据分散存储**
    - Redis Cluster 集群采用 HashSlot（哈希槽）分配数据，Redis 集群预先分好16384（16K）个槽，初始化集群时平均规划给每一台Redis Master
    - 然后对存入的Key取哈希并对16384（16K）取模得到指定的槽，存放到包含该槽的节点服务器上


## Kafka

### Kafka的使用场景

- 通过使用消息队列,我们可以异步处理请求,从而缓解系统的压力,同样可以达到削峰解耦的目的
- **日志订阅**:通过kafka对各个服务的日志进行收集,再开放给各个consumer,例如实现其他下发操作给其他系统,或者实现数据库与Redis的一致性
- **异步执行方法**:将方法名和入参存入MQ中,之后再统一消费并执行方法,起到缓解系统的压力

### 消息有序性

- 每个分区内,每条消息都有offset,所以只能在同一分区内有序,但不同的分区无法做到消息顺序性
- 生产者在写的时候可以指定一个 key,这个 key 对应的消息都会发送到同一个 partition 中,所以消费者消费到的消息也一定是有序的
- 消费者端可能需要使用多线程并发处理消息来提高吞吐量,每个线程线程处理消息的快慢是不一致的,导致最终消息有可能不一致,所以我们需要保证同一个Key的消息只被同一个线程处理,由此我们可以在线程处理前增加个内存队列,每个线程只负责处理其中一个内存队列的消息,同一个订单号的消息发送到同一个内存队列中即可

### 重复消费&丢失信息

- 由于 consumer 在消费过程中可能会出现断电宕机等故障,consumer 恢复后,需要从故障前的位置的继续消费,所以consumer 需要实时记录自己消费到了哪个 offset,以便故障恢复后继续消费
- 先处理后提交offset,会造成重复消费,先提交offset后处理,会造成数据丢失
- 消费者通过实现幂等性来解决重复消费的问题

### 分区

- 对于kafka集群来说,分区可以做到负载均衡,对于消费者来说,可以提高并发度,提高读取效率
- 在同一消费者组中,超过分区数的消费者就不会再接收数据
- **创建Topic分配分区**
    - 首先副本数不能超过broker数
    - 第一分区是随机从Broker中选择一个,然后其他分区相对于0号分区依次向后移
- **Topic修改分区数**
    - 可以增加,不可以减少,先有的分区数据难以处理
- **生产者分区策略**
    - 指明 partition 的情况下,直接将指明的值直接作为 partiton 值
    - 没有指明 partition 值但有 key 的情况下,将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值
    - 既没有 partition 值又没有 key 值的情况下,第一次调用时随机生成一个整数(后面每次调用在这个整数上自增),将这个值与 topic 可用的 partition 总数取余得到 partition值,也就是常说的 round-robin 算法
- **消费者分区策略**
    - Roudn Robin:先将每个topic的每个partition排序,然后以轮询的方式分配所有的分区给每个consumer
    - Range重分配策略:先计算各个consumer将会承载的分区数量,然后将指定数量的分区分配给该consumer
- 在进行分区分配时,不会考虑某一个消费者的授权和订阅信息,只会根据消费组整体的订阅情况进行分区分配,此种情况下可能会给消费者分配到未授权的Topic,因此会出现"Not authorized to access topics”的报错

### 文件存储

- partition位于一个文件夹下, 该文件夹的命名规则为:topic 名称+分区序号,例如,first 这个 topic 有三个分区,则其对应的文件夹为 first-0,first-1,first-2
- 由于生产者生产的消息会不断追加到 log 文件末尾,为防止 log 文件过大导致数据定位效率低下,Kafka 采取了**分片**和**索引**机制,将每个 partition 分为多个 segment
- 每个 segment对应两个文件,即`.index`文件和`.log`文件

### Kafka效率高的原因

1. kafka是分布式的消息队列

2. 对log文件进行了segment,并对segment建立了索引

3. 顺序写磁盘

    - Kafka 的 producer 生产数据,要写入到 log 文件中,写的过程是一直追加到文件末端,为顺序写,其省去了大量磁头寻址的时间
4. 零复制技术

    - IO操作不用经过User space,直接由kernel space 与 NIC 交互
    - NIC(Network Interface Controller)网络接口控制器

### Kafka 的高可靠性

- Kafka通过分区的多副本机制来保证消息的可靠性
- **ACK**
    - 当ack=0时,producer不等待broker的ack,不管数据有没有写入成功,都不再重复发该数据
    - 当ack=1时,broker会等到leader写完数据后,就会向producer发送ack,但不会等follower同步数据,如果这时leader挂掉,producer会对新的leader发送新的数据,在old的leader中不同步的数据就会丢失
    - 当ack=-1或者all时,broker会等到leader和isr中的所有follower都同步完数据,再向producer发送ack,有可能造成数据重复
- **数据一致性问题**:
    - **follower 故障**:follower 发生故障后会被临时踢出 ISR,待该 follower 恢复后,follower 会读取本地磁盘记录的上次的 HW,并将 log 文件高于 HW 的部分截取掉,从 HW 开始向 leader 进行同步,等该 follower 的 LEO 大于等于该 Partition 的 HW,即 follower 追上 leader 之后,就可以重新加入 ISR 了
    - **leader 故障**:leader 发生故障之后,会从 ISR 中选出一个新的 leader,之后,为保证多个副本之间的数据一致性,其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉,然后从新的 leader同步数据
    - **注意**:这只能保证副本之间的数据一致性,并不能保证数据不丢失或者不重复
- **ISR**:意为和 leader 保持同步的 follower 集合,当 ISR 中的 follower 完成数据的同步之后,就会给 leader 发送 ack,如果 follower长时间未向leader同步数据,则该follower将被踢出ISR,该时间阈值由`replica.lag.time.max.ms`参数设定,Leader 发生故障之后,就会从 ISR 中选举新的 leader

>**ISR,OSR,AR**
>
>- ISR (InSyncRepli): 速率和leader相差低于10秒的follower的集合
>- OSR(OutSyncRepli) : 速率和leader相差大于10秒的follower
>    - 将失效的follower踢出ISR
>    - 等速率接近leader10秒内,再加进ISR
>- AR(AllRepli) : 所有分区的follower
>
>**HW,LEO**
>
>- HW : 又名高水位,根据同一分区中,最低的LEO所决定
>- LEO : 每个分区的最高offset

### 消费方式

- 在producer阶段,是向broker用Push模式
- 在consumer阶段,是向broker用Pull模式
- 在Pull模式下,consumer可以根据自身速率选择如何拉取数据,避免了低速率的consumer发生崩溃的问题,但缺点是,consumer要时不时的去询问broker是否有新数据,容易发生死循环,内存溢出

### Controller的作用

- 负责kafka集群的上下线工作,所有topic的副本分区分配和选举leader工作
- **选举**:在ISR中需要选择,选择策略为先到先得

### 事务

- kafka事务有两种:producer事务和consumer事务
- producer事务是为了解决kafka跨分区跨会话问题
    - kafka不能跨分区跨会话的主要问题是每次启动的producer的PID都是系统随机给的,所以为了解决这个问题,我们就要手动给producer一个全局唯一的id,也就是transaction id 简称TID
    - 我们将TID和PID进行绑定,在producer带着TID和PID第一次向broker注册时,broker就会记录TID,并生成一个新的组件`__transaction_state`用来保存TID的事务状态信息,当producer重启后,就会带着TID和新的PID向broker发起请求,当发现TID一致时,producer就会获取之前的PID,将覆盖掉新的PID,并获取上一次的事务状态信息,从而继续上次工作
- consumer事务相对于producer事务就弱一点,需要先确保consumer的消费和提交位置为一致且具有事务功能,才能保证数据的完整,不然会造成数据的丢失或重复

### 生产者客户端的整体结构

- Kafka 的 Producer 发送消息采用的是异步发送的方式,在消息发送的过程中,涉及到了两个线程:main 线程和 sender 线程,以及一个线程共享变量: RecordAccumulator
    - main 线程将消息发送给 RecordAccumulator
    - sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka broker
- **相关参数**
    - batch.size:只有数据积累到batch.size之后,sender才会发送数据
    - linger.ms:如果数据迟迟未达到batch.size,sender等待linger.time之后就会发送数据

## Spring

### AOP

- AOP指面向切面编程,用于处理系统中分布于各个模块的横切关注点,把那些与业务无关,但是却为业务模块所共同调用的逻辑部分封装起来,从而使得业务逻辑各部分之间的耦合度降低, 提高程序的可重用性, 同时提高了开发的效率
- AOP实现的关键在于AOP框架自动创建的AOP代理,AOP代理主要分为静态代理和动态代理,静态代理的代表为AspectJ,而动态代理则以Spring AOP为代表
    - AspectJ是静态代理的增强,所谓的静态代理就是AOP框架会在编译阶段生成AOP代理类,因此也称为编译时增强

    - Spring AOP中的动态代理主要有两种方式,JDK动态代理和CGLIB动态代理,JDK动态代理通过反射来接收被代理的类,并且要求被代理的类必须实现一个接口,JDK动态代理的核心是InvocationHandler接口和Proxy类

    - Spring AOP中的代理使用的默认策略

        - 如果目标对象实现类接口,则默认采用JDK动态代理
        - 如果目标对象没有实现接口,则采用CGLIB进行动态代理

- 在AOP编程中,我们经常会遇到下面的概念:
    - Joinpoint:连接点,即定义在应用程序流程的何处插入切面的执行
    - Pointcut:切入点,即一组连接点的集合
    - Advice:增强,指特定连接点上执行的动作
    - Introduction:引介,特殊的增强,指为一个已有的Java对象动态地增加新的接口
    - Weaving:织入,将增强添加到目标类具体连接点上的过程
    - Aspect:切面,由切点和增强(引介)组成,包括了对横切关注功能的定义,已包括了对连接点的定义

### IoC

- 控制反转,是把传统上由程序代码直接操控的对象的调用权交给容器,由容器来创建对象并管理对象之间的依赖关系,DI是对IoC更准确的描述,即由容器动态的将某种依赖关系注入到组件之中

- **IoC的原理**

    1. 实例化后的对象被封装在BeanWrapper对象中,并且此时对象仍然是一个原生的状态,并没有进行依赖注入
    2. 紧接着,Spring根据BeanDefinition中的信息进行依赖注入
    3. 并且通过BeanWrapper提供的设置属性的接口完成依赖注入


### DI

- 依赖注入的方式有以下几种:
    - @Autowired,@Resource
    - Setter方法注入
    - p命名空间和c命名空间注入
    - 构造器注入
    - 自动装配
- @Autowired 和@Resource区别
    - @Autowired默认按类型装配(属于Spring规范),如果我们想使用名称装配可以结合@Qualifier注解进行使用
    - @Resource默认按照名称进行装配(属于J2EE规范), 名称可以通过name属性进行指定,而使用type属性时则使用byType自动注入策略

### Bean 作用域

- 在Spring的早期版本中仅有两个作用域:singleton和prototype,前者表示Bean以单例的方式存在,后者表示每次从容器中调用Bean时,都会返回一个新的实例
- Spring2.x中针对WebApplicationContext新增了3个作用域,分别是:request(每次HTTP请求都会创建一个新的Bean), session(同一个HttpSession共享同一个Beaan,不同的HttpSession使用不同的Bean)和globalSession(同一个全局Session共享一个Bean)

### Bean 的生命周期

1. 实例化Bean
    - 对于BeanFactory容器,当客户向容器请求一个尚未初始化的bean时,或初始化bean的时候需要注入另一个尚未初始化的依赖时,容器就会调用createBean进行实例化
    - 对于ApplicationContext容器,当容器启动结束后,便实例化所有的bean,容器通过获取BeanDefinition对象中的信息进行实例化,并且这一步仅仅是简单的实例化,并未进行依赖注入,实例化对象被包装在BeanWrapper对象中,BeanWrapper提供了设置对象属性的接口,从而避免了使用反射机制设置属性
2. 设置对象属性(依赖注入)
    - 实例化后的对象被封装在BeanWrapper对象中,并且此时对象仍然是一个原生的状态,并没有进行依赖注入
    - 紧接着,Spring根据BeanDefinition中的信息进行依赖注入
    - 并且通过BeanWrapper提供的设置属性的接口完成依赖注入
3. 注入Aware接口
    - 紧接着,Spring会检测该对象是否实现了xxxAware接口,并将相关的xxxAware实例注入给bean
        - 实现BeanFactoryAware 主要目的是为了获取Spring容器,如Bean通过Spring容器发布事件等
        - 实现BeanNameAware清主要是为了通过Bean的引用来获得Bean的ID,一般业务中是很少有用到Bean的ID的
        - 实现ApplicationContextAware接口,作用与BeanFactory类似都是为了获取Spring容器
4. BeanPostProcessor
    - 当经过上述几个步骤后,bean对象已经被正确构造,但如果你想要对象被使用前再进行一些自定义的处理,就可以通过BeanPostProcessor接口实现
    - `postProcessBeforeInitialzation( Object bean, String beanName ) `:当前正在初始化的bean对象会被传递进来,我们就可以对这个bean作任何处理,这个函数会先于InitialzationBean执行,因此称为前置处理,所有Aware接口的注入就是在这一步完成的
5. InitializingBean与init-method
    - 当BeanPostProcessor的前置处理完成后就会进入本阶段,InitializingBean接口只有一个函数:`afterPropertiesSet()`
    - 这一阶段也可以在bean正式构造完成前增加我们自定义的逻辑,但它与前置处理不同,由于该函数并不会把当前bean对象传进来,因此在这一步没办法处理对象本身,只能增加一些额外的逻辑
    - 若要使用它,我们需要让bean实现该接口,并把要增加的逻辑写在该函数中,然后Spring会在前置处理完成后检测当前bean是否实现了该接口,并执行afterPropertiesSet函数
    - 当然,Spring为了降低对客户代码的侵入性,给bean的配置提供了init-method属性,该属性指定了在这一阶段需要执行的函数名,Spring便会在初始化阶段执行我们设置的函数,init-method本质上仍然使用了InitializingBean接口
6. BeanPostProcessor
    - 与第4步类似,如果需要在初始化之后执行一些自定义的处理,就可以通过BeanPostProcessor接口实现
    - `postProcessAfterInitialzation( Object bean, String beanName ) `当前正在初始化的bean对象会被传递进来,我们就可以对这个bean作任何处理,这个函数会在InitialzationBean完成后执行,因此称为后置处理
7. DisposableBean和destroy-method
    - 和init-method一样,通过给destroy-method指定函数,就可以在bean销毁前执行指定的逻辑

### Spring ApplicationContext 容器

- **Application Context** :是BeanFactory的子类,因为古老的BeanFactory无法满足不断更新的spring的需求,于是ApplicationContext就基本上代替了BeanFactory的工作,它可以加载配置文件中定义的 bean,将所有的 bean 集中在一起,当有请求的时候分配 bean
- 最常被使用的 ApplicationContext 接口实现:
    - **FileSystemXmlApplicationContext**:该容器从 XML 文件中加载已被定义的 bean,在这里,你需要提供给构造器 XML 文件的完整路径
    - **ClassPathXmlApplicationContext**:该容器从 XML 文件中加载已被定义的 bean,在这里,你不需要提供 XML 文件的完整路径,只需正确配置 CLASSPATH 环境变量即可,因为,容器会从 CLASSPATH 中搜索 bean 配置文件
    - **WebXmlApplicationContext**:该容器会在一个 web 应用程序的范围内加载在 XML 文件中已被定义的 bean

### 事务

- **事务属性**:事务的一些基本配置,描述了事务策略如何应用到方法上,事务属性包含了5个方面:传播行为,隔离规则,回滚规则,事务超时,是否只读
- **隔离级别**:数据库默认,读未提交,读已提交,可重复读,序列化
- **事务传播行为**:当事务方法被另一个事务方法调用时,必须指定事务应该如何传播,Spring定义了七种传播行为
    - 默认的事务传播行为是`PROPAGATION_REQUIRED`, 它适合于绝大多数的情况:如果当前没有事务,就新建一个事务,如果已经存在一个事务,则加入到这个事务中

### Spring MVC的执行流程

1. `DispatcherServlet`表示前置控制器,是整个Spring MVC的控制中心,用户发出请求,`DispatcherServlet`接收请求并拦截请求
2. `HandlerMapping`为处理器映射,`DispatcherServlet`调用`HandlerMapping`,`HandlerMapping`根据请求url查找`Handler`
3. `HandlerExecution`表示具体的Handler,其主要作用是根据url查找Controller
4. `HandlerExecution`将解析后的信息传递给`DispatcherServlet`,如解析Controller映射等
5. `HandlerAdapter`表示处理器适配器,其按照特定的规则去执行`Handler`
6. `Handler`让具体的Controller执行
7. Controller将具体的执行信息返回给`HandlerAdapter`,如ModelAndView
8. `HandlerAdapter`将逻辑视图或模型传递给`DispatcherServlet`
9. `DispatcherServlet`调用`ViewResolver`将逻辑视图解析为真实视图对象
10. `ViewResolver`将解析的真实视图对象返回`DispatcherServlet`
11. `DispatcherServlet`利用是土地向对模型数据进行渲染
12. 最终视图呈现给客户端

### 过滤器和拦截器

1. 拦截器是基于java的反射机制的,而过滤器是基于函数回调
2. 拦截器不依赖servlet容器,过滤器依赖servlet容器
3. 拦截器只能对action请求起作用,而过滤器则可以对几乎所有的请求起作用
4. 拦截器可以访问action上下文,值栈里的对象,而过滤器不能访问
5. 在action的生命周期中,拦截器可以多次被调用,而过滤器只能在容器初始化时被调用一次
6. 拦截器可以获取IOC容器中的各个bean,而过滤器就不行,这点很重要,在拦截器里注入一个service,可以调用业务逻辑

### Spring MVC 拦截器的执行顺序

- **preHandler**
    - 调用时间:Controller方法处理之前
    - 执行顺序:链式Intercepter情况下,Intercepter按照声明的顺序一个接一个执行
    - 注意事项:若返回false,则中断执行,不会进入afterCompletion
- **postHandler**
    - 调用前提:preHandle返回true
    - 调用时间:Controller方法处理完之后,DispatcherServlet进行视图的渲染之前,也就是说在这个方法中你可以对ModelAndView进行操作
    - 执行顺序:链式Intercepter情况下,Intercepter按照声明的顺序倒着执行
- **afterCompletion**
    - 调用前提:preHandle返回true
    - 调用时间:DispatcherServlet进行视图的渲染之后
    - 注意事项:多用于清理资源

### Spring Security

#### 过滤器

- Spring Security 基本都是通过过滤器来完成配置的身份认证,权限认证以及登出
- Spring Security 在 Servlet 的过滤链(filter chain)中注册了一个过滤器 `FilterChainProxy`,它会把请求代理到 Spring Security 自己维护的多个过滤链,每个过滤链会匹配一些 URL,如果匹配则执行对应的过滤器,过滤链是有顺序的,一个请求只会执行第一条匹配的过滤链,Spring Security 的配置本质上就是新增,删除,修改过滤器
- 默认情况下系统帮我们注入的这 15 个过滤器,分别对应配置不同的需求,例如 `UsernamePasswordAuthenticationFilter` 是用来使用用户名和密码登录认证的过滤器,但是很多情况下登录不止是简单的用户名和密码,又可能是用到第三方授权登录,这个时候我们就需要使用自定义过滤器

#### 核心类

- **SecurityContextHolder**:`SecurityContextHolder` 存储 `SecurityContext` 对象,`SecurityContextHolder` 是一个存储代理,有三种存储模式分别是:
    - `MODE_THREADLOCAL:SecurityContext`:存储在线程中
    - `MODE_INHERITABLETHREADLOCAL`:`SecurityContext` 存储在线程中,但子线程可以获取到父线程中的 `SecurityContext`
    - `MODE_GLOBAL`:`SecurityContext` 在所有线程中都相同
    - `SecurityContextHolder` 默认使用 MODE_THREADLOCAL 模式,`SecurityContext` 存储在当前线程中,调用 `SecurityContextHolder` 时不需要显示的参数传递,在当前线程中可以直接获取到 `SecurityContextHolder` 对象
- **Authentication**:`Authentication` 即验证,表明当前用户是谁,什么是验证,比如一组用户名和密码就是验证,当然错误的用户名和密码也是验证,只不过 Spring Security 会校验失败
- **AuthenticationManager/ProviderManager/AuthenticationProvider**
    - 其实这三者很好区分,`AuthenticationManager` 主要就是为了完成身份认证流程,`ProviderManager`是 `AuthenticationManager` 接口的具体实现类,`ProviderManager` 里面有个记录 `AuthenticationProvider` 对象的集合属性 `providers`
    - 接下来就是遍历 `ProviderManager` 里面的 `providers` 集合,找到和合适的 `AuthenticationProvider`完成身份认证
- **UserDetailsService/UserDetails**:在 `UserDetailsService` 接口中只有一个简单的方法

### 身份认证流程

1. 在运行到 `UsernamePasswordAuthenticationFilter` 过滤器的时候首先是进入其父类 `AbstractAuthenticationProcessingFilter` 的 `doFilter()` 方法中
    - 判断请求的url是否与配置的一致
    - 调用子类的`attemptAuthentication()`方法
    - 根据返回值走认证成功或认证失败的handler(可以配置自定义的handler)
2. `AbstractAuthenticationProcessingFilter`调用`UsernamePasswordAuthenticationFilter`的`attemptAuthentication()`方法完成身份认证
    -  从request中拿到username和password
    - 将username与password封装成一个`UsernamePasswordAuthenticationToken`对象并传入对应provider的`authenticate()`方法中
3. 通过 `AuthenticationManager` 接口实现类 `ProviderManager` 来遍历得到Provider,调用对应的`authenticate()`方法
    - Provider可自定义配置注入
4. 调用Provider的`authenticate()`方法,并返回`Authentication`对象
    - 其中可调用`UserDetailsService`的`loadUserByUsername()`方法返回`UserDetails`对象
    - 根据UserDetails判断用户的状态,或者注入其他的属性,例如权限
    - 通过UserDetails中的属性组装新的AuthenticationToken,并返回

## Mybatis

### MyBatis核心类

### Mybatis的执行过程

1. 读取MyBatis的核心配置文件
2. 构造SqlSessionFactoryBuilder获取SqlSessionFactory
3. SqlSessionFactory创建会话对象SqlSession
4. 使用SqlSession获得Mapper
5. 调用Mapper接口中的方法

### Mybatis的Mapper只有接口没有实现类却能工作的原因

1. 获取已知的加载过的Mapper中获取出MapperProxyFactory,Mapper代理工厂是通过Class.forName反射生成namespace的对应接口的反射对象并将生成的对象传入MapperProxyFactory的构造函数,最后存入knownMappers集合
2. 代理工厂生成动态代理返回,调用MapperProxyFactory的newInstance方法封装InvocationHandler的实现类MapperProxy,最后并返回代理类

### 命名空间

- 在大型项目中,可能存在大量SQL语句,这时候为每个SQL语句起一个唯一的标识就变得并不容易了,为了解决这个问题,在Mybatis中,可以为每个映射文件起一个唯一的命名空间,这样定义在这个映射文件中的每个SQL语句就成了定义在这个命名空间中的一个ID,只要我们能保证每个命名空间中的这个ID是唯一的,即使在不同的映射文件中的语句ID相同,也不会再产生冲突了

### #{ } 和${ }的区别

- `#{}`:这种方式是使用的预编译的方式,一个#{}就是一个占位符,相当于jdbc的占位符PrepareStatement,设置值的时候会加上引号
- `${}`:这种方式是直接拼接的方式,不对数值做预编译,存在sql注入的现象,设置值的时候不会加上引号

### 缓存

- Mybatis中一级缓存是默认开启的,二级缓存默认是不开启的,一级缓存是对于一个sqlSeesion而言,而二级缓存是对于一个nameSpace而言,可以多个SqlSession共享
- 查出的数据都会被默认先放在一级缓存中,只有会话提交或者关闭以后, 一级缓存中的数据才会转到二级缓存中

## 数据结构

### 红黑树

- 平衡二叉树(AVL)为了追求高度平衡,需要通过平衡处理使得左右子树的高度差必须小于等于1,高度平衡带来的好处是能够提供更高的搜索效率,其最坏的查找时间复杂度都是O(logN),但是由于需要维持这份高度平衡,所付出的代价就是当对树种结点进行插入和删除时,需要经过多次旋转实现复衡,这导致AVL的插入和删除效率并不高,而红黑树能够兼顾搜索和插入删除的效率
- **性质**
    1. 每个结点要么是红的要么是黑的
    2. 根结点是黑的
    4. 父子节点之间不能出现两个连续的红节点,即如果一个结点是红的,那么它的两个子节点都是黑的
    5. 对于任意结点而言,其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点
    5. 每个叶结点(叶结点即指树尾端NIL指针或NULL结点)都是黑的
- 红黑树通过将结点进行红黑着色,使得原本高度平衡的树结构被稍微打乱,平衡程度降低,红黑树不追求完全平衡,只要求达到部分平衡,这是一种折中的方案,大大提高了结点删除和插入的效率

## 算法

### 查找算法

#### 二分查找

- 二分查找是一种在有序数组中查找某一特定元素的搜索算法,搜素过程从数组的中间元素开始
- 如果中间元素正好是要查找的元素,则搜素过程结束
- 如果某一特定元素大于或者小于中间元素,则在数组大于或小于中间元素的那一半中查找,而且跟开始一样从中间元素开始比较
- 如果在某一步骤数组已经为空,则表示找不到指定的元素,这种搜索算法每一次比较都使搜索范围缩小一半,其时间复杂度是O(logN)

```java
import java.util.Comparator;

public class MyUtil {

    public static <T extends Comparable<T>> int binarySearch(T[] x, T key) {
        return binarySearch(x, 0, x.length- 1, key);
    }

    // 使用循环实现的二分查找
    public static <T> int binarySearch(T[] x, T key, Comparator<T> comp) {
        int low = 0;
        int high = x.length - 1;
        while (low <= high) {
            int mid = (low + high) >>> 1;
            int cmp = comp.compare(x[mid], key);
            if (cmp < 0) {
                low= mid + 1;
            }
            else if (cmp > 0) {
                high= mid - 1;
            }
            else {
                return mid;
            }
        }
        return -1;
    }

    // 使用递归实现的二分查找
    private static<T extends Comparable<T>> int binarySearch(T[] x, int low, int high, T key) {
        if(low <= high) {
            int mid = low + ((high -low) >> 1);
            if(key.compareTo(x[mid])== 0) {
                return mid;
            }
            else if(key.compareTo(x[mid])< 0) {
                return binarySearch(x,low, mid - 1, key);
            }
            else {
                return binarySearch(x,mid + 1, high, key);
            }
        }
        return -1;
    }
}
```

> **说明**:上面的代码中给出了折半查找的两个版本,一个用递归实现,一个用循环实现,需要注意的是计算中间位置时不应该使用(high+ low) / 2的方式,因为加法运算可能导致整数越界,这里应该使用以下三种方式之一:`low + (high - low) / 2`或`low + (high – low) >> 1`或`(low + high) >>> 1`(>>>是逻辑右移,是不带符号位的右移)

### 排序算法

#### 冒泡排序

- 冒泡排序是一种简单的排序算法,它重复地走访过要排序的数列,一次比较两个元素,如果它们的顺序错误就把它们交换过来,走访数列的工作是重复地进行直到没有再需要交换,也就是说该数列已经排序完成,这个算法的名字由来是因为越小的元素会经由交换慢慢浮到数列的顶端
- **算法描述**
    1. 比较相邻的元素,如果第一个比第二个大,就交换它们两个
    2. 对每一对相邻元素作同样的工作,从开始第一对到结尾的最后一对,这样在最后的元素应该会是最大的数
    3. 针对所有的元素重复以上的步骤,除了最后一个
    4. 重复步骤1~3,直到排序完成

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123345.gif)

```java
public static void bubbleSort(int[] arr) {
    int temp = 0;
    for (int i = arr.length - 1; i > 0; i--) { // 每次需要排序的长度
        for (int j = 0; j < i; j++) { // 从第一个元素到第i个元素
            if (arr[j] > arr[j + 1]) {
                temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
            }
        }
    }
}
```

- **稳定性**:在相邻元素相等时,它们并不会交换位置,所以,冒泡排序是稳定排序
- **适用场景**:冒泡排序思路简单,代码也简单,特别适合小数据的排序,但是,由于算法复杂度较高,在数据量大的时候不适合使用
- **代码优化**:在数据完全有序的时候展现出最优时间复杂度,为O(n),其他情况下,几乎总是O( n^2^ ),因此,算法在数据基本有序的情况下,性能最好
    要使算法在最佳情况下有O(n)复杂度,需要做一些改进,增加一个`swap`的标志,当前一轮没有进行交换时,说明数组已经有序,没有必要再进行下一轮的循环了,直接退出

```java
public static void bubbleSort(int[] arr) {
    int temp = 0;
    boolean swap;
    for (int i = arr.length - 1; i > 0; i--) { // 每次需要排序的长度
        swap=false;
        for (int j = 0; j < i; j++) { // 从第一个元素到第i个元素
            if (arr[j] > arr[j + 1]) {
                temp = arr[j];
                arr[j] = arr[j + 1];
                arr[j + 1] = temp;
                swap=true;
            }
        }
        if (swap==false){
            break;
        }
    }
}
```

#### 选择排序

- 选择排序是一种简单直观的排序算法,它也是一种交换排序算法,和冒泡排序有一定的相似度,可以认为选择排序是冒泡排序的一种改进
- **算法描述**
    1. 在未排序序列中找到最小(大)元素,存放到排序序列的起始位置
    2. 从剩余未排序元素中继续寻找最小(大)元素,然后放到已排序序列的末尾
    3. 重复第二步,直到所有元素均排序完毕

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123157.gif)

```java
public static void selectionSort(int[] arr) {
    int temp, min = 0;
    for (int i = 0; i < arr.length - 1; i++) {
        min = i;
        // 循环查找最小值
        for (int j = i + 1; j < arr.length; j++) {
            if (arr[min] > arr[j]) {
                min = j;
            }
        }
        if (min != i) {
            temp = arr[i];
            arr[i] = arr[min];
            arr[min] = temp;
        }
    }
}
```

- **稳定性**:用数组实现的选择排序是不稳定的,用链表实现的选择排序是稳定的,不过,一般提到排序算法时,大家往往会默认是数组实现,所以选择排序是不稳定的
- **适用场景**:选择排序实现也比较简单,并且由于在各种情况下复杂度波动小,因此一般是优于冒泡排序的,在所有的完全交换排序中,选择排序也是比较不错的一种算法,但是,由于固有的O(n2)复杂度,选择排序在海量数据面前显得力不从心,因此,它适用于简单数据排序

#### 插入排序

- 插入排序是一种简单直观的排序算法,它的工作原理是通过构建有序序列,对于未排序数据,在已排序序列中从后向前扫描,找到相应位置并插入
- **算法描述**
    1. 把待排序的数组分成已排序和未排序两部分,初始的时候把第一个元素认为是已排好序的
    2. 从第二个元素开始,在已排好序的子数组中寻找到该元素合适的位置并插入该位置
    4. 重复上述过程直到最后一个元素被插入有序子数组中

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123205.gif" alt="img" style="zoom:50%;" />

```java
public static void insertionSort(int[] arr){
    for (int i = 1; i < arr.length; ++i){
        int value = arr[i];
        int position = i;
        while (position > 0 && arr[position-1] > value){
            arr[position] = arr[position - 1];
            position--;
        }
        arr[position] = value;
    }
}
```

- **稳定性**:由于只需要找到不大于当前数的位置而并不需要交换,因此,直接插入排序是稳定的排序方法
- **适用场景**:插入排序由于O( n^2^ )的复杂度,在数组较大的时候不适用,但是,在数据比较少的时候,是一个不错的选择,一般做为快速排序的扩充,例如,在STL的sort算法和stdlib的qsort算法中,都将插入排序作为快速排序的补充,用于少量元素的排序,又如,在JDK 7的`java.util.Arrays`所用的`sort()`方法的实现中,当待排数组长度小于47时,会使用插入排序

#### 快速排序

- 快速排序是一个知名度极高的排序算法,其对于大数据的优秀排序性能和相同复杂度算法中相对简单的实现使它注定得到比其他算法更多的宠爱
- **算法描述**
    1. 从数列中挑出一个元素,称为**基准**(pivot)
    2. 重新排序数列,所有比基准值小的元素摆放在基准前面,所有比基准值大的元素摆在基准后面(相同的数可以到任何一边),在这个分区结束之后,该基准就处于数列的中间位置,这个称为分区(partition)操作
    3. 递归地(recursively)把小于基准值元素的子数列和大于基准值元素的子数列排序

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123302.gif)

```java
public int[] sortArray(int[] nums) {
    randomizedQuicksort(nums, 0, nums.length - 1);
    return nums;
}

public void randomizedQuicksort(int[] nums, int l, int r) {
    if (l < r) {
        int pos = randomizedPartition(nums, l, r);
        randomizedQuicksort(nums, l, pos - 1);
        randomizedQuicksort(nums, pos + 1, r);
    }
}

public int randomizedPartition(int[] nums, int l, int r) {
    int i = new Random().nextInt(r - l + 1) + l; // 随机选一个作为基准
    swap(nums, r, i);
    return partition(nums, l, r);
}

public int partition(int[] nums, int l, int r) {
    int pivot = nums[r];
    int i = l;
    for (int j = l; j <= r - 1; ++j) {
        if (nums[j] <= pivot) {
            swap(nums, i, j);
            i++;
        }
    }
    swap(nums, i, r);
    return i;
}

private void swap(int[] nums, int i, int j) {
    int temp = nums[i];
    nums[i] = nums[j];
    nums[j] = temp;
}
```

- **稳定性**:快速排序并不是稳定的,这是因为我们无法保证相等的数据按顺序被扫描到和按顺序存放
- **适用场景**:快速排序在大多数情况下都是适用的,尤其在数据量大的时候性能优越性更加明显,但是在必要的时候,需要考虑下优化以提高其在最坏情况下的性能

#### 堆排序

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123329.jpeg)

- 堆排序就是把最大堆堆顶的最大数取出,将剩余的堆继续调整为最大堆,再次将堆顶的最大数取出,这个过程持续到剩余数只有一个时结束,在堆中定义以下几种操作
    - 最大堆调整(Max-Heapify):将堆的末端子节点作调整,使得子节点永远小于父节点
    - 创建最大堆(Build-Max-Heap):将堆所有数据重新排序,使其成为最大堆
    - 堆排序(Heap-Sort):移除位在第一个数据的根节点,并做最大堆调整的递归运算

- 由于数组都是 Zero-Based,这就意味着我们的堆数据结构模型要发生改变,相应的,几个计算公式也要作出相应调整

    - Parent(i) = floor((i-1)/2),i 的父节点下标
    - Left(i) = 2i + 1,i 的左子节点下标
    - Right(i) = 2(i + 1),i 的右子节点下标

- **堆的建立和维护**
    1. 给定一个无序数组,如何建立为堆？
        - 常规方法是从第一个非叶子结点向下筛选,直到根元素筛选完毕,这个方法叫筛选法,需要循环筛选n/2个元素
        - 但我们还可以借鉴插入排序的思路,我们可以视第一个元素为一个堆,然后不断向其中添加新元素,这个方法叫做插入法,需要循环插入(n-1)个元素

    2. 删除堆顶元素后,如何调整数组成为新堆？

        - 删除后的调整,是把最后一个元素放到堆顶,自上而下比较,插入是把新元素放在末尾,自下而上比较

        - 假定我们已经有一个现成的最小堆,现在我们删除了根元素,但并没有移动别的元素,根元素空了,但其它元素还保持着堆的性质,我们可以把**最后一个元素**(代号A)移动到根元素的位置,如果不是特殊情况,则堆的性质被破坏,但这仅仅是由于A小于其某个子元素,于是,我们可以把A和这个子元素调换位置,如果A大于其所有子元素,则堆调整好了,否则,重复上述过程,A元素在树形结构中不断下沉,直到合适的位置,数组重新恢复堆的性质,上述过程一般称为筛选,方向显然是自上而下
        - 插入一个新元素也是如此,不同的是,我们把新元素放在**末尾**,然后和其父节点做比较,即自下而上筛选

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/v2-c66a7e83189427b6a5a5c378f73c17ca_b.gif" alt="img" style="zoom:150%;" />

- **算法描述**
- 可以建立一个最小堆,然后每次输出根元素,但是,这个方法需要额外的空间(否则将造成大量的元素移动,其复杂度会飙升到O(n^2^)
- 可以建立最大堆,然后我们倒着输出,在最后一个位置输出最大值,次末位置输出次大值,由于每次输出的最大元素会腾出第一个空间

```java
public class ArrayHeap {
    private int[] arr;
    public ArrayHeap(int[] arr) {
        this.arr = arr;
    }
    private int getParentIndex(int child) {
        return (child - 1) / 2;
    }
    private int getLeftChildIndex(int parent) {
        return 2 * parent + 1;
    }
    private void swap(int i, int j) {
        int temp = arr[i];
        arr[i] = arr[j];
        arr[j] = temp;
    }
    /**
     * 调整堆
     */
    private void adjustHeap(int i, int len) {
        int left, right, j;
        left = getLeftChildIndex(i);
        while (left <= len) {
            right = left + 1;
            j = left;
            if (j < len && arr[left] < arr[right]) {
                j++;
            }
            if (arr[i] < arr[j]) {
                swap(array, i, j);
                i = j;
                left = getLeftChildIndex(i);
            } else {
                break; // 停止筛选
            }
        }
    }
    /**
     * 堆排序
     * */
    public void sort() {
        int last = arr.length - 1;
        // 初始化最大堆
        for (int i = getParentIndex(last); i >= 0; --i) {
            adjustHeap(i, last);
        }
        // 堆调整
        while (last >= 0) {
            swap(0, last--);
            adjustHeap(0, last);
        }
    }

}
```

- **稳定性**:堆排序存在大量的筛选和移动过程,属于不稳定的排序算法
- **适用场景**:堆排序在建立堆和调整堆的过程中会产生比较大的开销,在元素少的时候并不适用,但是,在元素比较多的情况下,还是不错的一个选择,尤其是在解决诸如"前n大的数”一类问题时,几乎是首选算法

#### 希尔排序

- 在希尔排序出现之前,计算机界普遍存在排序算法不可能突破O(n^2^)的观点,希尔排序是第一个突破O(n^2^)的排序算法,它是简单插入排序的改进版,希尔排序的提出,主要基于以下两点:
    1. 插入排序算法在数组基本有序的情况下,可以近似达到O(n)复杂度,效率极高
    2. 但插入排序每次只能将数据移动一位,在数组较大且基本无序的情况下性能会迅速恶化
- **算法描述**:先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序,具体算法描述:
    - 选择一个增量序列`t1,t2,...,tk`,其中`ti>tj`,`tk=1`
    - 按增量序列个数k,对序列进行 k 趟排序
    - 每趟排序,根据对应的增量ti,将待排序列分割成若干长度为m 的子序列,分别对各子表进行直接插入排序,仅增量因子为1 时,整个序列作为一个表来处理,表长度即为整个序列的长度

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123335.gif)

- Donald Shell增量

```java
public static void shellSort(int[] arr){
    int temp;
    for (int delta = arr.length/2; delta>=1; delta/=2){                              //对每个增量进行一次排序
        for (int i=delta; i<arr.length; i++){
            for (int j=i; j>=delta && arr[j]<arr[j-delta]; j-=delta){ //注意每个地方增量和差值都是delta
                temp = arr[j-delta];
                arr[j-delta] = arr[j];
                arr[j] = temp;
            }
        }//loop i
    }//loop delta
}
```

- O(n^3/2^) by Knuth

```java
public static void shellSort2(int[] arr){
    int delta = 1;
    while (delta < arr.length/3){//generate delta
        delta=delta*3+1;    // <O(n^(3/2)) by Knuth,1973>: 1, 4, 13, 40, 121, ...
    }
    int temp;
    for (; delta>=1; delta/=3){
        for (int i=delta; i<arr.length; i++){
            for (int j=i; j>=delta && arr[j]<arr[j-delta]; j-=delta){
                temp = arr[j-delta];
                arr[j-delta] = arr[j];
                arr[j] = temp;
            }
        }//loop i
    }//loop delta
}
```

- **希尔排序的增量**:希尔排序的增量数列可以任取,需要的唯一条件是最后一个一定为1(因为要保证按1有序),但是,不同的数列选取会对算法的性能造成极大的影响,上面的代码演示了两种增量
- **注意**:增量序列中每两个元素最好不要出现1以外的公因子(很显然,按4有序的数列再去按2排序意义并不大),下面是一些常见的增量序列
    - 第一种增量是最初Donald Shell提出的增量,即折半降低直到1,据研究,使用希尔增量,其时间复杂度还是O(n^2^)
    - 第二种增量Hibbard:{1, 3, ..., 2k-1},该增量序列的时间复杂度大约是O(n^3/2^)
    - 第三种增量Sedgewick增量:(1, 5, 19, 41, 109,...),其生成序列或者是`9*4i* *- 9*2i + 1`或者是`4i - 3*2i + 1`
- **稳定性**:我们都知道插入排序是稳定算法,但是,Shell排序是一个多次插入的过程,在一次插入中我们能确保不移动相同元素的顺序,但在多次的插入中,相同元素完全有可能在不同的插入轮次被移动,最后稳定性被破坏,因此,Shell排序不是一个稳定的算法
- **适用场景**:Shell排序虽然快,但是毕竟是插入排序,其数量级并没有快速排序O(nLogN)快,在大量数据面前,Shell排序不是一个好的算法,但是,中小型规模的数据完全可以使用它

#### 归并排序

- 归并排序是建立在归并操作上的一种有效的排序算法,该算法是采用分治法的一个非常典型的应用,将已有序的子序列合并,得到完全有序的序列,即先使每个子序列有序,再使子序列段间有序,若将两个有序表合并成一个有序表,称为2路归并
- **算法描述**
    - 递归法(Top-down)
        1. 申请空间,使其大小为两个已经排序序列之和,该空间用来存放合并后的序列
        2. 设定两个指针,最初位置分别为两个已经排序序列的起始位置
        3. 比较两个指针所指向的元素,选择相对小的元素放入到合并空间,并移动指针到下一位置
        4. 重复步骤3直到某一指针到达序列尾
        5. 将另一序列剩下的所有元素直接复制到合并序列尾
    - 迭代法(Bottom-up)
        1. 将序列每相邻两个数字进行归并操作,形成ceil(n/2)个序列,排序后每个序列包含两/一个元素
        2. 若此时序列数不是1个则将上述序列再次归并,形成ceil(n/4)个序列,每个序列包含四/三个元素
        3. 重复步骤2,直到所有元素排序完毕,即序列数为1

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123213.gif)

```java
public static void mergeSort(int[] arr){
    int[] temp =new int[arr.length];
    internalMergeSort(arr, temp, 0, arr.length-1);
}
private static void internalMergeSort(int[] arr, int[] temp, int left, int right){
    //当left==right的时,已经不需要再划分了
    if (left<right){
        int middle = (left+right)/2;
        internalMergeSort(arr, temp, left, middle);          //左子数组
        internalMergeSort(arr, temp, middle+1, right);       //右子数组
        mergeSortedArray(arr, temp, left, middle, right);    //合并两个子数组
    }
}
// 合并两个有序子序列
private static void mergeSortedArray(int arr[], int temp[], int left, int middle, int right){
    int i=left;
    int j=middle+1;
    int k=0;
    while (i<=middle && j<=right){
        temp[k++] = arr[i] <= arr[j] ? arr[i++] : arr[j++];
    }
    while (i <=middle){
        temp[k++] = arr[i++];
    }
    while ( j<=right){
        temp[k++] = arr[j++];
    }
    //把数据复制回原数组
    for (i=0; i<k; ++i){
        arr[left+i] = temp[i];
    }
}
```

- **稳定性**:因为我们在遇到相等的数据的时候必然是按顺序抄写到辅助数组上的,所以,归并排序同样是稳定算法
- **适用场景**:归并排序在数据量比较大的时候也有较为出色的表现(效率上),但是,其空间复杂度O(n)使得在数据量特别大的时候(例如,1千万数据)几乎不可接受,而且,考虑到有的机器内存本身就比较小,因此,采用归并排序一定要注意

#### 计数排序

- 计数排序不是基于比较的排序算法,其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中,作为一种线性时间复杂度的排序,计数排序要求输入的数据必须是有确定范围的整数
- **算法描述**
    1. 找出待排序的数组中最大和最小的元素
    2. 统计数组中每个值为i的元素出现的次数,存入数组C的第i项
    3. 对所有的计数累加(从C中的第一个元素开始,每一项和前一项相加)
    4. 反向填充目标数组:将每个元素i放在新数组的第C(i)项,每放一个元素就将C(i)减去1

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/v2-3c7ddb59df2d21b287e42a7b908409cb_b.gif)

```java
public static void countSort(int[] a, int max, int min) {
    int[] b = new int[a.length];//存储数组
    int[] count = new int[max - min + 1];//计数数组

    for (int num = min; num <= max; num++) {
        //初始化各元素值为0,数组下标从0开始因此减min
        count[num - min] = 0;
    }

    for (int i = 0; i < a.length; i++) {
        int num = a[i];
        count[num - min]++;//每出现一个值,计数数组对应元素的值+1
    }

    for (int num = min + 1; num <= max; num++) {
        //加总数组元素的值为计数数组对应元素及左边所有元素的值的总和
        count[num - min] += sum[num - min - 1]
    }

    for (int i = 0; i < a.length; i++) {
        int num = a[i];//源数组第i位的值
        int index = count[num - min] - 1;//加总数组中对应元素的下标
        b[index] = num;//将该值存入存储数组对应下标中
        count[num - min]--;//加总数组中,该值的总和减少1
    }

    //将存储数组的值一一替换给源数组
    for(int i=0;i<a.length;i++){
        a[i] = b[i];
    }
}
```

- **稳定性**:最后给 b 数组赋值是倒着遍历的,而且放进去一个就将C数组对应的值(表示前面有多少元素小于或等于A[i])减去一,如果有相同的数x1,x2,那么相对位置后面那个元素x2放在(比如下标为4的位置),相对位置前面那个元素x1下次进循环就会被放在x2前面的位置3,从而保证了稳定性
- **适用场景**:排序目标要能够映射到整数域,其最大值最小值应当容易辨别,例如高中生考试的总分数,显然用0-750就OK啦,又比如一群人的年龄,用个0-150应该就可以了,再不济就用0-200喽,另外,计数排序需要占用大量空间,它比较适用于数据比较集中的情况

#### 桶排序

- 桶排序又叫箱排序,是计数排序的升级版,它的工作原理是将数组分到有限数量的桶子里,然后对每个桶子再分别排序(有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序),最后将各个桶中的数据有序的合并起来
- 计数排序是桶排序的一种特殊情况,可以把计数排序当成每个桶里只有一个元素的情况
- **算法描述**
    1. 找出待排序数组中的最大值max,最小值min
    2. 我们使用 动态数组ArrayList 作为桶,桶里放的元素也用 ArrayList 存储,桶的数量为(max-min)/arr.length+1
    3. 遍历数组 arr,计算每个元素 arr[i] 放的桶
    4. 每个桶各自排序
    5. 遍历桶数组,把排序好的元素放进输出数组

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/v2-465190477b7fb90d17aef27c2a213368_720w.jpg" alt="img" style="zoom: 67%;" />

```java
public static void bucketSort(int[] arr){
    int max = Integer.MIN_VALUE;
    int min = Integer.MAX_VALUE;
    for(int i = 0; i < arr.length; i++){
        max = Math.max(max, arr[i]);
        min = Math.min(min, arr[i]);
    }
    //桶数
    int bucketNum = (max - min) / arr.length + 1;
    ArrayList<ArrayList<Integer>> bucketArr = new ArrayList<>(bucketNum);
    for(int i = 0; i < bucketNum; i++){
        bucketArr.add(new ArrayList<Integer>());
    }
    //将每个元素放入桶
    for(int i = 0; i < arr.length; i++){
        int num = (arr[i] - min) / (arr.length);
        bucketArr.get(num).add(arr[i]);
    }
    //对每个桶进行排序
    for(int i = 0; i < bucketArr.size(); i++){
        Collections.sort(bucketArr.get(i));
    }
    System.out.println(bucketArr.toString());
}
```

- **稳定性**:可以看出,在分桶和从桶依次输出的过程是稳定的,但是,由于我们在对每个桶进行排序时使用了其他算法,所以,桶排序的稳定性依赖于这一步,如果我们使用了快排,显然,算法是不稳定的
- **适用场景**:桶排序可用于最大最小值相差较大的数据情况,但桶排序要求数据的分布必须均匀,否则可能导致数据都集中到一个桶中,比如[104,150,123,132,20000], 这种数据会导致前4个数都集中到同一个桶中,导致桶排序失效

#### 基数排序

- 基数排序(Radix Sort)是桶排序的扩展,它的基本思想是:将整数按位数切割成不同的数字,然后按每个位数分别比较
排序过程:将所有待比较数值(正整数)统一为同样的数位长度,数位较短的数前面补零,然后,从最低位开始,依次进行一次排序,这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列
- **算法描述**
  1. 取得数组中的最大数,并取得位数
  2. arr为原始数组,从最低位开始取每个位组成radix数组
  3. 对radix进行计数排序(利用计数排序适用于小范围数的特点)

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/v2-3a6f1e5059386523ed941f0d6c3a136e_b.gif" alt="img" style="zoom:50%;" />

**算法实现**

```java
public abstract class Sorter {
    public abstract void sort(int[] array);
}

public class RadixSorter extends Sorter {

    private int radix;

    public RadixSorter() {
        radix = 10;
    }

    @Override
    public void sort(int[] array) {
        // 数组的第一维表示可能的余数0-radix,第二维表示array中的等于该余数的元素
        // 如:十进制123的个位为3,则bucket[3][] = {123}
        int[][] bucket = new int[radix][array.length];
        int distance = getDistance(array); // 表示最大的数有多少位
        int temp = 1;
        int round = 1; // 控制键值排序依据在哪一位
        while (round <= distance) {
            // 用来计数:数组counter[i]用来表示该位是i的数的个数
            int[] counter = new int[radix];
            // 将array中元素分布填充到bucket中,并进行计数
            for (int i = 0; i < array.length; i++) {
                int which = (array[i] / temp) % radix;
                bucket[which][counter[which]] = array[i];
                counter[which]++;
            }
            int index = 0;
            // 根据bucket中收集到的array中的元素,根据统计计数,在array中重新排列
            for (int i = 0; i < radix; i++) {
                if (counter[i] != 0)
                    for (int j = 0; j < counter[i]; j++) {
                        array[index] = bucket[i][j];
                        index++;
                    }
                counter[i] = 0;
            }
            temp *= radix;
            round++;
        }
    }

    private int getDistance(int[] array) {
        int max = computeMax(array);
        int digits = 0;
        int temp = max / radix;
        while(temp != 0) {
            digits++;
            temp = temp / radix;
        }
        return digits + 1;
    }

    private int computeMax(int[] array) {
        int max = array[0];
        for(int i=1; i<array.length; i++) {
            if(array[i]>max) {
                max = array[i];
            }
        }
        return max;
    }
}
```

- **稳定性**:通过上面的排序过程,我们可以看到,每一轮映射和收集操作,都保持从左到右的顺序进行,如果出现相同的元素,则保持他们在原始数组中的顺序,可见,基数排序是一种稳定的排序
- **适用场景**:基数排序要求较高,元素必须是整数,整数时长度10W以上,最大值100W以下效率较好,但是基数排序比其他排序好在可以适用字符串,或者其他需要根据多个条件进行排序的场景,例如日期,先排序日,再排序月,最后排序年,其它排序算法可是做不了的

#### 总结

| 排序算法     | 平均时间复杂度 | 最坏时间复杂度 | 最好时间复杂度 | 空间复杂度 | 稳定性 |
| ------------ | -------------- | -------------- | -------------- | ---------- | ------ |
| 冒泡排序     | O(n²)          | O(n²)          | O(n)           | O(1)       | 稳定   |
| 直接选择排序 | O(n²)          | O(n²)          | O(n)           | O(1)       | 不稳定 |
| 直接插入排序 | O(n²)          | O(n²)          | O(n)           | O(1)       | 稳定   |
| 快速排序     | O(nlogn)       | O(n²)          | O(nlogn)       | O(nlogn)   | 不稳定 |
| 堆排序       | O(nlogn)       | O(nlogn)       | O(nlogn)       | O(1)       | 不稳定 |
| 希尔排序     | O(nlogn)       | O(n)           | O(n)           | O(1)       | 不稳定 |
| 归并排序     | O(nlogn)       | O(nlogn)       | O(nlogn)       | O(n)       | 稳定   |
| 计数排序     | O(n+k)         | O(n+k)         | O(n+k)         | O(n+k)     | 稳定   |
| 基数排序     | O(N*M)         | O(N*M)         | O(N*M)         | O(M)       | 稳定   |

### 二叉树遍历

- 前序遍历:根结点 ---> 左子树 ---> 右子树
- 中序遍历:左子树---> 根结点 ---> 右子树
- 后序遍历:左子树 ---> 右子树 ---> 根结点
- 层次遍历:只需按层次遍历即可

#### 前序遍历

- 递归实现

```java
public void preOrderTraverse1(TreeNode root) {
    if (root != null) {
        System.out.print(root.val+"  ");
        preOrderTraverse1(root.left);
        preOrderTraverse1(root.right);
    }
}
```

- 非递归的实现
    1. 访问之,并把结点node入栈,当前结点置为左孩子
    2. 判断结点node是否为空,若为空,则取出栈顶结点并出栈,将右孩子置为当前结点,否则重复上一步直到当前结点为空或者栈为空(可以发现栈中的结点就是为了访问右孩子才存储的)

```java
public void preOrderTraverse2(TreeNode root) {
    LinkedList<TreeNode> stack = new LinkedList<>();
    TreeNode pNode = root;
    while (pNode != null || !stack.isEmpty()) {
        if (pNode != null) {
            System.out.print(pNode.val+"  ");
            stack.push(pNode);
            pNode = pNode.left;
        } else { //pNode == null && !stack.isEmpty()
            TreeNode node = stack.pop();
            pNode = node.right;
        }
    }
}
```

#### 中序遍历

- 递归实现

```java
public void inOrderTraverse1(TreeNode root) {
    if (root != null) {
        inOrderTraverse1(root.left);
        System.out.print(root.val+"  ");
        inOrderTraverse1(root.right);
    }
}
```

- 非递归实现,相同的道理,只不过访问的顺序移到出栈时

```java
public void inOrderTraverse2(TreeNode root) {
    LinkedList<TreeNode> stack = new LinkedList<>();
    TreeNode pNode = root;
    while (pNode != null || !stack.isEmpty()) {
        if (pNode != null) {
            stack.push(pNode);
            pNode = pNode.left;
        } else { //pNode == null && !stack.isEmpty()
            TreeNode node = stack.pop();
            System.out.print(node.val+"  ");
            pNode = node.right;
        }
    }
}
```

#### 后序遍历

- 递归实现

```java
public void postOrderTraverse1(TreeNode root) {
  if (root != null) {
    postOrderTraverse1(root.left);
    postOrderTraverse1(root.right);
    System.out.print(root.val+"  ");
  }
}
```

- 非递归实现

```java
public static void postTraverse(TreeNode node) {
    if (node == null)
        return;
    Deque<TreeNode> s = new LinkedList<>();

    TreeNode curNode; //当前访问的结点
    TreeNode lastVisitNode; //上次访问的结点
    curNode = node;
    lastVisitNode = null;

    //把currentNode移到左子树的最下边
    while (curNode != null) {
        s.push(curNode);
        curNode = curNode.left;
    }
    while (!s.isEmpty()) {
        curNode = s.pop();  //弹出栈顶元素
        //一个根节点被访问的前提是:无右子树或右子树已被访问过
        if (curNode.right != null && curNode.right != lastVisitNode) {
            //根节点再次入栈
            s.push(curNode);
            //进入右子树,且可肯定右子树一定不为空
            curNode = curNode.right;
            while (curNode != null) {
                //再走到右子树的最左边
                s.push(curNode);
                curNode = curNode.left;
            }
        } else {
            //访问
            System.out.print(curNode.val + "  ");
            //修改最近被访问的节点
            lastVisitNode = curNode;
        }
    } //while
```

#### 层次遍历

- 层次遍历的代码比较简单,只需要一个队列即可,先在队列中加入根结点,之后对于任意一个结点来说,在其出队列的时候,访问之,同时如果左孩子和右孩子有不为空的,入队列

```java
public void levelTraverse(TreeNode root) {
    if (root == null) {
        return;
    }
    LinkedList<TreeNode> queue = new LinkedList<>();
    queue.offer(root);
    while (!queue.isEmpty()) {
        TreeNode node = queue.poll();
        System.out.print(node.val+"  ");
        if (node.left != null) {
            queue.offer(node.left);
        }
        if (node.right != null) {
            queue.offer(node.right);
        }
    }
}
```

#### 为什么先序中序可以决定一颗树

- 前序和后序在本质上都是将父节点与子结点进行分离,但并没有指明左子树和右子树的能力,因此得到这两个序列只能明确父子关系,而不能确定一个二叉树

## 操作系统

### 原码/反码/补码

- 为了简化计算机集成电路的设计,根据运算法则减去一个正数等于加上一个负数, 即: 1-1 = 1 + (-1) = 0 , 所以机器可以只有加法而没有减法,这样计算机运算的设计就更简单了
- 如果用原码表示, 让符号位也参与计算,显然对于减法来说,结果是不正确的,这也就是为何计算机内部不使用原码表示一个数
- 为了解决原码做减法的问题, 出现了反码,发现用反码计算减法, 结果的真值部分是正确的,而唯一的问题其实就出现在"0"这个特殊的数值上,虽然人们理解上+0和-0是一样的,但是0带符号是没有任何意义的,而且会有[0000 0000]原和[1000 0000]原两个编码表示0
- 于是补码的出现,,解决了0的符号以及两个编码的问题,使用补码,不仅仅修复了0的符号以及存在两个编码的问题,而且还能够多表示一个最低数,这就是为什么8位二进制,使用原码或反码表示的范围为[-127, +127],而使用补码表示的范围为[-128, 127]

### 进程管理

#### 进程与线程

- 进程是操作系统资源分配的基本单位,是程序关于某个数据集合上的一次运行活动
- 线程是进程的一个实体,是CPU调度的基本单位,一个进程可以包含多个线程,它可与同属一个进程的其他的线程共享进程所拥有的全部资源,线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈)
- 进程之间的切换会有较大的开销而线程之间切换的开销小
    - 每当切换进程时,必须要考虑保存当前进程的状态,状态包括存放在内存中的程序的代码和数据,它的栈,通用目的寄存器的内容,程序计数器,环境变量以及打开的文件描述符的集合,这个状态叫做上下文(Context)
    - 同样线程有自己的上下文,包括唯一的整数线程ID,栈,栈指针,程序计数器,通用目的寄存器和条件码,可以理解为线程上下文是进程上下文的子集

#### 上下文切换

- 对于单核单线程CPU而言,在某一时刻只能执行一条CPU指令,上下文切换(Context Switch)是一种将CPU资源从一个进程分配给另一个进程的机制
- 在切换的过程中,操作系统需要先存储当前进程的状态(包括内存空间的指针,当前执行完的指令等等),再读入下一个进程的状态,然后执行此进程
- 从用户角度看,计算机能够并行运行多个进程,这恰恰是操作系统通过快速上下文切换造成的结果

#### 线程同步

- **互斥量**:采用互斥对象机制,只有拥有互斥对象的线程才有访问公共资源的权限,因为互斥对象只有一个,所以可以保证公共资源不会被多个线程同时访问
- **信号量**:它允许同一时刻多个线程访问同一资源,但是需要控制同一时刻访问此资源的最大线程数量
- **事件(信号)**:通过通知操作的方式来保持多线程同步,还可以方便的实现多线程优先级的比较操作

#### 进程同步

- 进程间同步的主要方法有原子操作,信号量机制,自旋锁,管程,会合,分布式系统等

#### 线程间的通信方式

- **锁机制**:包括互斥锁,条件变量,读写锁
    - 互斥锁提供了以排他方式防止数据结构被并发修改的方法
    - 读写锁允许多个线程同时读共享数据,而对写操作是互斥的
    - 条件变量可以以原子的方式阻塞进程,直到某个特定条件为真为止,对条件的试是在互斥锁的保护下进行的,条件变量始终与互斥锁一起使用
- **信号量机制(Semaphore)**:包括无名线程信号量和命名线程信号量
- **信号机制(Signal)**:类似进程间的信号处理

#### 进程间的通信方式

- **管道( pipe )**:管道是一种半双工的通信方式,数据只能单向流动,而且只能在具有亲缘关系的进程间使用,进程的亲缘关系通常是指父子进程关系
-  **有名管道 (namedpipe)** :有名管道也是半双工的通信方式,但是它允许无亲缘关系进程间的通信
-  **信号量(semophore )**:信号量是一个计数器,可以用来控制多个进程对共享资源的访问,它常作为一种锁机制,防止某进程正在访问共享资源时,其他进程也访问该资源,因此,主要作为进程间以及同一进程内不同线程之间的同步手段
-  **消息队列( messagequeue )** :消息队列是由消息的链表,存放在内核中并由消息队列标识符标识,消息队列克服了信号传递信息少,管道只能承载无格式字节流以及缓冲区大小受限等缺点
-  **信号 (sinal )** :信号是一种比较复杂的通信方式,用于通知接收进程某个事件已经发生
-  **共享内存(shared memory )** :共享内存就是映射一段能被其他进程所访问的内存,这段共享内存由一个进程创建,但多个进程都可以访问,共享内存是最快的 IPC 方式,它是针对其他进程间通信方式运行效率低而专门设计的,它往往与其他通信机制,如信号量,配合使用,来实现进程间的同步和通信
-  **套接字(socket )** :套接口也是一种进程间通信机制,与其他通信机制不同的是,它可用于不同及其间的进程通信

#### 进程的状态

- **就绪状态**:进程已获得除处理机以外的所需资源,等待分配处理机资源
- **运行状态**:占用处理机资源运行,处于此状态的进程数小于等于CPU数
- **阻塞状态**:进程等待某种条件(例如IO操作),在条件满足之前无法执行

#### 处理机调度

##### 调度算法

- 调度本质上就是一种资源分配,饥饿指某个进程一直在等待,得不到处理
- 调度算法的分类
    - **抢占式**(当前进程可以被抢):可以暂停某个正在执行的进程,将处理及重新分配给其他进程
    - **非抢占式**(当前进程不能被抢走):一旦处理及分配给了某个进程,他就一直运行下去,直到结束
- **高级调度**(作业调度/长程调度)(频率低):将外存作业调入内存
- **低级调度**(进程调度/短程调度)(频率高):决定就绪队列中哪个进程获得处理机并执行

**进程调度**

1. 先来先服务(FCFS):按照到达顺序,非抢占式,不会饥饿
2. 短作业/进程优先(SJF):抢占/非抢占,会饥饿
3. 高响应比优先(HRRN):综合考虑等待时间和要求服务时间计算一个优先权,非抢占,不会饥饿
4. 时间片轮转(RR):轮流为每个进程服务,抢占式,不会饥饿
5. 优先级:根据优先级,抢占/非抢占,会饥饿
6. 多级反馈队列:
    - 设置多个就绪队列,每个队列的进程按照先来先服务排队,然后按照时间片轮转分配时间片
    - 若时间片用完还没有完成,则进入下一级队尾,只有当前队列为空时,才会为下一级队列分配时间片
    - 抢占式,可能会饥饿

**作业调度**

- 先来先服务调度算法
- 短作业优先调度算法
- 优先级调度算法

##### 中断和轮询

- **轮询**:
    - 轮询(Polling)I／O方式或程序控制I／O方式,是让CPU以一定的周期按次序查询每一个外设,看它是否有数据输入或输出的要求,若有,则进行相应的输入／输出服务,若无,或I／O处理完完毕,CPU就接着查询下一个外设
    - 效率低,等待时间很长,CPU利用率不高
- **中断**:
    - 程序中断通常简称中断,是指CPU在正常运行程序的过程中,由于预选安排或发生了各种随机的内部或外部事件,使CPU中断正在运行的程序,而转到为相应的服务程序去处理,这个过程称为程序中断
    - 提高 CPU 的效率,只有当服务对象向 CPU 发出中断申请时 才去为它服务,这样,就可以利用中断功能同时为多个对象服务,从而大大提高了 CPU 的工作效率

##### 并发与并行

-   并行是指两个或者多个事件在同一时刻发生,而并发是指两个或多个事件在同一时间间隔发生
-   **并发**:一个处理器同时处理多个任务
-   **并行**:多个处理器或者是多核的处理器同时处理多个不同的任务

##### 同步和异步

-   同步和异步关注的是消息通信机制,所谓同步,就是在发出一个调用时,在没有得到结果之前,该调用就不返回,但是一旦调用返回,就得到返回值了,换句话说,就是由调用者主动等待这个调用的结果
-   而异步则是相反,调用在发出之后,这个调用就直接返回了,所以没有返回结果,换句话说,当一个异步过程调用发出后,调用者不会立刻得到结果,而是在调用发出后,被调用者通过状态,通知机制来通知调用者,或通过回调函数处理这个调用

##### 阻塞与非阻塞

-   阻塞与非阻塞关注的是程序在等待调用结果(消息,返回值)时的状态
-   阻塞调用时指调用结果返回之前,当前线程被挂起,调用线程只有在得到结果之后才会返回
-   非阻塞调用时指在不能立刻得到结果之前,该调用不会阻塞当前线程

#### 死锁

-  两个或多个进程被**无限期地阻塞,相互等待**的一种状态
    -   **互斥**:一个资源每次只能被一个进程使用
    -   **请求与保持**:一个进程因请求资源而阻塞时,对已获得的资源保持不放
    -   **不可剥夺**:进程已获得的资源,在末使用完之前,不能强行剥夺
    -   **循环等待**:若干进程之间形成一种头尾相接的循环等待资源关系
-  这四个条件是死锁的必要条件,只要系统发生死锁,这些条件必然成立,而只要上述条件之一不成立,则死锁解除
-  **预防死锁**:
    -  **静态资源分配法**:在进程运行之初,一次性请求所有需要的资源,破坏了循环等待条件
    -  **资源顺序分配法**:规定每个线程必须按顺序请求资源,同类资源一次性申请完,破坏了循环等待条件
    -  **剥夺控制法**:破坏了不可剥夺条件

##### 临界区

- 每个进程中访问临界资源的那段程序称为临界区,每次只准许一个进程进入临界区,进入后不允许其他进程进入
- 任何时候,**处于临界区内的进程不可多于一个**,如已有进程进入自己的临界区,则其它所有试图进入临界区的进程必须等待
- 进入临界区的进程要在**有限时间内退出**,以便其它进程能及时进入自己的临界区
- 如果进程不能进入自己的临界区,则应**让出CPU**,避免进程出现"忙等”现象

#### 用户态和内核态

- 由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级 --**用户态** 和 **内核态**
- **内核态**:CPU可以访问内存所有数据, 包括外围设备, 例如硬盘, 网卡. CPU也可以将自己从一个程序切换到另一个程序
- **用户态**:只能受限的访问内存, 且不允许访问外围设备,占用CPU的能力被剥夺, CPU资源可以被其他程序获取
- **用户态与内核态的切换**:所有用户程序都是运行在用户态的, 但是有时候程序确实需要做一些内核态的事情, 例如从硬盘读取数据, 或者从键盘获取输入等,这是需要切换至内核态
- **系统调用**:这是处于用户态的进程主动请求切换到内核态的一种方式,用户态的进程通过系统调用申请使用操作系统提供的系统调用服务例程来处理任务,在CPU中的实现称之为**陷阱指令**(Trap Instruction)
    1. 用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务
    2. 用户态程序执行陷阱指令
    3. CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问
    4. 这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务
    5. 系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果

### 内存管理

#### 虚拟内存

- 每个进程拥有独立的地址空间,这个空间被分为大小相等的多个块,称为页(Page),每个页都是一段连续的地址,这些页**被映射到物理内存**,但并不是所有的页都必须在内存中才能运行程序
- 对于进程而言,逻辑上似乎有很大的内存空间,实际上其中一部分对应物理内存上的一块(称为帧,通常页和帧大小相等),还有一些没加载在内存中的对应在硬盘上,注意,请求分页系统,请求分段系统和请求段页式系统都是针对虚拟内存的,通过请求实现内存与外存的信息置换　　　　　　　　
- 页表实际上存储在 CPU 的**内存管理单元**(MMU)中,于是 CPU 就可以直接通过 MMU,找出要实际要访问的物理内存地址,而当进程访问的虚拟地址在页表中查不到时,系统会产生一个**缺页异常**,进入系统内核空间分配物理内存,更新进程页表,最后再返回用户空间,恢复进程的运行

#### 页面置换算法

- 如果内存空间不够,操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉,也就是暂时写在硬盘上,称为**换出**(Swap Out),一旦需要的时候,再加载进来,称为**换入**(Swap In),所以,一次性写入磁盘的也只有少数的一个页或者几个页,不会花太多时间,**内存交换的效率就相对比较高,**
- **FIFO先进先出算法**:将先进入的页面置换出去
- **LRU(Least recently use)最近最少使用算法**:选择未使用时间最长的页面置换出去
- **LFU(Least frequently use)最少使用次数算法**:将一段时间内使用次数最少页面置换出去
- **OPT(Optimal replacement)最优置换算法**:理论的最优,将实际内存中最晚使用的页面置换出去

#### 内存碎片

- 内部碎片:给一个进程分配一块空间,这块空间没有用完的部分叫做内部碎片
- 外部碎片:给每个进程分配空间以后,内存中会存在一些区域由于太小而无法利用的空间,叫做外部碎片

#### 连续内存分配方式

- 概念:连续分配为用户分配一个连续的内存空间,比如某个作业需要100mb的内存空间,就为这个作业在内存中划分一个100mb的内存空间

##### 单一连续分配

- 分配方法:将内存去划分为系统区域用户区,系统区为操作系统使用,剩下的用户区给**一个进程或作业**使用
- 特点:操作简单,没有外部碎片,适合单道处理系统,但是会有大量的内部碎片浪费资源,存储效率低

##### 分区式存储管理

###### 固定分区分配

- 分配方法:将内存划分成若干个固定大小的块,分区大小可以相等也可不相等(划分之后不再改变),根据程序的大小,分配当前空闲的,适当大小的分区
- 特点:固定分区分配虽然没有外部碎片,但是会造成大量的内部碎片,分区大小相等缺乏灵活性,大的进程可能放不进去,分区大小不等可能会造成大量的内部碎片,利用率极低

###### 动态分区分配

- 分配方法:不会先划分内存区域,当进程进入内存的时候才会根据进程大小动态的为其建立分区,使分区大小刚好适合进程的需要
- 特点:随着进程的消亡,会出现很多成段的内存空间,时间越来越长就会导致很多不可利用的外部碎片,降低内存的利用率,这时需要分配算法来解决

**分配算法**

- **首次适应算法**:进程进入内存之后从头开始查找第一个适合自己大小的分区,空间分区就是按照地址递增的顺序排列,算法开销小,回收后放到原位置就好,综合看这个算法性能最好
- **最佳适应算法**:将分区从从小到大排列(容量递增),找到最适合自己的分区,这样会有更大的分区被保留下来,满足别的进程需要,但是算法开销大,每次进程消亡产生新的区域后要重新排序,并且当使用多次后会产生很多外部碎片
- **最坏适应算法**:将分区从从大到小排列(容量递减),进程每次都找最大的区域来使用,可以减少难以利用的外部碎片,但是大分区很快就被用完了,大的进程可能会有饥饿的现象,算法开销也比较大
- **邻近适应算法**:空间分区按照地址递增的顺序进行排列,是由首次适应演变而来,进程每次寻找空间,从上一次查找的地址以后开始查找(不同于首次适应,首次适应每次从开头查找),算法开销小,大的分区也会很快被用完

##### 可重定位分区分配

- 分区式存储管理常采用的一项技术就是内存紧缩(compaction):将各个占用分区向内存一端移动,然后将各个空闲分区合并成为一个空闲分区,这种技术在提供了某种程 度上的灵活性的同时,也存在着一些弊端,例如:对占用分区进行内存数据搬移占用CPU时间,如果对占用分区中的程序进行"浮动”,则其重定位需要硬件支持
- 由于若干次内存分配与回收之后,各个空闲的内存块不连续了,通过"重定位”将已经分配的内存"紧凑”在一块,从而空出一大块空闲的内存,"紧凑”是需要开销的,比如需要重新计算地址
- 而离散分配方式—>不管是分页还是分段,都是直接将程序放到各个离散的页中,从而就不存在"紧凑”一说

#### 非连续分配方式

##### 分段管理

- 段式存储管理是一种符合用户视角的内存分配管理方案,在段式存储管理中,将程序的地址空间划分为若干段(segment),如代码段,数据段,堆栈段
- 这样每个进程有一个**二维地址空间**,相互独立,互不干扰
- 段式管理的优点是**没有内碎片**(因为段大小可变,改变段大小来消除内碎片),但**段换入换出时,会产生外碎片**

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/qU5b1kzBolR9Vvy.png" alt="1229382-20200706184122714-1327445390" style="zoom:50%;" />

##### 分页管理

- 在页式存储管理中,将程序的**逻辑地址划分为固定大小的页(page)**,而**物理内存**划分为同样大小的**帧**,程序加载时,可以将任意一页放入内存中任意一个帧,这些帧不必连续,从而实现了离散分离
- 页式存储管理的优点是**没有外碎片**(因为页的大小固定),但**会产生内碎片**(一个页可能填充不满)

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/1229382-20200706184202119-510553889.png" alt="1229382-20200706184202119-510553889" style="zoom:50%;" />

##### 多级页表

- 如果使用了二级分页,一级页表就可以覆盖整个 4GB 虚拟地址空间,但**如果某个一级页表的页表项没有被用到,也就不需要创建这个页表项对应的二级页表了,即可以在需要时才创建二级页表**,做个简单的计算,假设只有 20% 的一级页表项被用到了,那么页表占用的内存空间就只有 4KB(一级页表)+ 20% * 4MB(二级页表)= `0.804MB`,这对比单级页表的 `4MB` 是一个巨大的节约

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/1229382-20200706184216560-1820925289.png" alt="1229382-20200706184216560-1820925289" style="zoom:50%;" />

##### 段页式管理

- 先将程序划分为多个有逻辑意义的段,也就是前面提到的分段机制,接着再把每个段划分为多个页,也就是对分段划分出来的连续空间,再划分固定大小的页
- 这样,地址结构就由**段号,段内页号和页内位移**三部分组成
- 用于段页式地址变换的数据结构是每一个程序一张段表,每个段又建立一张页表,段表中的地址是页表的起始地址,而页表中的地址则为某页的物理页号
- **段页式地址变换中要得到物理地址须经过三次内存访问**
    1. 第一次访问段表,得到页表起始地址
    2. 第二次访问页表,得到物理页号
    3. 第三次将物理页号与页内位移组合,得到物理地址

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/1229382-20200706184248663-2120430485.png" alt="1229382-20200706184248663-2120430485" style="zoom:50%;" />

- **快表**:把最常访问的几个页表项存储到访问速度更快的硬件,在 CPU 芯片中,加入了一个专门存放程序最常访问的页表项的 Cache,这个 Cache 就是 TLB(*Translation Lookaside Buffer*),通常称为页表缓存,转址旁路缓存,快表等

#### 重定位

- 对程序进行重定位的技术按重定位的时机可分为两种:静态重定位和动态重定位
- **静态重定位**:是在目标程序装入内存时,由装入程序对目标程序中的指令和数据的地址进行修改,即把程序的逻辑地址都改成实际的地址,对每个程序来说,这种地址变换只是在装入时一次完成,在程序运行期间不再进行重定位
    - **优点**:是无需增加硬件地址转换机构,便于实现程序的静态连接,在早期计算机系统中大多采用这种方案
        **缺点**
        - 程序的存储空间只能是连续的一片区域,而且在重定位之后就不能再移动,这不利于内存空间的有效使用
        - 各个用户进程很难共享内存中的同一程序的副本
- **动态重定位**:是在程序执行期间每次访问内存之前进行重定位,这种变换是靠硬件地址变换机构实现的,通常采用一个重定位寄存器,其中放有当前正在执行的程序在内存空间中的起始地址,而地址空间中的代码在装入过程中不发生变化,现在一般计算机系统中都采用动态重定位方法
    - **优点**
        - 程序占用的内存空间动态可变,不必连续存放在一处
        - 比较容易实现几个进程对同一程序副本的共享使用
    - **缺点**:是需要附加的硬件支持,增加了机器成本,而且实现存储管理的软件算法比较复杂

### 文件管理

#### 位图

- 位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配
- 在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。

#### inode

- 储存文件元信息的区域就叫做inode,也称为**索引节点**
- 每个inode都有一个编号,操作系统用inode编号来识别不同的文件
    - Unix/Linux系统内部不使用文件名,而使用inode号码来识别文件,对于系统来说,文件名只是inode号码便于识别的别称或者绰号
    - 表面上,用户通过文件名,打开文件,实际上,系统内部这个过程分成三步:首先,系统找到这个文件名对应的inode号码,其次,通过inode号码,获取inode信息,最后,根据inode信息,找到文件数据所在的block,读出数据
- inode包含文件的元信息,具体来说有以下内容
    - 文件的字节数
    - 文件拥有者的User ID
    - 文件的Group ID
    - 文件的读,写,执行权限
    - 文件的时间戳,共有三个:
        - ctime指inode上一次变动的时间
        - mtime指文件内容上一次变动的时间
        - atime指文件上一次打开的时间
    - 链接数,即有多少文件名指向这个inode
    - 文件数据block的位置
- **superblock**:记录此 filesystem 的整体信息,包括inode/block的总量,使用量,剩余量,以及文件系统的格式与相关信息等

#### 文件空间分配

- **连续分配**:为文件分配连续的磁盘块
    - 目录项:起始块号,文件长度
    - 优点:顺序存取速度快,支持随机访问
    - 缺点:会产生碎片,不利于文件扩展
- **链接分配**
    - **隐式链接**:除文件的最后一个盘块之外,每个盘块中都存在有指向下一个盘块的指针
        - 目录项:起始块号,结束块号
        - 优点:可解决碎片问题,外存利用率高,文件扩展实现方便
        - 缺点:只能顺序访问,不能随机访问
    - **显示链接**:建立一张常驻内存的文件分配表(FAT),显示记录盘块的先后关系
        - 目录项:起始块号
        - 优点:除了隐式连接的优点外,还支持随机访问
        - 缺点:FAT需要专用一定的存储空间
- **索引分配**:为文件数据块建立索引表,若文件太大,可采用链接方案,多层索引,混合索引
    - 目录项:链接方案记录的是第一个索引块的块号,多层,混合索引记录的是顶级索引块的块
    - 优点:支持随机访问,易于实现文件的扩展
    - 缺点:索引表需要占用一定的存储空间,访问数块前需要先读入索引块,若采用链接方案,查找索引块时可能需要很多次读磁盘操作
- **显示链接与索引分配的区别**
    - 显示链接分配只是将指针信息按照先后顺序记录在FAT中,解决的隐式链接无法随机访问的问题,但是在逻辑上还是顺序的记录磁盘块的信息
    - 索引分配在逻辑上更像是包含关系,因为索引块记录的是顶层索引块,顶层索引块中记录的是一级索引块,而一级索引块中又记录的是(若有)二级索引块

#### 磁盘模型

- **磁头**磁头是硬盘中对盘片进行读写工作的工具
- **盘片**:硬盘中一般会有多个盘片组成,每个盘片包含两个面,每个盘面都对应地有一个读/写磁头
- **磁道**:当磁盘旋转时,磁头若保持在一个位置上,则每个磁头都会在磁盘表面划出一个圆形轨迹,这些圆形轨迹就叫做磁道
- **扇区**:磁盘上的每个磁道被等分为若干个弧段,这些弧段便是磁盘的扇区
- **柱面**:硬盘通常由重叠的一组盘片构成,每个盘面都被划分为数目相等的磁道,并从外缘的`0`开始编号,具有相同编号的磁道形成一个圆柱,称之为磁盘的柱面
- **容量**:存储容量=磁头数 × 磁道(柱面)数 × 每道扇区数 × 每扇区字节数
- **块/簇**:块是操作系统中最小的逻辑存储单位,操作系统与磁盘打交道的最小单位是磁盘块
    - 在Windows下如NTFS等文件系统中叫做簇
    - 在Linux下如Ext4等文件系统中叫做块(block)
    - 每个簇或者块可以包括2,4,8,16,32,64…2的n次方个扇区

#### 磁盘寻道算法

- **先来先服务算法(FCFS)**:根据进程请求访问磁盘的先后次序进行调度
- **最短寻道时间优先算法(SSTF)**:访问的磁道与当前磁头所在的磁道距离最近,以使每次的寻道时间最短,该算法可以得到比较好的吞吐量,但却不能保证平均寻道时间最短
- **扫描算法(SCAN)电梯调度**:扫描算法不仅考虑到欲访问的磁道与当前磁道的距离,更优先考虑的是磁头的当前移动方向
- **循环扫描算法(CSCAN)**:循环扫描算法是对扫描算法的改进,如果对磁道的访问请求是均匀分布的,当磁头到达磁盘的一端,并反向运动时落在磁头之后的访问请求相对较少,这是由于这些磁道刚被处理,而磁盘另一端的请求密度相当高,且这些访问请求等待的时间较长,为了解决这种情况,循环扫描算法规定磁头单向移动,例如,只自里向外移动,当磁头移到最外的被访问磁道时,磁头立即返回到最里的欲访磁道,即将最小磁道号紧接着最大磁道号构成循环,进行扫描

### IO管理

#### 通道

- 通道是一个独立于 CPU的I/O处理机,它控制I/O设备与内存直接进行数据交换,通道有自己的通道指令,这些通道指令由CPU启动,并在操作结束时向CPU发中断信号
- CPU把数据传输功能下放给通道,这样,通道与CPU分时使用内存(资源),就可以实现CPU与I/O设备的并行工作
- 用通道指令编制通道程序,存入存储器
    - 当需要进行I/O操作时,CPU只需启动通道,然后可以继续执行自身程序
    - 通道则执行通道程序,管理与实现I/O操作
- 整个系统分为二级管理
    - 一级是CPU对通道的管理
    - 二级是通道对设备控制的管理

#### 虚拟设备

- 虚拟设备是通过SPOOLing技术把独占设备变成能为若干用户共享的设备

#### 缓冲

- 缓冲技术是用在外部设备与其他硬件部件之间的一种数据暂存技术,它利用存储器件在外部设备中设置了数据的一个存储区域,称为缓冲区
- 采用缓冲技术的原因是,CPU处理数据速度与设备传输数据速度不相匹配,需要用缓冲区缓解其间的速度矛盾
- 缓冲技术一般有两种用途,一种是用在外部设备与外部设备之间的通信上的,还有一种是用在外部设备和处理器之间的
- **缓冲区溢出**
    - 缓冲区溢出是指当计算机向缓冲区内填充数据时**超过了缓冲区本身的容量**,溢出的数据覆盖在合法数据上,造成缓冲区溢出的主原因是程序中**没有仔细检查用户输入的参数**
    - 危害有以下两点
        - **程序崩溃**,导致拒绝服务
        - 跳转并且**执行一段恶意代码**

#### IO多路复用机制

- select，poll，epoll都是IO多路复用的机制。I/O多路复用就通过一种机制，可以监视多个文件描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。
- select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。
- **select**
    - select本质上是通过轮询文件标识符并进行下一步处理。效率较低，仅知道有I/O事件发生，却不知是哪几个流，只会无差异轮询所有流，找出能读数据或写数据的流进行操作。同时处理的流越多，无差别轮询时间越长
    - 每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大
    - 同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大
    - select支持的文件描述符数量太小了，默认是1024
- **poll**
    - poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，其他的都差不多,管理多个描述符也是进行轮询，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。
- **epoll**
    - 可理解为**event poll**，epoll会把哪个流发生哪种I/O事件通知我们。所以epoll是事件驱动（每个事件关联fd）的，此时我们对这些流的操作都是有意义的。复杂度也降低到了O(1)。
    - select和poll都只提供了一个函数——select或者poll函数。而epoll提供了三个函数，epoll_create是创建一个epoll句柄；epoll_ctl是注册要监听的事件类型；epoll_wait则是等待事件的产生。
    - epoll的解决方案不像select或poll一样每次都把当前进程轮流加入fd对应的设备等待队列中，而只在epoll_ctl时把当前进程挂一遍（这一遍必不可少）并为每个fd指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的fd加入一个就绪链表。epoll_wait的工作实际上就是在这个就绪链表中查看有没有就绪的fd
    - 在epoll_ctl函数中。每次注册新的事件到epoll句柄中时（在epoll_ctl中指定EPOLL_CTL_ADD），会把所有的fd拷贝进内核，而不是在epoll_wait的时候重复拷贝。epoll保证了每个fd在整个过程中只会拷贝一次。

## 计算机网络

### OSI七层模型和协议

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2021-04-21-v2-2d62ba265be486cb94ab531912aa3b9c_720w.jpg)

### TCP协议

#### TCP的可靠性传输

- TCP主要提供了检验和,序列号/确认应答,超时重传,最大消息长度,滑动窗口控制等方法实现了可靠性传输
- **检验和**:通过检验和的方式,接收端可以检测出来数据是否有差错和异常,假如有差错就会直接丢弃TCP段,重新发送,TCP在计算检验和时,会在TCP首部加上一个12字节的伪首部,检验和总共计算3部分:TCP首部,TCP数据,TCP伪首部
- **序列号/确认应答**:发送端发送信息给接收端,接收端会回应一个包,这个包就是应答包,只要发送端有一个包传输,接收端没有回应确认包(ACK包),都会重发,或者接收端的应答包,发送端没有收到也会重发数据,这就可以保证数据的完整性
- **超时重传**:超时重传是指发送出去的数据包到接收到确认包之间的时间,如果超过了这个时间会被认为是丢包了,需要重传
    - 超时重传时间设置要比数据报往返时间(往返时间,简称RTT)长一点
    - 一来一回的时间总是差不多的,都会有一个类似于平均值的概念,比如发送一个包到接收端收到这个包一共是0.5s,然后接收端回发一个确认包给发送端也要0.5s,这样的两个时间就是RTT(往返时间),然后可能由于网络原因的问题,时间会有偏差,称为抖动(方差)
- **最大消息长度**:在建立TCP连接的时候,双方约定一个最大的长度(MSS)作为发送的单位,重传的时候也是以这个单位来进行重传,理想的情况下是该长度的数据刚好不被网络层分块

#### TCP报文首部格式

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210707143947.png)

#### 三次握手四次挥手的过程

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2021-03-12-v2-e8aaab48ff996e5cd8a5b39dc450bd6a_1440w.jpg" alt="img" style="zoom: 33%;" />

- **TCP连接建立过程**:首先Client端发送连接请求报文SYN进入SYN-SEND状态,Server端接受连接后回复ACK报文进入SYN-RCVD状态,Client端接收到ACK报文后也向Server端发送ACK报文进入ESTABLISED状态,Server端收到ACK报文也进入ESTABLISED状态,这样TCP连接就建立了
- **TCP连接断开过程**:Client端发起发送FIN报文中断连接请求进入FIN-WAIT-1状态,Server端接到FIN报文后,如果服务端还有数据没有发送完成,则不必急着关闭Socket,可以继续发送数据,所以服务端先发送ACK报文进入CLOSE-WAIT状态,这个时候Client端收到ACK报文就进入FIN_WAIT-2状态,继续等待Server端的FIN报文,当Server端确定数据已发送完成,则向Client端发送FIN报文进入LAST-ACK状态,Client端收到FIN报文后发送ACK后进入TIME_WAIT状态,如果Server端没有收到ACK则可以重传,Server端收到ACK后进入CLOSED状态,Client端等待了2MSL后依然没有收到回复,则证明Server端已正常关闭,Client进入CLOSED状态
- **为什么要三次握手？**
    - 在只有两次"握手"的情形下,假设Client想跟Server建立连接,但是却因为中途连接请求的数据报丢失了,故Client端不得不重新发送一遍,这个时候Server端仅收到一个连接请求,因此可以正常的建立连接,但是,有时候Client端重新发送请求不是因为数据报丢失了,而是有可能数据传输过程因为网络并发量很大在某结点被阻塞了,这种情形下Server端将先后收到2次请求,并持续等待两个Client请求向他发送数据Cient端实际上只有一次请求,而Server端却有2个响应,极端的情况可能由于Client端多次重新发送请求数据而导致Server端最后建立了N多个响应在等待,因而造成极大的资源浪费
- **为什么要四次挥手？**
    - 假如现在Client想断开跟Server的所有连接,第一步,Client先停止向Server端发送数据,并等待Server的回复,虽然Client不往Server发送数据了,但是因为之前已经建立好平等的连接,所以此时Server也有主动权向Client发送数据,故Server端还得终止主动向Client发送数据,并等待Client的确认
- **为什么建立连接是三次握手,而关闭连接却是四次挥手呢？**
    - 这是因为服务端在LISTEN状态下,收到建立连接请求的SYN报文后,把ACK和SYN放在一个报文里发送给客户端,而关闭连接时,当收到对方的FIN报文时,仅仅表示对方不再发送数据了但是还能接收数据,己方是否现在关闭发送数据通道,需要上层应用来决定,因此,己方ACK和FIN一般都会分开发送
- **为什么客户端最后还要等待2MSL？**
    - MSL(Maximum Segment Lifetime),TCP允许不同的实现可以设置不同的MSL值
    - 第一,保证客户端发送的最后一个ACK报文能够到达服务器,因为这个ACK报文可能丢失,站在服务器的角度看来,我已经发送了FIN+ACK报文请求断开了,客户端还没有给我回应,应该是我发送的请求断开报文它没有收到,于是服务器又会重新发送一次,而客户端就能在这个2MSL时间段内收到这个重传的报文,接着给出回应报文,并且会重启2MSL计时器
    - 第二,防止类似与"三次握手”中提到了的"已经失效的连接请求报文段”出现在本连接中,客户端发送完最后一个确认报文后,在这个2MSL时间中,就可以使本连接持续的时间内所产生的所有报文段都从网络中消失,这样新的连接中不会出现旧连接的请求报文

#### 半连接队列和全连接队列

- 半连接队列,被称为SYN队列
- 全连接队列,被称为 accept队列
- **流程**
    1. 客户端发送SYN包,并进入SYN_SENT状态
    2. 服务端接收到数据包将相关信息放入半连接队列(SYN 队列),并返回SYN+ACK包给客户端
    3. 服务端接收客户端ACK数据包,这时如果全连接队列(accept 队列)没满,就会从半连接队列里面将数据取出来放入全连接队列,等待应用使用,当队列已满就会跟据tcp_abort_on_overflow配置执行策略
- 全连接队列大小取决于backlog 和somaxconn 的最小值,也就是 min(backlog,somaxconn)
    - somaxconn 是Linux内核参数,默认128,可通过/proc/sys/net/core/somaxconn进行配置
    - backlog是 listen(int sockfd,int backlog)函数中的参数backlog,Tomcat 默认100,Nginx 默认511.

#### 流量控制

- 如果发送者发送数据过快,接收者来不及接收,那么就会有分组丢失,为了避免分组丢失,控制发送者的发送速度,使得接收者来得及接收,这就是流量控制,流量控制根本目的是防止分组丢失,它是构成TCP可靠性的一方面
- **如何实现流量控制**
    - 由滑动窗口协议(连续ARQ协议)实现,滑动窗口协议既保证了分组无差错,有序接收,也实现了流量控制,主要的方式就是接收方返回的 ACK 中会包含自己的接收窗口的大小,并且利用大小来控制发送方的数据发送
- **怎么避免流量控制引发的死锁**
    - 当发送者收到了一个窗口为0的应答,发送者便停止发送,等待接收者的下一个应答,但是如果这个窗口不为0的应答在传输过程丢失,发送者一直等待下去,而接收者以为发送者已经收到该应答,等待接收新数据,这样双方就相互等待,从而产生死锁
    - 为了避免流量控制引发的死锁,TCP使用了持续计时器,每当发送者收到一个零窗口的应答后就启动该计时器,时间一到便主动发送报文询问接收者的窗口大小,若接收者仍然返回零窗口,则重置该计时器继续等待,若窗口不为0,则表示应答报文丢失了,此时重置发送窗口后开始发送,这样就避免了死锁的产生

#### 拥塞控制

**慢开始算法**

- 发送方维持一个叫做拥塞窗口cwnd(congestion window)的状态变量,拥塞窗口的大小取决于网络的拥塞程度,并且动态地在变化,发送方让自己的发送窗口等于拥塞窗口,另外考虑到接受方的接收能力,发送窗口可能小于拥塞窗口
- 慢开始算法的思路就是,不要一开始就发送大量的数据,先探测一下网络的拥塞程度,也就是说由小到大逐渐增加拥塞窗口的大小

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123119.jpeg)

- 从上图可以看到,一个传输轮次所经历的时间其实就是往返时间RTT,而且每经过一个传输轮次(transmission round),拥塞窗口cwnd就加倍
- 为了防止cwnd增长过大引起网络拥塞,还需设置一个慢开始门限ssthresh状态变量,ssthresh的用法如下
    - 当cwnd<ssthresh时,使用慢开始算法
    - 当cwnd>ssthresh时,改用拥塞避免算法
    - 当cwnd=ssthresh时,慢开始与拥塞避免算法任意
- **注意**,这里的"慢”并不是指cwnd的增长速率慢,而是指在TCP开始发送报文段时先设置cwnd=1,然后逐渐增大,这当然比按照大的cwnd一下子把许多报文段突然注入到网络中要"慢得多”

**拥塞避免算法**

- 拥塞避免算法让拥塞窗口缓慢增长,即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1,而不是加倍,这样拥塞窗口按线性规律缓慢增长
- 无论是在慢开始阶段还是在拥塞避免阶段,只要发送方判断网络出现拥塞,就把慢开始门限ssthresh设置为出现拥塞时的发送窗口大小的一半(但不能小于2),然后把拥塞窗口cwnd重新设置为1,执行慢开始算法,这样做的目的就是要迅速减少主机发送到网络中的分组数,使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123131.jpeg" alt="img" style="zoom:50%;" />

- 乘法减小(Multiplicative Decrease)和加法增大(Additive Increase)
    - **乘法减小**:指的是无论是在慢开始阶段还是在拥塞避免阶段,只要发送方判断网络出现拥塞,就把慢开始门限ssthresh设置为出现拥塞时的发送窗口大小的一半,并执行慢开始算法,所以当网络频繁出现拥塞时,ssthresh下降的很快,以大大减少注入到网络中的分组数
    - **加法增大**:是指执行拥塞避免算法后,使拥塞窗口缓慢增大,以防止过早出现拥塞,常合起来成为AIMD算法
- **注意**:"拥塞避免”并非完全能够避免了阻塞,而是使网络比较不容易出现拥塞

**快重传算法**

- 快重传要求接收方在收到一个失序的报文段后就立即发出重复确认,使发送方及早知道有报文段没有到达对方,而不要等到自己发送数据时捎带确认,快重传算法规定,发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段,而不必继续等待设置的重传计时器时间到期

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123139.jpeg" alt="img" style="zoom: 67%;" />

**快恢复算法**

- 快重传配合使用的还有快恢复算法:当发送方连续收到三个重复确认时,就执行"乘法减小”算法,把ssthresh门限减半(为了预防网络发生拥塞),考虑到如果网络出现拥塞的话就不会收到好几个重复的确认,所以发送方现在认为网络可能没有出现拥塞,所以此时不执行慢开始算法,而是将cwnd设置为ssthresh减半后的值,然后执行拥塞避免算法,使cwnd缓慢增大

<img src="https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/20210611123148.jpeg" alt="img" style="zoom: 50%;" />

- **注意**:在采用快恢复算法时,慢开始算法只是在TCP连接建立时和网络出现超时时才使用

**拥塞控制和流量控制的区别**

- **拥塞控制**:拥塞控制是作用于网络的,它是防止过多的数据注入到网络中,避免出现网络负载过大的情况,常用的方法
    - 慢开始,拥塞避免
    - 快重传,快恢复
- **流量控制**:流量控制是作用于接收者的,它是控制发送者的发送速度从而使接收者来得及接收,防止分组丢失的

### UDP协议

- UDP用户数据报协议,是面向无连接的通讯协议,由于通讯不需要连接,所以可以实现广播发送
- UDP通讯时不需要接收方确认,属于不可靠的传输,可能会出现丢包现象
- 每个UDP报文分UDP报头和UDP数据区两部分,报头由四个16位长(2字节)字段组成,分别说明该报文的源端口,目的端口,报文长度以及校验值,UDP报头由4个域组成,其中每个域各占用2个字节
- 使用UDP协议包括:TFTP(简单文件传输协议),SNMP(简单网络管理协议),DNS(域名解析协议),NFS,BOOTP
- **TCP 与 UDP 的区别**:TCP是面向连接的,可靠的字节流服务,UDP是面向无连接的,不可靠的数据报服务

### HTTP协议

#### HTTP与HTTPS的区别

- HTTPS协议需要到CA申请证书,一般免费证书较少,因而需要一定费用
- HTTP是超文本传输协议,信息是明文传输,HTTPS则是具有安全性的SSL加密传输协议
- HTTP和HTTPS使用的是完全不同的连接方式,用的端口也不一样,前者是80,后者是443

**HTTPS的工作原理**

1. 客户端使用HTTPS的URL访问Web服务器,要求与Web服务器建立SSL连接
2. Web服务器收到客户端请求后,会将网站的证书信息(证书中包含公钥)传送一份给客户端
3. 客户端的浏览器与Web服务器开始协商SSL连接的安全等级,也就是信息加密的等级,客户端的浏览器根据双方同意的安全等级,建立会话密钥,然后利用网站的公钥将会话密钥加密,并传送给服务器
5. Web服务器利用自己的私钥解密出会话密钥,Web服务器利用会话密钥加密与客户端之间的通信

**SSL四次握手**

1. 客户端请求建立SSL链接,并向服务端发送一个随机数–Client random和客户端支持的加密方法,比如RSA公钥加密,此时是明文传输
2. 服务端回复一种客户端支持的加密方法,一个随机数–Server random,授信的服务器证书和非对称加密的公钥
3. 客户端收到服务端的回复后利用服务端的公钥,加上新的随机数–Premaster secret 通过服务端下发的公钥及加密方法进行加密,发送给服务器
4. 服务端收到客户端的回复,利用已知的加解密方式进行解密,同时利用Client random,Server random和Premaster secret通过一定的算法生成HTTP链接数据传输的对称加密key – session key

- 此后的HTTP链接数据传输即通过对称加密方式进行加密传输

**对称加密与非对称加密**

- **对称加密**:指的就是加,解密使用的同是一串密钥,所以被称做对称加密,对称加密只有一个密钥作为私钥,常见的对称加密算法:DES,AES等
- **非对称加密**:指的是加,解密使用不同的密钥,一把作为公开的公钥,另一把作为私钥,公钥加密的信息,只有私钥才能解密,反之,私钥加密的信息,只有公钥才能解密

#### HTTP长连接短连接

- Connection:keep-alive
- **无状态连接**:
- **短连接**
    - 建立连接——数据传输——关闭连接...建立连接——数据传输——关闭连接
    - 由于Web服务器不保存发送请求的Web浏览器进程的任何信息,因此HTTP协议属于无状态协议(Stateless Protocol)
- **长连接**的操作步骤是:
    - 建立连接——数据传输...(保持连接)...数据传输——关闭连接
    - 多用于操作频繁,点对点的通讯,而且连接数不能太多情况,,每个TCP连接都需要三步握手,这需要时间,如果每个操作都是先连接,再操作的话那么处理速度会降低很多,所以每个操作完后都不断开,下次处理时直接发送数据包就OK了,不用建立TCP连接,例如:数据库的连接用长连接,如果用短连接频繁的通信会造成 socket 错误,而且频繁的 socket 创建也是对资源的浪费
    - 从`HTTP/1.1`起,默认都开启了Keep-Alive,保持连接特性,简单地说,当一个网页打开完成后,客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭,如果客户端再次访问这个服务器上的网页,会继续使用这一条已经建立的连接
    - `Keep-Alive`不会永久保持连接,它有一个保持时间,可以在不同的服务器软件(如Apache)中设定这个时间

#### 常见状态码

| 状态码 | 原因短语                               |
| :----- | :------------------------------------- |
| 200    | OK (成功)                              |
| 301    | Moved Permanently (永久移动)           |
| 302    | Found (临时移动)                       |
| 304    | Not Modified (未修改)                  |
| 400    | Bad Request (错误请求)                 |
| 401    | Unauthorized (未授权)                  |
| 403    | Forbidden (禁止访问)                   |
| 404    | Not Found (未找到)                     |
| 500    | Internal Server Error (内部服务器错误) |
| 502    | Bad Gateway (网关错误)                 |
| 503    | Service Unavailable (服务不可用)       |

#### POST 与 GET 的区别

- **请求参数**:GET请求参数是通过URL传递的,多个参数以&连接,POST请求放在RequestBody中
- **请求缓存**:GET请求会被缓存,而POST请求不会,除非手动设置
- **安全性**:POST比GET安全,GET请求在浏览器回退时是无害的,而POST会再次请求
- **历史记录**:GET请求参数会被完整保留在浏览历史记录里,而POST中的参数不会被保留
- **编码方式**:GET请求只能进行URL编码,而POST支持多种编码方式
- **对参数的数据类型**:GET只接受ASCII字符,而POST没有限制

### ARP协议

- 地址解析协议,即ARP(Address Resolution Protocol),是根据IP地址获取物理MAC地址的一个TCP/IP协议
- 主机发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机,并接收返回消息,以此确定目标的物理地址
- 收到返回消息后将该IP地址和物理地址存入本机ARP缓存中并保留一定时间,下次请求时直接查询ARP缓存以节约资源

### RARP协议

- 逆地址解析协议,即RARP,功能和ARP协议相对,其将局域网中某个主机的物理地址转换为IP地址
- 比如局域网中有一台主机只知道物理地址而不知道IP地址,那么可以通过RARP协议发出征求自身IP地址的广播请求,然后由RARP服务器负责回答

### ICMP协议

- Ping 的原理是 ICMP 协议:确认IP包是否成功送达目标地址以及通知在发送过程当中IP包被废弃的具体原因
- 差错报文,例如:差错报文,时间超过报文
- 询问报文:回送请求,应答报文,时间戳报文

### IP协议

#### IP数据报的格式

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2020-12-10-UCtIPhfpVnv7DuF.png)

#### 分类的IP地址

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2020-12-10-G9oiYI8saleWEfn.png)

![](https://raw.githubusercontent.com/LuShan123888/Files/main/Pictures/2020-12-10-la9Hrsd8OPA1Jk6.png)

### 路由转发算法

1. 先看是不是路由器就在目标网络里,如果在直接发给目的主机
2. 若路由表中有这个主机的地址,就直接发送给这个主机
3. 如果路由表有这个网络的路由地址,就发送到目标网络路由中去
4. 再没有就发送到默认路由
5. 都没有就直接ICMP差错报文

### 路由选择协议

#### RIP路由协议

- 它选择路由的度量标准(metric)是跳数,最大跳数是15跳,如果大于15跳,它就会丢弃数据包
- 每30s都都广播一次RIP路由更新信息,把跳数最少的路径更新

#### OSPF协议

- Open Shortest Path First开放式最短路径优先,底层是迪杰斯特拉算法,是链路状态路由选择协议,它选择路由的度量标准是带宽,延迟
- 直接广播,利用Dijkstra算法构造最优的路由表

### DNS协议

- DNS就是进行域名解析的服务器,可以简单地理解为将URL转换为IP地址

1. 浏览器将会检查缓存中有没有这个域名对应的解析过的IP地址,如果有该解析过程将会结束
2. 如果用户的浏览器中缓存中没有,操作系统会先检查自己本地的hosts文件是否有这个网址映射关系,如果有,就先调用这个IP地址映射关系,完成域名解析
3. 如果hosts里没有这个域名的映射,则查找本地DNS解析器缓存,是否有这个网址映射关系或缓存信息,如果有,直接返回给浏览器,完成域名解析
4. 如果hosts与本地DNS解析器缓存都没有相应的网址映射关系,则会首先找本地DNS服务器,一般是公司内部的DNS服务器,此服务器收到查询,如果此本地DNS服务器查询到相对应的IP地址映射或者缓存信息,则返回解析结果给客户机,完成域名解析,此解析具有权威性
5. 如果本地DNS服务器无法查询到,则根据本地DNS服务器设置的转发器进行查询
    - **未用转发模式**:本地DNS就把请求发至根DNS进行(迭代)查询,根DNS服务器收到请求后会判断这个域名(.com)是谁来授权管理,并会返回一个负责该顶级域名服务器的一个IP,本地DNS服务器收 到IP信息后,将会联系负责.com域的这台服务器,这台负责.com域的服务器收到请求后,如果自己无法解析,它就会找一个管理.com域的下一级 DNS服务器地址给本地DNS服务器,当本地DNS服务器收到这个地址后,就会找域名域服务器,重复上面的动作,进行查询,直至找到域名对应的主机
    - **使用转发模式**:此DNS服务器就会把请求转发至上一级DNS服务器,由上一级服务器进行解析,上一级服务器如果不能解析,或找根DNS或把转请求转至上上级,以此循环,不管是本地DNS服务器用是是转发,还是根提示,最后都是把结果返回给本地DNS服务器,由此DNS服务器再返回给客户机

### 网络地址转换

- NAT的实现方式有三种,即静态转换Static Nat,动态转换Dynamic Nat 和 端口多路复用OverLoad
- **静态转换**(Static Nat):是指将内部网络的私有IP地址转换为公有IP地址,IP地址对是一对一的,是一成不变的,某个私有IP地址只转换为某个公有IP地址,借助于静态转换,可以实现外部网络对内部网络中某些特定设备(如服务器)的访问
- **动态转换**(Dynamic Nat):是指将内部网络的私有IP地址转换为公用IP地址时,IP地址对是不确定的,而是随机的,所有被授权访问上Internet的私有IP地址可随机转换为任何指定的合法IP地址,也就是说,只要指定哪些内部地址可以进行转换,以及用哪些合法地址作为外部地址时,就可以进行动态转换,动态转换可以使用多个合法外部地址集,当ISP提供的合法IP地址略少于网络内部的计算机数量时,可以采用动态转换的方式
- **端口多路复用**(OverLoad):是指改变外出数据包的源端口并进行端口转换,即端口地址转换(PAT,Port Address Translation).采用端口多路复用方式,内部网络的所有主机均可共享一个合法外部IP地址实现对Internet的访问,从而可以最大限度地节约IP地址资源,同时,又可隐藏网络内部的所有主机,有效避免来自internet的攻击,因此,目前网络中应用最多的就是端口多路复用方式

### DHCP协议

- DHCP协议采用UDP作为传输协议,主机发送请求消息到DHCP服务器的67号端口,DHCP服务器回应应答消息给主机的68号端口
- 服务器控制一段IP地址范围,客户机登录服务器时就可以自动获得服务器分配的IP地址和子网掩码

### 网络字节序和主机字节序

- 主机字节序:自己的主机内部,内存中数据的处理方式,可以分为两种
    - 大端字节序:按照内存的增长方向,高位数据存储于低位内存中(最直观的字节序)
    - 小端字节序:按照内存的增长方向,高位数据存储于高位内存中(计算机电路先处理低位字节,效率比较高)
- 网络字节序:网络数据流也有大小端之分,网络数据流的地址规定:先发出的数据是低地址,后发出的数据是高地址,发送主机通常将发送缓冲区中的数据按内存地址从低到高的顺序发出,为了不使数据流乱序,接收主机也会把从网络上接收的数据按内存地址从低到高的顺序保存在接收缓冲区中
- TCP/IP协议规定:网络数据流应采用大端字节序,即低地址高字节

### 网络连接过程

1. 读取本地缓存
2. DNS解析,找到IP地址
3. 根据IP地址,找到对应的服务器
4. 建立TCP连接( 三次握手)
5. 连接建立后,发出HTTP请求
6. 服务器根据请求作出HTTP响应
7. 浏览器得到响应内容,进行解析与渲染,并显示
8. 断开连接(四次挥手)

### 断点续传

- HTTP 1.1默认支持断点续传。 
- **Range**:用于客户端到服务端的请求，可以通过改字段指定下载文件的某一段大小及其单位，字节偏移从0开始。
- **If-Range**:用于客户端到服务端的请求，用于判断实体是否发生改变，必须与Range配合使用。若实体未被修改，则响应所缺少的那部分；否则，响应整个新的实体。
- **Accept-Ranges**:用于server到client的应答，client通过该自段判断server是否支持断点续传。
- **Content-Ranges**:用于sever到client的应答，与Accept-Ranges在同一个报文内，通过该字段指定了返回的文件资源的字节范围。
- **状态码**：断点续传，如果返回文件的一部分，则使用HTTP 206状态码；如果返回整个文件，则使用HTTP 200响应码。

## 分布式与微服务

- 随着互联网的发展,网站应用的规模不断扩大,常规的垂直应用架构已无法应对,分布式服务架构势在必行
- 分布式系统是由一组通过网络进行通信,为了完成共同的任务而协调工作的计算机节点组成的系统,其目的是**利用更多的机器,处理更多的数据**
- **微服务**是一种开发软件的架构和组织方法,其中软件由通过明确定义的API 进行通信的小型独立服务组成,服务之间通过接口来进行交互,接口契约不变的情况下可独立变化

### CAP 理论

- CAP 理论指出对于一个分布式计算系统来说,不可能同时满足以下三点
    - **一致性**:代表更新操作成功后,所有节点在同一时间的数据完全一致
    - **可用性**:代表用户访问数据时,系统是否能在正常响应时间内返回预期的结果
    - **分区容错性**:代表分布式系统在遇到节点或网络故障时,仍然能够对外提供一致性或可用性服务
- **权衡**
    - 在分布式系统中,分区容忍性必不可少,因为需要总是假设网络是不可靠的,因此,CAP 理论实际上是要在可用性和一致性之间做权衡
    - 可用性和一致性往往是冲突的,很难使它们同时满足,在多个节点之间进行数据同步时
        - 为了保证一致性(CP),不能访问未同步完成的节点,也就失去了部分可用性
        - 为了保证可用性(AP),允许读取所有节点的数据,但是数据可能不一致
    - Zookeeper 保证的是 CP,对比 Spring Cloud 系统中的注册中心 eruka 实现的是 AP

### BASE 理论

- BASE 是 Basically Available(基本可用),Soft-state(软状态) 和 Eventually Consistent(最终一致性) 三个短语的缩写
- BASE 理论是对 CAP 中一致性和可用性权衡的结果,它的核心思想是:即使无法做到强一致性,但每个应用都可以根据自身业务特点,采用适当的方式来使系统达到最终一致性
    - **基本可用**:指分布式系统在出现故障的时候,保证核心可用,允许损失部分可用性,例如,电商在做促销时,为了保证购物系统的稳定性,部分消费者可能会被引导到一个降级的页面
    - **软状态**:指允许系统中的数据存在中间状态,并认为该中间状态不会影响系统整体可用性,即允许系统不同节点的数据副本之间进行同步的过程存在时延
    - **最终一致性**:
        - 最终一致性强调的是系统中所有的数据副本,在经过一段时间的同步后,最终能达到一致的状态
        - ACID 要求强一致性,通常运用在传统的数据库系统上,而 BASE 要求最终一致性,通过牺牲强一致性来达到可用性,通常运用在大型分布式系统中
        - 在实际的分布式场景中,不同业务单元和组件对一致性的要求是不同的,因此 ACID 和 BASE 往往会结合在一起使用

### RPC

- RPC(Remote Procedure Call)是指远程过程调用,是一种进程间通信方式,是一种技术的思想,而不是规范,它允许程序调用另一个地址空间(通常是共享网络的另一台机器上)的过程或函数,而不用程序员显式编码这个远程调用的细节

### Dubbo

- Apache Dubbo 是一款高性能,轻量级的开源Java RPC框架,它提供了三大核心能力:面向接口的远程方法调用,智能容错和负载均衡,以及服务自动注册和发现
- **服务提供者**(Provider):暴露服务的服务提供方,服务提供者在启动时,向注册中心注册自己提供的服务
- **服务消费者**(Consumer):调用远程服务的服务消费方,服务消费者在启动时,向注册中心订阅自己所需的服务,服务消费者,从提供者地址列表中,基于软负载均衡算法,选一台提供者进行调用,如果调用失败,再选另一台调用
- **注册中心**(Registry):注册中心返回服务提供者地址列表给消费者,如果有变更,注册中心将基于长连接推送变更数据给消费者
- **监控中心**(Monitor):服务消费者和提供者,在内存中累计调用次数和调用时间,定时每分钟发送一次统计数据到监控中心
- **调用关系说明**
    1.  服务容器负责启动,加载,运行服务提供者
    2.  服务提供者在启动时,向注册中心注册自己提供的服务
    3.  服务消费者在启动时,向注册中心订阅自己所需的服务
    4.  注册中心返回服务提供者地址列表给消费者,如果有变更,注册中心将基于长连接推送变更数据给消费者
    5.  服务消费者,从提供者地址列表中,基于软负载均衡算法,选一台提供者进行调用,如果调用失败,再选另一台调用
    6.  服务消费者和提供者,在内存中累计调用次数和调用时间,定时每分钟发送一次统计数据到监控中心

### 分布式事务

- **两阶段提交**(Two-phase Commit,2PC):通过引入协调者(Coordinator)来协调参与者的行为,并最终决定这些参与者是否要真正执行事务
    - 第一阶段(prepare):协调者询问参与者事务是否执行成功,参与者发回事务执行结果,询问可以看成一种投票,需要参与者都同意才能执行
    - 第二阶段 (commit/rollback):如果事务在每个参与者上都执行成功,事务协调者发送通知让参与者提交事务,否则,协调者发送通知让参与者回滚事务,需要注意的是,在准备阶段,参与者执行了事务,但是还未提交,只有在提交阶段接收到协调者发来的通知后,才进行提交或者回滚
    - **存在的问题**
        - **同步阻塞**:所有事务参与者在等待其它参与者响应的时候都处于同步阻塞等待状态,无法进行其它操作
        - **单点问题**:协调者在 2PC 中起到非常大的作用,发生故障将会造成很大影响,特别是在提交阶段发生故障,所有参与者会一直同步阻塞等待,无法完成其它操作
        - **数据不一致**:在提交阶段,如果协调者只发送了部分 Commit 消息,此时网络发生异常,那么只有部分参与者接收到 Commit 消息,也就是说只有部分参与者提交了事务,使得系统数据不一致
        - **太过保守**:任意一个节点失败就会导致整个事务失败,没有完善的容错机制
- **本地消息表**:本地消息表与业务数据表处于同一个数据库中,这样就能利用本地事务来保证在对这两个表的操作满足事务特性,并且使用了消息队列来保证最终一致性
    1. 在分布式事务操作的一方完成写业务数据的操作之后向本地消息表发送一个消息,本地事务能保证这个消息一定会被写入本地消息表中
    2. 之后将本地消息表中的消息转发到消息队列中,如果转发成功则将消息从本地消息表中删除,否则继续重新转发
    3. 在分布式事务操作的另一方从消息队列中读取一个消息,并执行消息中的操作
- **尽最大努力通知**
    - 最大努力通知是最简单的一种柔性事务,适用于一些最终一致性时间敏感度低的业务,且被动方处理结果不影响主动方的处理结果
    - 这个方案的大致意思就是:
        1. 系统 A 本地事务执行完之后,发送个消息到 MQ
        2. 这里会有个专门消费 MQ 的服务,这个服务会消费 MQ 并调用系统 B 的接口
        3. 要是系统 B 执行成功就 OK 了,要是系统 B 执行失败了,那么最大努力通知服务就定时尝试重新调用系统 B, 反复 N 次,最后还是不行就放弃

### 一致性Hash

- 将Hash空间 [0, 2^32^-1] 看成一个Hash环,每个服务器节点都配置到Hash环上,每个数据对象通过Hash取模得到Hash值之后,存放到Hash环中顺时针方向第一个大于等于该Hash值的节点上
- 一致性Hash在增加或者删除节点时只会影响到Hash环中相邻的节点,例如新增节点 X,只需要将它前一个节点 C 上的数据重新进行分布即可,对于其他节点
- **虚拟节点**
    - 上面描述的一致性Hash存在数据分布不均匀的问题,节点存储的数据量有可能会存在很大的不同
    - 数据不均匀主要是因为节点在Hash环上分布的不均匀,这种情况在节点数量很少的情况下尤其明显
    - 解决方式是通过增加虚拟节点,然后将虚拟节点映射到真实节点上,虚拟节点的数量比真实节点来得多,那么虚拟节点在Hash环上分布的均匀性就会比原来的真实节点好,从而使得数据分布也更加均匀

## 设计模式

### 设计模式种类

- **创建型模式**:关注对象的创建过程
    - 单例模式
    - 工厂模式
    - 抽象工厂模式
    - 建造者模式
    - 原型模式
- **结构型模式**:关注系统中对象之间的相互交互,研究系统在运行时对象之间的相互通信和协作,进一步明确对象的职责
    - 适配器模式
    - 桥接模式
    - 装饰模式
    - 组合模式
    - 外观模式
    - 享元模式
    - 代理模式
- **行为型模式**:关注对象和类的组织
    - 模板方法模式
    - 命令模式
    - 迭代器模式
    - 观察者模式
    - 中介者模式
    - 备忘录模式
    - 解释器模式
    - 状态模式
    - 策略模式
    - 职责链模式
    - 访问者模式

### DCL

- DCL使用volatile关键字,是为了禁止指令重排序,避免返回还没完成初始化的singleton对象,导致调用报错,也保证了线程的安全

```java
public class Singleton {
    //Singleton对象属性,加上volatile关键字是为了防止指定重排序,要知道singleton = new Singleton()拆分成cpu指令的话,有足足3个步骤
    private volatile static Singleton singleton;

    //对外提供的获取实例的方法
    public static Singleton getInstance() {
        if (singleton == null) {
            synchronized (Singleton.class) {
                if (singleton == null) {
                    singleton = new Singleton();
                }
            }
        }
        return singleton;
    }
}
```

- 第一个 singleton == null的判断是为了避免线程串行化,如果为空,就进入synchronized代码块中,获取锁后再操作,如果不为空,直接就返回singleton对象了,无需再进行锁竞争和等待了
- 第二个singleton == null的判断是为了防止有多个线程同时跳过第一个singleton == null的判断,比如线程一先获取到锁,进入同步代码块中,发现singleton实例还是null,就会做new操作,然后退出同步代码块并释放锁,这时一起跳过第一层singleton == null的判断的还有线程二,这时线程一释放了锁,线程二就会获取到锁,如果没有第二层的singleton == null这个判断挡着,那就会再创建一个singleton实例,就违反了单例的约束了

## JS

### 什么是闭包

- 闭包就是能够读取其他函数内部变量的函数
- 由于在 Javascript 语言中,只有函数内部的子函数才能读取局部变量,因此可以把闭包简单理解成"定义在一个函数内部的函数"
- 所以在本质上,闭包就是将函数内部和函数外部连接起来的一座桥梁

## 场景题

### 对大量元素出现的频率排序或取前n个

1. Hash映射：遍历文件，对于每个词代入Hash函数，分布到多个小文件中。这样相同的元素在同一个文件中
2. HashMap统计：对每个小文件，采用HashMap等统计每个文件中出现的元素以及相应的频率。并根据出现频率进行排序
3. 堆/归并排序：
    - **只取前n个**：取出出现频率最大的n个元素后存入文件，最后对这些文件进行堆排序或归并排序（内排序与外排序相结合）
    - **频率排序**：将排序好的键值对输出到文件中。最后对这些文件进行归并排序（内排序与外排序相结合）

### 找出两个大文件中相同的元素

- **方案1**
    1. Hash映射：遍历文件，对于每个词代入Hash函数，分布到多个小文件中。这样相同的元素在同一个文件中
    2. HashSet统计：求每对小文件中相同的url时，可以把其中一个小文件的url存储到HashSet中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
- **方案2**：如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。

### 对大量元素去重

1. Hash映射：遍历文件，对于每个词代入Hash函数，分布到多个小文件中。这样相同的元素在同一个文件中
2. HashSet统计：将各个文件中的元素放入HashSet中，利用Set性质去重
3. 归并：将各个文件的HashSet集合合并

### 找出大量数字中只出现一次的

- **方案1**：hash映射，然后hashmap统计，最后找出所有value为1的key值
- **方案2**：采用2-BitMap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，然后遍历所有数字，设置BitMap中相对应位，如果是00变01，01变10，10保持不变。初始化BitMap之后，把BitMap中对应位是01的整数输出即可。

### 判断某个数字是否在大量数据中

- **方案1**：使用BitMap，一个bit位代表一个unsigned int值。遍历所有数字并设置相应的bit位，初始化BitMap之后，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。
- **方案2**
    1. 将所有数字分成两类: 最高位为0和最高位为1，并将这两类分别写入到两个文件中，与要查找的数的最高位比较并接着进入相应的文件再查找。
    2. 再然后把这个文件为又分成两类: 次最高位为0和次最高位为1。并将这两类分别写入到两个文件中，以此类推，时间复杂度为O(logn)

### 对大量非重复数字排序

- 使用BitMap，将每个数字映射在BitMap的一个位置上并置为1，最后按顺序读取
    1. 初始化bitMap[capacity]
    2. 顺序所有读入数字，并转换为int类型，修改位向量值bitMap[number]=1
    3. 遍历bitMap数组，如果bitMap[index]=1，则输出index

### 对大量重复数字排序并取前n个

- **方案1**：用一个含n元素的最大堆完成。复杂度为O(总数据量*lgn)。
- **方案2**：采用快速排序的思想，每次分割之后只考虑比轴小的一部分，直到比轴小的一部分比n多的时候，采用传统排序算法排序，取前n个。复杂度为O(总数据量*n)。
- **方案3**：采用局部淘汰法。选取前n个元素并排序，然后逐个遍历剩余的元素，如果这个元素比前n个元素中最大的要小，那么把这个最大的元素移除，并利用插入排序的思想，插入到前n个元素中。复杂度为O(总数据量*n)。
